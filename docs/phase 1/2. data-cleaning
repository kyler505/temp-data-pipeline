# Developer Flow: Cleaning Hourly ISD Temperature Data

## Goal of the cleaning stage

Transform **raw hourly observations** into **clean, schema-valid hourly data** while:

* **preserving information** (flag, don’t delete aggressively)
* **preventing silent errors**
* preparing the data for **daily Tmax aggregation**

**Input:** raw `hourly_obs` parquet (from fetcher)
**Output:** cleaned `hourly_obs` parquet (same schema, updated `qc_flags`)

---

## Design principles (important)

1. **Cleaning ≠ filtering**
   Most issues should be *flagged*, not dropped.
2. **Deterministic rules only**
   No ML, no heuristics that depend on future data.
3. **Idempotent**
   Running cleaning twice should not change results.
4. **Schema-safe**
   Output must still pass `validate_hourly_obs`.

---

## Where this lives in your repo

Create:

```
src/tempdata/clean/
└── clean_hourly.py
```

This module should export one main function:

```python
def clean_hourly_obs(df: pd.DataFrame) -> pd.DataFrame
```

---

## Cleaning steps (in order)

### Step 1 — Enforce base schema (early fail)

First thing inside `clean_hourly_obs`:

```python
validate_hourly_obs(df)
```

Why:

* If raw data is malformed, stop immediately.
* Prevents “garbage-in → silent corruption”.

---

### Step 2 — Sort + deduplicate

**Primary key:** (`ts_utc`, `station_id`)

Rules:

* Sort by `ts_utc`
* Drop duplicate timestamps **keeping the first**
* Flag duplicates

Implementation idea:

```python
dup_mask = df.duplicated(subset=["ts_utc", "station_id"], keep="first")
df.loc[dup_mask, "qc_flags"] |= QC_DUPLICATE_TS
df = df[~dup_mask]
```

Why drop duplicates?

* Tmax aggregation depends on a clean hourly series
* Keeping both duplicates breaks uniqueness guarantees

---

### Step 3 — Missing temperature values

If `temp_c` is null:

* Flag `QC_MISSING_VALUE`
* Keep the row (important for coverage accounting later)

```python
missing = df["temp_c"].isna()
df.loc[missing, "qc_flags"] |= QC_MISSING_VALUE
```

Do **not** forward-fill or interpolate here.

---

### Step 4 — Out-of-range temperature check

Use **very wide physical bounds** (not climate bounds):

* Valid range: `[-90, 60] °C`

Rules:

* If outside range:

  * Flag `QC_OUT_OF_RANGE`
  * Set `temp_c = NaN`

Why NaN?

* Prevents invalid values from becoming Tmax
* Keeps the timestamp for coverage accounting

```python
oor = (df["temp_c"] < -90) | (df["temp_c"] > 60)
df.loc[oor, "qc_flags"] |= QC_OUT_OF_RANGE
df.loc[oor, "temp_c"] = None
```

---

### Step 5 — Spike detection (hour-to-hour jumps)

This catches sensor glitches.

Rule (conservative):

* If `|temp[t] - temp[t-1]| > 15 °C` → spike

Implementation:

```python
delta = df["temp_c"].diff().abs()
spike = delta > 15
df.loc[spike, "qc_flags"] |= QC_SPIKE_DETECTED
```

Important:

* **Do not delete spikes**
* Do **not** auto-correct
* Let aggregation decide whether to exclude flagged hours

---

### Step 6 — Final validation

After all flags are applied:

```python
validate_hourly_obs(df)
```

This ensures:

* Schema still valid
* No duplicates
* Temps within allowed range (or NaN)

---

## What the cleaning stage must NOT do

❌ Interpolate temperatures
❌ Smooth data
❌ Drop entire days
❌ Apply station-specific heuristics
❌ Convert timezones

All of that comes later (if ever).

---

## Output contract (important)

The cleaned dataset must:

* Have **the same columns** as raw
* Preserve all timestamps (except true duplicates)
* Use `qc_flags` to encode issues
* Be safe to aggregate

---

## Where cleaning is called in the pipeline

Your `run_pipeline.py` flow should now look like:

```text
fetch_isd_hourly
   ↓
validate_hourly_obs
   ↓
clean_hourly_obs
   ↓
validate_hourly_obs
   ↓
write cleaned hourly parquet
   ↓
aggregate → daily_tmax
```

---

## Minimal reporting (add this now)

After cleaning, print:

* total rows
* number of rows with any QC flag
* counts by QC flag
* min/max temp (non-null)

This will save you hours later.

---

## Definition of done (cleaning stage)

You are done when:

* [ ] cleaned hourly parquet writes successfully
* [ ] schema validation passes
* [ ] duplicate timestamps are gone
* [ ] spikes and bad values are flagged, not leaked
* [ ] no Fahrenheit / scaling issues remain
* [ ] daily Tmax aggregation produces sane values
