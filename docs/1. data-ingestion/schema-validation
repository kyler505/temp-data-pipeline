# Design Doc: Schema Runtime Validation + Pytest + CI

## Goal

Make schema issues **fail fast** and **never regress** by:

1. Running a **runtime validator** at the end of each pipeline stage (fetch → clean → aggregate) before writing outputs.
2. Running **pytest + CI** to enforce those schemas on every push/PR.

Non-goals:

* Building a full data quality system (Great Expectations/Pandera) right now
* Enforcing business-level correctness beyond a few critical invariants

---

## Scope

Applies to datasets produced by the pipeline:

* `hourly_obs` (canonical hourly observations)
* `daily_tmax` (market-aligned daily high temperature)

---

## Architecture Overview

### Pipeline stages

* **Fetch stage**: parse raw source → canonical `hourly_obs` (minimal QC, mostly parsing)
* **Clean stage**: dedupe, flag, normalize → cleaned `hourly_obs`
* **Aggregate stage**: compute local-day Tmax → `daily_tmax`

### Validation strategy

* **Runtime validation**: lightweight checks in-process right before `to_parquet()` at the end of each stage.
* **Test validation**: pytest loads sample outputs (fixtures or small generated data) and calls the same validators.
* **CI enforcement**: GitHub Actions runs formatting/lint/type checks (optional) + pytest.

---

## Design Principles

* **Single source of truth**: validators live in `src/tempdata/schemas/…`, and both pipeline code and tests import them.
* **Fail fast**: if validation fails, stage exits non-zero and does not write outputs.
* **Minimal but meaningful**: validate columns, dtypes, uniqueness, UTC/timezone invariants, sane ranges, and a few key constraints.
* **Stable contracts**: column names/types are treated as API contracts across your repo.

---

## Module Layout

```
src/tempdata/schemas/
  qc_flags.py
  validate.py
  hourly_obs.py
  daily_tmax.py

tests/
  fixtures/
    hourly_obs_sample.parquet
    daily_tmax_sample.parquet
  test_hourly_obs_schema.py
  test_daily_tmax_schema.py
  test_pipeline_smoke.py   (optional)
```

---

## Runtime Validator Design

### Public API

Each dataset schema exposes:

* `REQUIRED_COLUMNS: list[str]`
* `validate_<dataset>(df: pd.DataFrame) -> None`  (raises ValueError with actionable message)

Example:

* `validate_hourly_obs(df)`
* `validate_daily_tmax(df)`

### Validation helper functions (`schemas/validate.py`)

Provide generic assertion helpers:

* `require_columns(df, required)`
* `require_dtypes(df, expected)` (soft dtype checks, not overly strict)
* `require_no_nulls(df, cols)`
* `require_unique(df, key_cols)`
* `require_timezone_utc(df, ts_col)`
* `require_range(df, col, lo, hi, allow_null=False)`
* `require_int_range(df, col, lo, hi)`
* `require_close(df, col_a, col_b, tol)` (e.g., Celsius↔Fahrenheit consistency)

All helpers raise `ValueError` with a message that includes:

* dataset name
* offending columns
* a count of failing rows
* (optional) a small sample of failing rows indices

### Dataset validators

#### `hourly_obs` required checks

**Columns**

* `ts_utc`, `station_id`, `lat`, `lon`, `temp_c`, `source`, `qc_flags`

**Constraints**

* `ts_utc` is tz-aware and UTC
* uniqueness: (`ts_utc`, `station_id`)
* non-null: `ts_utc`, `station_id`, `temp_c`, `source`, `qc_flags`
* sane ranges:

  * `temp_c` in [-90, 60] (wide, not climate-specific)
  * `lat` in [-90, 90], `lon` in [-180, 180]
* `qc_flags` is int-like and >= 0

**Notes**

* Fetch stage may have missing hours: that’s OK.
* Duplicates should be prevented by stage end (or flagged and deduped in clean stage; if you prefer, enforce uniqueness only after clean).

#### `daily_tmax` required checks

**Columns**

* `date_local`, `station_id`, `tmax_c`, `tmax_f`, `coverage_hours`, `source`, `qc_flags`, `updated_at_utc`

**Constraints**

* uniqueness: (`date_local`, `station_id`)
* `coverage_hours` int in [0, 24]
* `tmax_c` in [-90, 60], `tmax_f` in [-130, 140] (wide)
* `updated_at_utc` tz-aware UTC
* Fahrenheit/Celsius consistency:

  * `abs(tmax_f - (tmax_c * 9/5 + 32)) <= 0.2` (tolerance for rounding)

**Timezone semantics**

* `date_local` should be a date (or midnight local). For v0:

  * Accept either `datetime64[ns]` at 00:00 or a pandas `date` column
  * Enforce “no time component” if stored as datetime (`hour==0 && minute==0 && second==0`)

---

## How Runtime Validation Is Called

### Pattern (end-of-stage)

At the end of each stage, just before writing:

```python
validate_hourly_obs(df)
df.to_parquet(out_path, index=False)
```

### Where to place calls

* Fetch stage writer (the module that outputs canonical hourly parquet)
* Clean stage writer
* Aggregate stage writer

### Failure behavior

If validation fails:

* Raise `ValueError`
* CLI catches and prints a readable error
* Exit code non-zero (CI and Colab will show failure clearly)
* Do **not** write corrupt outputs (write temp file then rename on success)

### Atomic writes (recommended)

* Write to `out_path.tmp`
* Validate (or validate before write if df is final)
* Rename to `out_path`

---

## Pytest Design

### Test types

1. **Schema unit tests (fast)**
   Load small fixture parquet files and run validators.

2. **Pipeline smoke test (optional but useful)**
   Run a tiny local pipeline on a short date range (e.g., 2–3 days) with network access off by default.

   * Prefer using saved fixtures to avoid flaky network in CI.
   * If you do run network tests, mark them `@pytest.mark.integration` and don’t run by default.

### Fixtures strategy (recommended)

Commit small parquet fixtures under `tests/fixtures/`:

* `hourly_obs_sample.parquet` (100–1000 rows)
* `daily_tmax_sample.parquet` (10–50 rows)

These fixtures should represent:

* normal values
* at least one QC-flagged record (still schema-valid)

### Example tests

* `test_hourly_obs_schema.py`

  * `df = pd.read_parquet("tests/fixtures/hourly_obs_sample.parquet")`
  * `validate_hourly_obs(df)` should pass
  * One negative test: remove a column and assert it raises

* `test_daily_tmax_schema.py`

  * Validate pass
  * Negative test: set coverage_hours=25 and assert raises

---

## CI Design (GitHub Actions)

### What CI runs on

* Push to `main`
* Pull requests to `main`

### CI jobs

**Job: tests**

* Checkout
* Setup Python (3.11 or 3.12)
* Install dependencies
* Run `pytest`

Optional add-ons later:

* `ruff` for lint
* `black` formatting check
* `mypy` if you adopt typing more heavily

### Minimal `requirements-dev.txt`

* `pytest`
* `pandas`
* `pyarrow` (parquet)
* plus your project deps

### Minimal workflow file

Create `.github/workflows/ci.yml` that runs:

* `pip install -e .` (or `pip install -r requirements.txt`)
* `pytest -q`

Keep it simple first; add linting once tests are stable.

---

## Error Messaging Requirements

Every validator failure should:

* Name the dataset (`hourly_obs` / `daily_tmax`)
* Name the violated rule (“Missing columns”, “Duplicate keys”, “Out of range”, etc.)
* Provide counts and a short sample (first 5 bad rows indices)

This makes Colab debugging painless.

---

## Rollout Plan

1. Implement `schemas/validate.py` helpers
2. Implement `validate_hourly_obs` + call it in fetch writer
3. Implement `validate_daily_tmax` + call it in aggregator writer
4. Add fixtures + pytest schema tests
5. Add CI workflow
6. Expand checks gradually only if they catch real bugs (avoid over-strict early rules)

---

## Acceptance Criteria

* Any schema break fails locally during pipeline run (before output is finalized).
* `pytest` fails if someone changes output columns/types/constraints without updating schema intentionally.
* CI blocks merges that fail schema tests.
