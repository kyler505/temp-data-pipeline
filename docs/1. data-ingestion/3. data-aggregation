# Daily Tmax Aggregation – Developer Design Document

## Purpose

Convert **cleaned hourly temperature observations** into a **market-aligned daily maximum temperature (Tmax)** dataset.

This stage produces the **ground-truth label** used for:

* backtesting
* model training
* trading validation

If this stage is wrong, *everything downstream is wrong*.

---

## Inputs & Outputs

### Input

**Dataset:** cleaned hourly observations
**Schema:** `hourly_obs`
**Key columns:**

* `ts_utc` (timezone-aware UTC)
* `station_id`
* `temp_c`
* `qc_flags`

Source: `data/clean/hourly_obs/<station_id>/*.parquet`

---

### Output

**Dataset:** daily Tmax
**Schema:** `daily_tmax`
**Key columns:**

* `date_local`
* `station_id`
* `tmax_c`
* `tmax_f`
* `coverage_hours`
* `qc_flags`
* `source`
* `updated_at_utc`

Destination:
`data/clean/daily_tmax/<station_id>.parquet`

---

## Design Principles

1. **Market-aligned**
   Tmax must reflect the **station-local calendar day**, not UTC.

2. **Deterministic**
   Same input → same output. No randomness, no learned rules.

3. **Conservative QC**
   Exclude clearly invalid hours, but avoid over-filtering.

4. **Transparent quality**
   Every day must carry a quality signal (`coverage_hours`, `qc_flags`).

---

## Timezone Handling (Critical)

### Rule

* Convert `ts_utc` → **station-local time**
* Group by **local calendar date**

For KLGA:

```text
Timezone: America/New_York
```

### Non-negotiable

* **Do not aggregate by UTC date**
* **Do not mix timezones**
* **Do not drop timezone info early**

### Implementation guideline

* Convert to local tz
* Extract `date_local`
* Discard time component after grouping

---

## Aggregation Logic

### Step 1 — Load cleaned hourly data

* Concatenate all hourly partitions for the station
* Assume schema validation has already passed

---

### Step 2 — Convert timestamps to local time

* Create a new column: `ts_local`
* Derive `date_local` from `ts_local`

`date_local` should represent the **local calendar day**, not a timestamp with time-of-day meaning.

---

### Step 3 — Determine valid hourly observations

An hourly observation is **eligible for Tmax** if:

* `temp_c` is not null
* `QC_OUT_OF_RANGE` is **not** set

Initial v0 policy:

* **Include spike-flagged values**
* Propagate spike info to the day via `qc_flags`

This avoids accidentally removing real heat spikes.

---

### Step 4 — Aggregate by day

For each `(station_id, date_local)` group:

#### Coverage

* `coverage_hours = count(valid hourly observations)`
* Expected maximum: `24`

#### Tmax

* `tmax_c = max(valid temp_c)`
* `tmax_f = tmax_c * 9/5 + 32`

---

### Step 5 — Assign daily QC flags

Daily `qc_flags` should encode **data sufficiency**, not model confidence.

Suggested rules:

| Condition                          | Flag                          |
| ---------------------------------- | ----------------------------- |
| `coverage_hours < 18`              | `QC_LOW_COVERAGE`             |
| `coverage_hours == 0`              | `QC_INCOMPLETE_DAY`           |
| Any hourly spike detected that day | propagate `QC_SPIKE_DETECTED` |

You are not deciding “good vs bad days” here — only labeling.

---

### Step 6 — Metadata fields

For each output row:

* `source = "noaa_isd"`
* `updated_at_utc = now (UTC)`

---

## Validation (Mandatory)

At the **end of aggregation**, before writing output:

```python
validate_daily_tmax(df_daily)
```

This must enforce:

* required columns exist
* unique (`station_id`, `date_local`)
* `coverage_hours ∈ [0, 24]`
* Celsius ↔ Fahrenheit consistency
* sane temperature bounds
* timezone correctness

**Do not write output if validation fails.**

---

## File & Module Structure

### Code

```
src/tempdata/aggregate/
└── build_daily_tmax.py
```

Exports:

```python
def build_daily_tmax(
    hourly_df: pd.DataFrame,
    station_tz: str
) -> pd.DataFrame
```

---

### CLI

```
scripts/build_daily_tmax.py
```

Example usage:

```bash
python scripts/build_daily_tmax.py \
  --station KLGA \
  --timezone America/New_York
```

---

## Failure Modes This Design Prevents

| Failure                       | Prevention                          |
| ----------------------------- | ----------------------------------- |
| UTC day mismatch              | Mandatory local timezone conversion |
| Silent missing hours          | Explicit `coverage_hours`           |
| Sensor glitches becoming Tmax | QC-based exclusion                  |
| Bad data poisoning training   | Runtime validation                  |
| Irreproducible backtests      | Deterministic aggregation           |

---

## Definition of Done

This stage is considered complete when:

* [ ] Daily Tmax parquet writes successfully
* [ ] Schema validation passes
* [ ] Coverage stats are sensible (most days ≥ 22 hours)
* [ ] Tmax values match spot-checked known days
* [ ] No off-by-one date errors observed around DST

--
