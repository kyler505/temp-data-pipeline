# Developer Flow: NOAA Hourly Fetcher (v0)

## Outcome

A command you can run like:

```bash
python scripts/run_pipeline.py --station KLGA --start 2024-01-01 --end 2024-03-01
```

…and it produces:

```
data/raw/noaa_hourly/KLGA/2024-01.parquet
data/raw/noaa_hourly/KLGA/2024-02.parquet
```

With a stable raw schema:

* `ts_utc`
* `station_id`
* `lat`, `lon`
* `temp_c`
* `source="noaa"`
* `qc_flags` (default 0 for raw)

---

# 0) Decide which NOAA API you’ll use (pick one now)

## Recommended for v0: NOAA NCEI “Integrated Surface Database (ISD)” **via NOAA API**

Reason: easiest hourly station observations at scale; good historical coverage.

**Assumption:** KLGA is a valid station in that dataset. If station IDs differ, you’ll store a mapping later.

**Dev decision:** don’t scrape Wunderground. Don’t do fancy joins yet. Just get hourly temps reliably.

---

# 1) Repo setup (1-time)

### Files to create

* `src/tempdata/fetch/noaa_hourly.py`
* `src/tempdata/config.py`
* `scripts/fetch_noaa_hourly.py`
* `.env.example`

### Environment variables

Put this in `.env.example`:

```
NOAA_TOKEN=your_token_here
```

Also add `python-dotenv` to your deps so local dev is painless.

---

# 2) Define the raw record contract (don’t skip)

In `src/tempdata/schemas/hourly_obs.py` (or inline for v0) define exactly what you output:

Raw record fields:

* `ts_utc`: timezone-aware UTC timestamp
* `station_id`: "KLGA" (your internal id)
* `lat`, `lon`: floats
* `temp_c`: float
* `source`: "noaa"
* `qc_flags`: int (0 for raw)

Why now: once this is stable, everything else plugs in easily.

---

# 3) Implement the fetcher module (core work)

## `noaa_hourly.py` responsibilities

### Inputs

* `station_id` (string, like `"KLGA"`)
* `lat`, `lon`
* `start_date`, `end_date` (inclusive/exclusive; pick one and stick to it)
* `out_dir` (default `data/raw/noaa_hourly/<station_id>/`)

### Behavior

* Chunk requests by month (or 7–31 day chunks) to avoid huge responses
* Retry on 429/5xx with exponential backoff
* Normalize timestamps → UTC
* Parse temperature to **Celsius** float
* Write **one parquet per month**

### Output

* returns list of written file paths
* logs: number of rows, coverage start/end, missing hours count (basic)

---

# 4) CLI wrapper script (so you can run it anywhere)

Create `scripts/fetch_noaa_hourly.py`:

CLI args:

* `--station KLGA`
* `--lat 40.7769 --lon -73.8740` (KLGA)
* `--start 2024-01-01`
* `--end 2024-03-01`

This script should:

* load `NOAA_TOKEN` from env
* call `fetch_noaa_hourly(...)`
* print a small summary

---

# 5) Data validation steps (must run every time)

After writing each parquet:

1. Check timestamps are sorted
2. Check min/max temperature sanity
3. Compute expected hours vs actual hours
4. Print a warning if missing > 5%

This is your early “alarm system”.

---

# 6) Colab test plan (deployment-like test)

In Colab:

1. `git clone`
2. `pip install -r ...`
3. set env var `NOAA_TOKEN`
4. run:

```bash
python scripts/fetch_noaa_hourly.py --station KLGA --lat 40.7769 --lon -73.8740 --start 2024-01-01 --end 2024-02-01
```

5. confirm parquet exists + row count looks right

---

# 7) Development checklist (what “done” looks like)

**Functional**

* [ ] Fetches at least 1 month without crashing
* [ ] Writes parquet in correct folder
* [ ] `ts_utc` is UTC and hourly-aligned (or at least parseable)
* [ ] `temp_c` is float and not all null
* [ ] Basic rate-limit handling exists

**Quality**

* [ ] Logs row count + time coverage
* [ ] Missing-hour report prints
* [ ] No secrets committed (token only in env)

---

# 8) Practical notes (so you don’t get stuck)

* NOAA APIs sometimes use different station identifiers than “KLGA”.

  * For v0, hardcode KLGA lat/lon and query by location/time if needed.
  * Later: add a `stations.csv` mapping.

* Start with a **small date range** (7–14 days) until it’s stable.
