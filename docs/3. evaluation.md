# Temperature Evaluation Framework

The core of the pipeline is the temperature evaluation framework, which assesses the accuracy of daily Tmax predictions against ground truth.

## Running Evaluation

Use `scripts/eval_daily_tmax.py` to run an evaluation.

### Basic Usage

```bash
# Evaluate KLGA from Jan 2020 to Dec 2024
python scripts/eval_daily_tmax.py --station KLGA --start 2020-01-01 --end 2024-12-31
```

### Advanced Usage with Config

For reproducible runs, use a JSON configuration file.

```bash
python scripts/eval_daily_tmax.py --config configs/eval_klga_v1.json
```

### Key Arguments

- `--station`: Station ID (e.g., `KLGA`).
- `--start` / `--end`: Evaluation period.
- `--model-type`: `ridge` (default) or `passthrough` (baseline).
- `--sigma-type`: Uncertainty estimation method (`bucketed`, `rolling`, or `global`).
- `--feature-file`: Path to pre-built features (optional, but recommended for best results).
- `--run-id`: Custom identifier for the run output.

## Configuration (`EvalConfig`)

Configuration is defined in `src/tempdata/eval/config.py`. A config file allows you to freeze all parameters:

```json
{
  "run_name": "eval_klga_v1",
  "station_ids": ["KLGA"],
  "start_date_local": "2020-01-01",
  "end_date_local": "2024-12-31",
  "min_coverage_hours": 18,
  "split": {
    "type": "static",
    "train_frac": 0.7,
    "val_frac": 0.15,
    "test_frac": 0.15
  },
  "model": {
    "type": "ridge",
    "alpha": 1.0
  },
  "uncertainty": {
    "type": "bucketed",
    "sigma_floor": 1.0
  }
}
```

## Output Artifacts

Each run creates a directory in `runs/<run_id>/` containing:

1.  **`metrics.json`**: Aggregate performance metrics (MAE, RMSE, Bias, Coverage).
2.  **`slices.json`**: Metrics broken down by month, lead time, and season.
3.  **`predictions.parquet`**: DataFrame containing:
    -   `target_date_local`
    -   `lead_hours`
    -   `y_pred_f` (Model prediction)
    -   `y_true_f` (Observed truth)
    -   `y_pred_sigma_f` (Uncertainty estimate)
4.  **`config.json`**: The frozen configuration used for the run.
5.  **`meta.json`**: Metadata (git commit, timestamp).

## Metrics

We track the following key metrics:

-   **MAE**: Mean Absolute Error (primary accuracy metric).
-   **RMSE**: Root Mean Squared Error (penalty for large errors).
-   **Bias**: Mean error (Positive = Warmer than observed, Negative = Cooler than observed).
-   **Calibration**: Percentage of truth falling within predicted uncertainty intervals (e.g., 50%, 90%).
