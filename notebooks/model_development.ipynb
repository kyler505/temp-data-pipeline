{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model-dev-header"
   },
   "source": [
    "# Temperature Model Development\n",
    "\n",
    "This notebook orchestrates the training and evaluation of temperature forecasting models.\n",
    "\n",
    "**Goals:**\n",
    "1.  **Baseline comparison**: Evaluate Persistence and kNN models against current Ridge baseline.\n",
    "2.  **Model development**: Iteratively develop and tune advanced models (XGBoost, Random Forest, LSTM).\n",
    "3.  **Ensembling**: Combine model outputs to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df6f96a",
   "metadata": {
    "id": "universal-colab-setup"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from types import ModuleType\n",
    "import importlib\n",
    "\n",
    "# --- 1. Mount Google Drive ---\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# --- 2. Setup Workspace ---\n",
    "if IN_COLAB:\n",
    "    REPO_NAME = \"temp-data-pipeline\"\n",
    "    WORKSPACE_DIR = Path(f\"/content/drive/MyDrive/{REPO_NAME}\")\n",
    "    REPO_URL = f\"https://github.com/kyler505/{REPO_NAME}.git\"\n",
    "\n",
    "    # Clone if missing\n",
    "    if not WORKSPACE_DIR.exists():\n",
    "        print(f\"Cloning {REPO_NAME} to Drive...\")\n",
    "        !git clone {REPO_URL} {str(WORKSPACE_DIR)}\n",
    "\n",
    "    os.chdir(WORKSPACE_DIR)\n",
    "    if str(WORKSPACE_DIR) not in sys.path:\n",
    "        sys.path.insert(0, str(WORKSPACE_DIR))\n",
    "else:\n",
    "    # Local development setup\n",
    "    cwd = Path.cwd().resolve()\n",
    "    project_root = None\n",
    "    for parent in [cwd] + list(cwd.parents):\n",
    "        if (parent / \"pyproject.toml\").exists():\n",
    "            project_root = parent\n",
    "            break\n",
    "    if project_root:\n",
    "        os.chdir(project_root)\n",
    "        if str(project_root) not in sys.path:\n",
    "            sys.path.insert(0, str(project_root))\n",
    "        print(f\"Local environment ready: {project_root}\")\n",
    "\n",
    "# --- 3. Python 3.12 Compatibility Shim (imp module) ---\n",
    "try:\n",
    "    import imp\n",
    "except ImportError:\n",
    "    print(\"Applying 'imp' module shim...\")\n",
    "    imp_shim = ModuleType(\"imp\")\n",
    "    imp_shim.reload = importlib.reload\n",
    "    sys.modules[\"imp\"] = imp_shim\n",
    "\n",
    "# --- 4. Run Bootstrap ---\n",
    "try:\n",
    "    from tempdata.utils.colab import bootstrap\n",
    "    bootstrap(use_wandb=True)\n",
    "except ImportError:\n",
    "    print(\"Failed to import bootstrap utility. Ensure repo is on sys.path.\")\n",
    "\n",
    "print(\"Environment initialization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dd2619",
   "metadata": {
    "id": "setup-header"
   },
   "source": [
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f32eac7",
   "metadata": {
    "id": "load-data"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5354 forecast rows and 5718 truth rows.\n",
      "Dataset ready. Train: 2557, Val: 548, Test: 549\n"
     ]
    }
   ],
   "source": [
    "# Create base configuration for loading data\n",
    "base_config = EvalConfig(\n",
    "    run_name=\"dev_baseline\",\n",
    "    station_ids=[STATION],\n",
    "    start_date_local=START_DATE,\n",
    "    end_date_local=END_DATE,\n",
    "    split=SplitConfig(\n",
    "        type=\"static\",\n",
    "        train_frac=0.7,\n",
    "        val_frac=0.15,\n",
    "        test_frac=0.15\n",
    "    )\n",
    ")\n",
    "\n",
    "# Load data (this might take a moment)\n",
    "# Note: In a real run, we'd use scripts/eval_daily_tmax.py's load_data helper\n",
    "# Here we manually constructing inputs or mocking for demonstration if files exist\n",
    "# We will rely on the library to handle file loading if paths are standard\n",
    "\n",
    "# Re-using the load_data logic from the script is a bit tricky without copying it.\n",
    "# For now, we will assume standard paths or use the EvalConfig's load_dataset capability if it exists.\n",
    "# Actually, EvalConfig doesn't hold data paths, logic is in script.\n",
    "# Let's define a helper here to load standard paths:\n",
    "\n",
    "from tempdata.config import data_root\n",
    "\n",
    "def load_standard_data(station, start_date, end_date):\n",
    "    # Attempt to load cleaned data directly\n",
    "    # This is a simplified version of the CLI loader\n",
    "    # Truth\n",
    "    truth_dir = data_root() / \"clean\" / \"daily_tmax\" / station\n",
    "    truth_files = sorted(list(truth_dir.glob(\"*.parquet\")))\n",
    "    truth_df = pd.concat([pd.read_parquet(f) for f in truth_files])\n",
    "\n",
    "    # Forecast (Open-Meteo for recent years)\n",
    "    fc_dir = data_root() / \"clean\" / \"forecasts\" / \"openmeteo\" / station\n",
    "    if not fc_dir.exists():\n",
    "         fc_dir = data_root() / \"raw\" / \"forecasts\" / \"openmeteo\" / station\n",
    "    fc_files = sorted(list(fc_dir.glob(\"*.parquet\")))\n",
    "    forecast_df = pd.concat([pd.read_parquet(f) for f in fc_files])\n",
    "\n",
    "    return forecast_df, truth_df\n",
    "\n",
    "try:\n",
    "    forecast_raw, truth_raw = load_standard_data(STATION, START_DATE, END_DATE)\n",
    "    print(f\"Loaded {len(forecast_raw)} forecast rows and {len(truth_raw)} truth rows.\")\n",
    "\n",
    "    # Process into Dataset object (handles joining, feature engineering, splitting)\n",
    "    dataset = load_eval_data(\n",
    "        config=base_config,\n",
    "        forecast_df=forecast_raw,\n",
    "        truth_df=truth_raw\n",
    "    )\n",
    "    print(f\"Dataset ready. Train: {len(dataset.train)}, Val: {len(dataset.val)}, Test: {len(dataset.test)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Ensure you have run the data pipeline (notebooks/temp_data_pipeline.ipynb) to populate data/clean.\")\n",
    "\n",
    "# Initialize global model registry\n",
    "model_configs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64950ce",
   "metadata": {
    "id": "baselines-header"
   },
   "source": [
    "## 2. Baseline Models Experimentation\n",
    "\n",
    "We will evaluate three baseline approaches:\n",
    "1.  **Persistence**: Tomorrow = Today.\n",
    "2.  **Ridge**: Simple linear correction of forecast.\n",
    "3.  **kNN**: Nearest neighbors in feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2182a2a",
   "metadata": {
    "id": "run-baselines"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Persistence to registry...\n",
      "Adding Ridge to registry...\n",
      "Adding kNN (k=50) to registry...\n",
      "Current models: ['Persistence', 'Ridge', 'kNN (k=50)']\n",
      "Running baseline evaluation...\n",
      "\n",
      "============================================================\n",
      "MULTI-MODEL EVALUATION: 20260121_173246\n",
      "Models: Persistence, Ridge, kNN (k=50)\n",
      "============================================================\n",
      "\n",
      "\n",
      "--- Evaluating Model: Persistence ---\n",
      "\n",
      "[eval] Fitting model: persistence\n",
      "[eval] Generating predictions...\n",
      "[eval] Computing metrics...\n",
      "\n",
      "============================================================\n",
      "EVALUATION METRICS SUMMARY\n",
      "============================================================\n",
      "\n",
      "--- FORECAST PERFORMANCE ---\n",
      "  Samples:     549\n",
      "  MAE:         2.57°F\n",
      "  RMSE:        4.74°F\n",
      "  Bias:        +0.00°F\n",
      "  Std Error:   4.74°F\n",
      "\n",
      "--- CALIBRATION ---\n",
      "  Mean σ:      5.61°F\n",
      "  50% PI cov:  71.8% (target: 50%)\n",
      "  80% PI cov:  88.0% (target: 80%)\n",
      "  90% PI cov:  93.8% (target: 90%)\n",
      "  90% width:   18.5°F\n",
      "\n",
      "\n",
      "--- Evaluating Model: Ridge ---\n",
      "\n",
      "[eval] Fitting model: ridge\n",
      "[eval] Generating predictions...\n",
      "[eval] Computing metrics...\n",
      "\n",
      "============================================================\n",
      "EVALUATION METRICS SUMMARY\n",
      "============================================================\n",
      "\n",
      "--- FORECAST PERFORMANCE ---\n",
      "  Samples:     549\n",
      "  MAE:         1.15°F\n",
      "  RMSE:        1.40°F\n",
      "  Bias:        +0.73°F\n",
      "  Std Error:   1.20°F\n",
      "\n",
      "--- CALIBRATION ---\n",
      "  Mean σ:      1.50°F\n",
      "  50% PI cov:  49.0% (target: 50%)\n",
      "  80% PI cov:  81.1% (target: 80%)\n",
      "  90% PI cov:  92.3% (target: 90%)\n",
      "  90% width:   4.9°F\n",
      "\n",
      "\n",
      "--- Evaluating Model: kNN (k=50) ---\n",
      "\n",
      "[eval] Fitting model: knn\n",
      "[eval] Generating predictions...\n",
      "[eval] Computing metrics...\n",
      "\n",
      "============================================================\n",
      "EVALUATION METRICS SUMMARY\n",
      "============================================================\n",
      "\n",
      "--- FORECAST PERFORMANCE ---\n",
      "  Samples:     549\n",
      "  MAE:         1.23°F\n",
      "  RMSE:        1.52°F\n",
      "  Bias:        +0.78°F\n",
      "  Std Error:   1.30°F\n",
      "\n",
      "--- CALIBRATION ---\n",
      "  Mean σ:      1.55°F\n",
      "  50% PI cov:  48.6% (target: 50%)\n",
      "  80% PI cov:  80.0% (target: 80%)\n",
      "  90% PI cov:  92.7% (target: 90%)\n",
      "  90% width:   5.1°F\n",
      "\n",
      "\n",
      "Multi-model run complete: 20260121_173246\n",
      "Directory: runs/20260121_173246\n",
      "Baseline evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# Define Baseline Configs\n",
    "models_to_test = [\n",
    "    (\"Persistence\", ModelConfig(type=\"persistence\")),\n",
    "    (\"Ridge\", ModelConfig(type=\"ridge\", alpha=1.0)),\n",
    "    (\"kNN (k=50)\", ModelConfig(type=\"knn\"))\n",
    "]\n",
    "\n",
    "for name, model_conf in models_to_test:\n",
    "    print(f\"Adding {name} to registry...\")\n",
    "    model_configs[name] = EvalConfig(\n",
    "        run_name=f\"dev_{name.lower().replace(' ', '_')}\",\n",
    "        station_ids=[STATION],\n",
    "        start_date_local=START_DATE,\n",
    "        end_date_local=END_DATE,\n",
    "        split=base_config.split,\n",
    "        model=model_conf\n",
    "    )\n",
    "\n",
    "print(f\"Current models: {list(model_configs.keys())}\")\n",
    "\n",
    "# Execute Baselines\n",
    "print(\"Running baseline evaluation...\")\n",
    "multimodel_result = run_multi_model_evaluation(\n",
    "    configs=model_configs,\n",
    "    forecast_df=forecast_raw,\n",
    "    truth_df=truth_raw,\n",
    "    verbose=True\n",
    ")\n",
    "results = multimodel_result.results\n",
    "print(\"Baseline evaluation complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd60d2",
   "metadata": {
    "id": "advanced-header"
   },
   "source": [
    "## 3. Advanced Models\n",
    "\n",
    "Planned models for implementation:\n",
    "- **XGBoost**: Gradient boosted trees for non-linear bias correction.\n",
    "- **Random Forest**: Ensemble of trees for robustness.\n",
    "- **LSTM/GRU**: Sequential models to capture temporal dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xg-init-header",
   "metadata": {},
   "source": [
    "### 3.1 XGBoost: Initial Analysis (Stabilization)\n",
    "\n",
    "We start by training a default XGBoost model with Early Stopping to check stability, learning curves, and feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d081d127",
   "metadata": {
    "id": "xgboost-implementation-1"
   },
   "outputs": [],
   "source": [
    "from tempdata.eval.models import XGBoostForecaster\n",
    "from xgboost import plot_importance\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "print(\"Initializing XGBoost...\")\n",
    "\n",
    "# --- Initial Config & Training ---\n",
    "xgb_params_init = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"n_estimators\": 1000,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"max_depth\": 6,\n",
    "    \"min_child_weight\": 5,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"early_stopping_rounds\": 50,\n",
    "    \"tree_method\": \"hist\"\n",
    "}\n",
    "\n",
    "# Instantiate from library\n",
    "xgb_model = XGBoostForecaster(hyperparams=xgb_params_init)\n",
    "\n",
    "# Train with Early Stopping (using eval_set passed to library model)\n",
    "xgb_model.fit(\n",
    "    dataset.train,\n",
    "    eval_set=[dataset.val],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"Initial Best Score (RMSE): {xgb_model.model.best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xg-diagnostics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Diagnostics: Learning Curves & Importance ---\n",
    "results_dict = xgb_model.model.evals_result()\n",
    "epochs = len(results_dict['validation_0']['rmse'])\n",
    "x_axis = range(0, epochs)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(x_axis, results_dict['validation_0']['rmse'], label='Validation RMSE')\n",
    "plt.title('XGBoost Learning Curve (Initial)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_importance(xgb_model.model, max_num_features=20, title='Feature Importance (Initial)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xg-tuning-header",
   "metadata": {},
   "source": [
    "### 3.2 XGBoost: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xg-tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[Tuning] Running Grid Search...\")\n",
    "\n",
    "param_grid = {\n",
    "    \"max_depth\": [4, 6],\n",
    "    \"learning_rate\": [0.05, 0.1],\n",
    "    \"subsample\": [0.7, 0.9]\n",
    "}\n",
    "\n",
    "best_score = float(\"inf\")\n",
    "best_params = xgb_params_init.copy()\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    current_params = xgb_params_init.copy()\n",
    "    current_params.update(params)\n",
    "\n",
    "    temp_model = XGBoostForecaster(hyperparams=current_params)\n",
    "    temp_model.fit(dataset.train, eval_set=[dataset.val], verbose=False)\n",
    "\n",
    "    score = temp_model.model.best_score\n",
    "    if score < best_score:\n",
    "        best_score = score\n",
    "        best_params = current_params\n",
    "        print(f\"New best: {score:.4f} with {params}\")\n",
    "\n",
    "print(f\"Best Params: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xg-final-header",
   "metadata": {},
   "source": [
    "### 3.3 XGBoost: Final Training & Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xg-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[Final] Training Final Model...\")\n",
    "final_xgb = XGBoostForecaster(hyperparams=best_params)\n",
    "final_xgb.fit(dataset.train, eval_set=[dataset.val], verbose=False)\n",
    "\n",
    "# --- Integration ---\n",
    "# Generate predictions\n",
    "preds_test = final_xgb.predict_mu(dataset.test)\n",
    "\n",
    "# Construct EvalResult\n",
    "xgb_preds_df = dataset.test.copy()\n",
    "xgb_preds_df[\"y_pred_f\"] = preds_test\n",
    "xgb_preds_df[\"y_true_f\"] = dataset.test[\"tmax_actual_f\"]\n",
    "\n",
    "from tempdata.eval.metrics import compute_forecast_metrics, EvalMetrics\n",
    "f_met = compute_forecast_metrics(xgb_preds_df)\n",
    "xgb_metrics = EvalMetrics(forecast=f_met, calibration=None, slices=None)\n",
    "\n",
    "# Add to results (Ensure 'results' dict exists!)\n",
    "if 'results' not in globals():\n",
    "    results = {}\n",
    "\n",
    "xgb_config = EvalConfig(\n",
    "    run_name=\"dev_xgboost_tuned\",\n",
    "    station_ids=base_config.station_ids,\n",
    "    start_date_local=base_config.start_date_local,\n",
    "    end_date_local=base_config.end_date_local,\n",
    "    split=base_config.split,\n",
    "    model=ModelConfig(type=\"xgboost\", hyperparams=best_params)\n",
    ")\n",
    "\n",
    "results[\"XGBoost\"] = EvalResult(\n",
    "    run_id=\"manual_xgb\",\n",
    "    config=xgb_config,\n",
    "    predictions_df=xgb_preds_df,\n",
    "    metrics=xgb_metrics,\n",
    "    artifacts={}\n",
    ")\n",
    "print(\"XGBoost integration complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af75b614",
   "metadata": {
    "id": "rf-scaffold"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement RandomForestForecaster\n",
    "# Config placeholder:\n",
    "# rf_config = ModelConfig(type=\"random_forest\", hyperparams={\"n_estimators\": 100})\n",
    "\n",
    "print(\"Random Forest implementation pending...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604fdd29",
   "metadata": {
    "id": "dl-scaffold"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement LSTM/GRU using PyTorch or TensorFlow\n",
    "# These will likely need a different data loader to handle sequences/windows\n",
    "\n",
    "print(\"Deep Learning models (LSTM/GRU) implementation pending...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b21ee3",
   "metadata": {
    "id": "ensemble-header"
   },
   "source": [
    "## 4. Ensembling Experiment\n",
    "\n",
    "Combine predictions from the best models.\n",
    "Simple Average Ensemble implementation strategy:\n",
    "1. Collect predictions from multiple models on the *same* test set.\n",
    "2. Average `y_pred_f` columns.\n",
    "3. Compute metrics on averaged prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119649e9",
   "metadata": {
    "id": "ensemble-demo"
   },
   "outputs": [],
   "source": [
    "if len(results) >= 2:\n",
    "    print(\"Simulating Ensemble (Ridge + kNN)...\")\n",
    "\n",
    "    # Use results from the multi-model run\n",
    "    p1_df = results[\"Ridge\"].predictions_df\n",
    "    p2_df = results[\"kNN (k=50)\"].predictions_df\n",
    "\n",
    "    # Align indices (just to be safe, though they should match)\n",
    "    common_idx = p1_df.index.intersection(p2_df.index)\n",
    "    p1 = p1_df.loc[common_idx, \"y_pred_f\"]\n",
    "    p2 = p2_df.loc[common_idx, \"y_pred_f\"]\n",
    "    y_true = p1_df.loc[common_idx, \"y_true_f\"]\n",
    "\n",
    "    # Simple Average\n",
    "    ensemble_pred = (p1 + p2) / 2\n",
    "\n",
    "    # Calculate Metrics\n",
    "    ens_mae = (ensemble_pred - y_true).abs().mean()\n",
    "    ens_rmse = ((ensemble_pred - y_true)**2).mean() ** 0.5\n",
    "    ens_bias = (ensemble_pred - y_true).mean()\n",
    "    print(f\"Ensemble MAE: {ens_mae:.4f}, RMSE: {ens_rmse:.4f}\")\n",
    "\n",
    "    # Create Ensemble Artifacts\n",
    "    # 1. Create Predictions DataFrame\n",
    "    ens_preds_df = p1_df.copy()\n",
    "    ens_preds_df.loc[common_idx, \"y_pred_f\"] = ensemble_pred\n",
    "    # Note: Sigma/LeadHours/etc are copied from p1 which is fine for now\n",
    "\n",
    "    # 2. Compute full metrics object\n",
    "    from tempdata.eval.metrics import compute_forecast_metrics, EvalMetrics\n",
    "    f_met = compute_forecast_metrics(ens_preds_df)\n",
    "    ens_metrics = EvalMetrics(forecast=f_met, calibration=None, slices=None)\n",
    "\n",
    "    # 3. Save Artifacts for Ensemble\n",
    "    ens_config = EvalConfig(\n",
    "        run_name=\"ensemble_avg\",\n",
    "        station_ids=[STATION],  # Reuse global\n",
    "        start_date_local=START_DATE,\n",
    "        end_date_local=END_DATE,\n",
    "        split=base_config.split,\n",
    "        model=ModelConfig(type=\"ensemble_mean\")\n",
    "    )\n",
    "\n",
    "    print(\"Saving Ensemble artifacts...\")\n",
    "    artifacts = write_model_artifacts(\n",
    "        config=ens_config,\n",
    "        metrics=ens_metrics,\n",
    "        predictions_df=ens_preds_df,\n",
    "        run_dir=multimodel_result.run_path,\n",
    "        model_name=\"Ensemble\"\n",
    "    )\n",
    "\n",
    "    # 4. Update Result and Comparison\n",
    "    # Add to results dict\n",
    "    results[\"Ensemble\"] = EvalResult(\n",
    "        run_id=multimodel_result.run_id,\n",
    "        config=ens_config,\n",
    "        predictions_df=ens_preds_df,\n",
    "        metrics=ens_metrics,\n",
    "        artifacts=artifacts\n",
    "    )\n",
    "\n",
    "    # Re-generate comparison summary\n",
    "    updated_comparison = write_comparison_summary(\n",
    "        results=results,\n",
    "        run_dir=multimodel_result.run_path\n",
    "    )\n",
    "\n",
    "    print(\"Updated Comparison:\")\n",
    "    print(pd.DataFrame(updated_comparison[\"models\"]).set_index(\"model\").sort_values(\"mae\"))\n",
    "\n",
    "\n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    subset = slice(0, 100) # First 100 days\n",
    "    plt.plot(y_true.iloc[subset].values, label=\"Actual\", color=\"black\", alpha=0.5)\n",
    "    plt.plot(p1.iloc[subset].values, label=\"Ridge\", alpha=0.7)\n",
    "    plt.plot(p2.iloc[subset].values, label=\"kNN\", alpha=0.7)\n",
    "    plt.plot(ensemble_pred.iloc[subset].values, label=\"Ensemble\", linestyle=\"--\", color=\"red\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Model Comparison (First 100 Test Days)\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
