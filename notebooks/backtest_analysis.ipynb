{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtester Analysis & Orchestration\n",
    "\n",
    "A Jupyter notebook for orchestrating backtester runs and analyzing results.\n",
    "\n",
    "**Uses pyproject.toml dependencies** with `[backtest]` extras for analysis libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup & Imports\n",
    "\n",
    "Configure environment, imports, and project paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Detect Colab\n",
    "IN_COLAB = \"google.colab\" in sys.modules or \"COLAB_GPU\" in os.environ\n",
    "\n",
    "PROJECT_ROOT = None\n",
    "\n",
    "if IN_COLAB:\n",
    "    import subprocess\n",
    "    colab_root = Path(\"/content/temp-data-pipeline\")\n",
    "    if not (colab_root / \"pyproject.toml\").exists():\n",
    "        # Clone repo if not present\n",
    "        subprocess.run(\n",
    "            [\"git\", \"clone\", \"https://github.com/kyler505/temp-data-pipeline.git\", str(colab_root)],\n",
    "            check=True,\n",
    "        )\n",
    "    else:\n",
    "        # Pull latest changes\n",
    "        subprocess.run([\"git\", \"pull\"], cwd=colab_root, check=True)\n",
    "    PROJECT_ROOT = colab_root\n",
    "else:\n",
    "    # Local: search upward for pyproject.toml\n",
    "    cwd = Path.cwd().resolve()\n",
    "    for parent in [cwd] + list(cwd.parents):\n",
    "        if (parent / \"pyproject.toml\").exists():\n",
    "            PROJECT_ROOT = parent\n",
    "            break\n",
    "    # Fallback to common dev location\n",
    "    if PROJECT_ROOT is None:\n",
    "        candidate = Path.home() / \"Documents\" / \"temp-data-pipeline\"\n",
    "        if (candidate / \"pyproject.toml\").exists():\n",
    "            PROJECT_ROOT = candidate\n",
    "\n",
    "if PROJECT_ROOT is None:\n",
    "    raise FileNotFoundError(\"Could not find project root. Set PROJECT_ROOT manually.\")\n",
    "\n",
    "# Add to Python path\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "src_path = PROJECT_ROOT / \"src\"\n",
    "if src_path.exists() and str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Import analysis dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Environment: {'Colab' if IN_COLAB else 'Local'}\")\n",
    "print(f\"Python path includes src: {src_path in sys.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages\n",
    "\n",
    "Install project dependencies in editable mode with backtest extras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import importlib\n",
    "\n",
    "# Always reinstall in editable mode to pick up any code changes\n",
    "# Include backtest extras for analysis dependencies\n",
    "if (PROJECT_ROOT / \"pyproject.toml\").exists():\n",
    "    subprocess.run(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-e\", f\"{PROJECT_ROOT}[backtest]\"],\n",
    "        check=True,\n",
    "    )\n",
    "    # Clear cached imports so we get the latest code\n",
    "    for mod_name in list(sys.modules.keys()):\n",
    "        if mod_name.startswith(\"tempdata\"):\n",
    "            del sys.modules[mod_name]\n",
    "    print(\"Installed/updated tempdata[backtest] in editable mode\")\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"pyproject.toml not found in {PROJECT_ROOT}. \"\n",
    "        \"Update PROJECT_ROOT in the setup cell.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Select Config + Run ID\n",
    "\n",
    "Choose a configuration file and run identifier.\n",
    "\n",
    "**Note:** This cell defines the parameters for the backtest run. Change these values to experiment with different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration selection\n",
    "CONFIG_PATH = \"configs/bt_klga_baseline.json\"  # Path to config file\n",
    "\n",
    "# Run identifier (leave empty for auto-generated)\n",
    "RUN_ID = \"bt_klga_baseline_2025_01_19\"  # Or None for timestamp-based ID\n",
    "\n",
    "# Alternative configs for comparison\n",
    "CONFIGS = {\n",
    "    \"baseline\": \"configs/bt_klga_baseline.json\",\n",
    "    \"aggressive\": \"configs/bt_klga_aggressive.json\",\n",
    "}\n",
    "\n",
    "# Validate config exists\n",
    "config_file = PROJECT_ROOT / CONFIG_PATH\n",
    "if not config_file.exists():\n",
    "    print(f\"⚠️  Config file not found: {config_file}\")\n",
    "    print(\"Available configs:\", list(PROJECT_ROOT.glob(\"configs/*.json\")))\n",
    "else:\n",
    "    print(f\"✅ Config selected: {CONFIG_PATH}\")\n",
    "\n",
    "print(f\"Run ID: {RUN_ID or 'auto-generated'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Run Backtester\n",
    "\n",
    "All backtester logic lives in the Python modules. This notebook only orchestrates execution.\n",
    "\n",
    "**Options:**\n",
    "- Option A: CLI invocation (recommended)\n",
    "- Option B: Python API (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: CLI invocation (recommended)\n",
    "import subprocess\n",
    "\n",
    "# Load config and build command arguments\n",
    "with open(PROJECT_ROOT / CONFIG_PATH) as f:\n",
    "    config_data = json.load(f)\n",
    "\n",
    "# Build command with individual arguments\n",
    "cmd = [\n",
    "    \"python\",\n",
    "    \"scripts/backtest_daily_tmax.py\",\n",
    "    \"--station\", config_data[\"station_ids\"][0],  # Use first station\n",
    "    \"--start\", config_data[\"start_date_local\"],\n",
    "    \"--end\", config_data[\"end_date_local\"],\n",
    "    \"--min-coverage\", str(config_data[\"min_coverage_hours\"]),\n",
    "    \"--model-type\", config_data[\"model_type\"],\n",
    "    \"--model-alpha\", str(config_data[\"model_alpha\"]),\n",
    "    \"--sigma-type\", config_data[\"sigma_type\"],\n",
    "    \"--sigma-floor\", str(config_data[\"sigma_floor\"]),\n",
    "    \"--edge-min\", str(config_data[\"edge_min\"]),\n",
    "    \"--max-per-market\", str(config_data[\"max_per_market_pct\"]),\n",
    "    \"--max-total\", str(config_data[\"max_total_pct\"]),\n",
    "    \"--slippage\", str(config_data[\"slippage\"]),\n",
    "    \"--initial-bankroll\", str(config_data[\"initial_bankroll\"]),\n",
    "    \"--price-type\", config_data[\"price_type\"],\n",
    "    \"--price-noise\", str(config_data[\"price_noise\"]),\n",
    "    \"--train-frac\", str(config_data[\"train_frac\"]),\n",
    "    \"--val-frac\", str(config_data[\"val_frac\"]),\n",
    "    \"--seed\", str(config_data[\"random_seed\"]),\n",
    "]\n",
    "\n",
    "if RUN_ID:\n",
    "    cmd.extend([\"--run-id\", RUN_ID])\n",
    "\n",
    "# Handle lead hours if specified\n",
    "if config_data.get(\"lead_hours_allowed\"):\n",
    "    lead_hours_str = \",\".join(str(h) for h in config_data[\"lead_hours_allowed\"])\n",
    "    cmd.extend([\"--lead-hours\", lead_hours_str])\n",
    "\n",
    "# Add any additional CLI args here\n",
    "# cmd.extend([\"--quiet\"])\n",
    "\n",
    "print(f\"Running: {' '.join(cmd)}\")\n",
    "print(f\"Working directory: {PROJECT_ROOT}\")\n",
    "print(f\"Config: {CONFIG_PATH}\")\n",
    "\n",
    "# Execute\n",
    "result = subprocess.run(cmd, cwd=PROJECT_ROOT, capture_output=True, text=True)\n",
    "\n",
    "print(\"\\nSTDOUT:\")\n",
    "print(result.stdout)\n",
    "\n",
    "if result.stderr:\n",
    "    print(\"\\nSTDERR:\")\n",
    "    print(result.stderr)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f\"\\n✅ Backtest completed successfully\")\n",
    "    run_dir = PROJECT_ROOT / \"runs\" / (RUN_ID or \"latest\")\n",
    "    if run_dir.exists():\n",
    "        print(f\"Artifacts: {run_dir}\")\n",
    "else:\n",
    "    print(f\"\\n❌ Backtest failed with return code {result.returncode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Load Run Artifacts\n",
    "\n",
    "**Read-only operations only.** This cell loads artifacts from the completed run.\n",
    "\n",
    "All data loaded here is immutable - treat run directories as historical records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine run directory\n",
    "if RUN_ID:\n",
    "    run_dir = PROJECT_ROOT / \"runs\" / RUN_ID\n",
    "else:\n",
    "    # Find most recent run\n",
    "    runs_dir = PROJECT_ROOT / \"runs\"\n",
    "    if runs_dir.exists():\n",
    "        run_dirs = [d for d in runs_dir.iterdir() if d.is_dir()]\n",
    "        if run_dirs:\n",
    "            run_dir = max(run_dirs, key=lambda d: d.stat().st_mtime)\n",
    "            RUN_ID = run_dir.name\n",
    "        else:\n",
    "            raise FileNotFoundError(\"No run directories found\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Runs directory not found\")\n",
    "\n",
    "print(f\"Loading run: {RUN_ID}\")\n",
    "print(f\"Run directory: {run_dir}\")\n",
    "\n",
    "# Load artifacts\n",
    "config_path = run_dir / \"config.json\"\n",
    "metrics_path = run_dir / \"metrics.json\"\n",
    "trades_path = run_dir / \"trades.parquet\"\n",
    "daily_path = run_dir / \"daily_results.parquet\"\n",
    "predictions_path = run_dir / \"predictions.parquet\"\n",
    "\n",
    "# Check what files exist\n",
    "existing_files = [f for f in [config_path, metrics_path, trades_path, daily_path, predictions_path] if f.exists()]\n",
    "print(f\"Found artifacts: {[f.name for f in existing_files]}\")\n",
    "\n",
    "# Load configuration\n",
    "with open(config_path) as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Load metrics\n",
    "with open(metrics_path) as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "# Load DataFrames (if they exist)\n",
    "trades_df = pd.read_parquet(trades_path) if trades_path.exists() else None\n",
    "daily_df = pd.read_parquet(daily_path) if daily_path.exists() else None\n",
    "predictions_df = pd.read_parquet(predictions_path) if predictions_path.exists() else None\n",
    "\n",
    "print(f\"\\n✅ Loaded run {RUN_ID}\")\n",
    "print(f\"Config: {config.get('model_type', 'unknown')} model, {config.get('station_ids', [])} stations\")\n",
    "print(f\"Date range: {config.get('start_date_local')} to {config.get('end_date_local')}\")\n",
    "\n",
    "if predictions_df is not None:\n",
    "    print(f\"Predictions: {len(predictions_df)} samples\")\n",
    "if trades_df is not None:\n",
    "    print(f\"Trades: {len(trades_df)} trades\")\n",
    "if daily_df is not None:\n",
    "    print(f\"Daily results: {len(daily_df)} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Core Analysis Blocks\n",
    "\n",
    "## A. Forecast Performance\n",
    "\n",
    "Analyze forecast accuracy, calibration, and residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast performance summary\n",
    "if 'forecast' in metrics:\n",
    "    fc = metrics['forecast']\n",
    "    print(\"=\" * 50)\n",
    "    print(\"FORECAST PERFORMANCE SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Samples:      {fc.get('n_samples', 'N/A'):,}\")\n",
    "    print(f\"MAE:          {fc.get('mae', 'N/A'):.2f}°F\")\n",
    "    print(f\"RMSE:         {fc.get('rmse', 'N/A'):.2f}°F\")\n",
    "    print(f\"Bias:         {fc.get('bias', 'N/A'):+.2f}°F\")\n",
    "    print(f\"90% PI cov:   {fc.get('coverage_90', 'N/A'):.1%}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# Residual analysis\n",
    "if predictions_df is not None and 'residual' in predictions_df.columns:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "    # Residual distribution\n",
    "    predictions_df['residual'].hist(bins=50, ax=axes[0], alpha=0.7)\n",
    "    axes[0].axvline(0, color='red', linestyle='--', alpha=0.7)\n",
    "    axes[0].set_title('Residual Distribution')\n",
    "    axes[0].set_xlabel('Residual (°F)')\n",
    "\n",
    "    # Residuals vs predictions\n",
    "    axes[1].scatter(predictions_df['tmax_pred_f'], predictions_df['residual'], alpha=0.5, s=1)\n",
    "    axes[1].axhline(0, color='red', linestyle='--', alpha=0.7)\n",
    "    axes[1].set_title('Residuals vs Predictions')\n",
    "    axes[1].set_xlabel('Predicted Tmax (°F)')\n",
    "    axes[1].set_ylabel('Residual (°F)')\n",
    "\n",
    "    # QQ plot approximation\n",
    "    from scipy import stats\n",
    "    residuals = predictions_df['residual'].dropna()\n",
    "    if len(residuals) > 10:\n",
    "        (osm, osr), (slope, intercept, r) = stats.probplot(residuals, dist=\"norm\")\n",
    "        axes[2].scatter(osm, osr, alpha=0.5, s=1)\n",
    "        axes[2].plot(osm, slope*osm + intercept, 'r-', alpha=0.7)\n",
    "        axes[2].set_title(f'Q-Q Plot (R² = {r:.3f})')\n",
    "        axes[2].set_xlabel('Theoretical Quantiles')\n",
    "        axes[2].set_ylabel('Sample Quantiles')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Bias by month\n",
    "if predictions_df is not None and 'target_date_local' in predictions_df.columns:\n",
    "    predictions_df['month'] = pd.to_datetime(predictions_df['target_date_local']).dt.month\n",
    "    monthly_bias = predictions_df.groupby('month')['residual'].agg(['mean', 'count']).round(3)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(monthly_bias.index, monthly_bias['mean'])\n",
    "    plt.axhline(0, color='red', linestyle='--', alpha=0.7)\n",
    "    plt.title('Forecast Bias by Month')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Mean Residual (°F)')\n",
    "    plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                              'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    display(monthly_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Calibration Analysis\n",
    "\n",
    "Analyze prediction interval coverage and calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration analysis\n",
    "if 'calibration' in metrics:\n",
    "    cal = metrics['calibration']\n",
    "    print(\"=\" * 50)\n",
    "    print(\"CALIBRATION ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    if 'coverage_by_bin' in cal:\n",
    "        coverage = cal['coverage_by_bin']\n",
    "        bins = list(coverage.keys())\n",
    "        observed = [coverage[b]['observed_coverage'] for b in bins]\n",
    "        expected = [coverage[b]['expected_coverage'] for b in bins]\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(expected, observed, 'bo-', alpha=0.7)\n",
    "        plt.plot([0, 1], [0, 1], 'r--', alpha=0.7, label='Perfect calibration')\n",
    "        plt.xlabel('Expected Coverage')\n",
    "        plt.ylabel('Observed Coverage')\n",
    "        plt.title('Calibration Curve')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Print coverage table\n",
    "        coverage_df = pd.DataFrame({\n",
    "            'Bin': bins,\n",
    "            'Expected': expected,\n",
    "            'Observed': observed,\n",
    "            'Difference': [o - e for o, e in zip(observed, expected)]\n",
    "        })\n",
    "        display(coverage_df.round(3))\n",
    "\n",
    "# Uncertainty distribution\n",
    "if predictions_df is not None and 'sigma' in predictions_df.columns:\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    predictions_df['sigma'].hist(bins=50, alpha=0.7)\n",
    "    plt.title('Uncertainty Distribution')\n",
    "    plt.xlabel('Sigma (°F)')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.scatter(predictions_df['sigma'], np.abs(predictions_df['residual']), alpha=0.3, s=1)\n",
    "    plt.title('Uncertainty vs |Error|')\n",
    "    plt.xlabel('Sigma (°F)')\n",
    "    plt.ylabel('|Residual| (°F)')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    # Reliability diagram\n",
    "    predictions_df['error_bin'] = pd.qcut(np.abs(predictions_df['residual']), q=10, duplicates='drop')\n",
    "    reliability = predictions_df.groupby('error_bin')['sigma'].mean()\n",
    "    error_levels = predictions_df.groupby('error_bin')['residual'].apply(lambda x: np.abs(x).mean())\n",
    "\n",
    "    plt.scatter(reliability, error_levels, alpha=0.7)\n",
    "    plt.plot([0, reliability.max()], [0, reliability.max()], 'r--', alpha=0.7)\n",
    "    plt.title('Reliability Diagram')\n",
    "    plt.xlabel('Predicted Uncertainty (Sigma)')\n",
    "    plt.ylabel('Observed |Error|')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Trading Performance\n",
    "\n",
    "Analyze P&L, drawdown, and trade statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trading performance summary\n",
    "if 'trading' in metrics:\n",
    "    tr = metrics['trading']\n",
    "    print(\"=\" * 50)\n",
    "    print(\"TRADING PERFORMANCE SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total trades: {tr.get('total_trades', 'N/A'):,}\")\n",
    "    print(f\"Total PnL:    ${tr.get('total_pnl', 'N/A'):,.2f}\")\n",
    "    print(f\"Return:       {tr.get('total_return_pct', 'N/A'):.1%}\")\n",
    "    print(f\"Sharpe:       {tr.get('sharpe_ratio', 'N/A'):.2f}\")\n",
    "    print(f\"Max DD:       ${tr.get('max_drawdown', 'N/A'):,.2f} ({tr.get('max_drawdown_pct', 'N/A'):.1%})\")\n",
    "    print(f\"Win rate:     {tr.get('win_rate', 'N/A'):.1%}\")\n",
    "    print(f\"Avg edge:     {tr.get('avg_edge', 'N/A'):.1%}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# Equity curve and drawdown\n",
    "if daily_df is not None and len(daily_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "    # Equity curve\n",
    "    axes[0].plot(daily_df['date'], daily_df['bankroll_end'], 'b-', linewidth=1.5)\n",
    "    axes[0].set_title('Equity Curve')\n",
    "    axes[0].set_ylabel('Bankroll ($)')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "    # Drawdown\n",
    "    if 'drawdown' in daily_df.columns:\n",
    "        axes[1].fill_between(daily_df['date'], 0, -daily_df['drawdown'], color='red', alpha=0.3)\n",
    "        axes[1].plot(daily_df['date'], -daily_df['drawdown'], 'r-', linewidth=1)\n",
    "        axes[1].set_title('Drawdown')\n",
    "        axes[1].set_ylabel('Drawdown ($)')\n",
    "        axes[1].set_xlabel('Date')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Trade analysis\n",
    "if trades_df is not None and len(trades_df) > 0:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "    # PnL distribution\n",
    "    trades_df['pnl'].hist(bins=50, ax=axes[0], alpha=0.7)\n",
    "    axes[0].axvline(0, color='red', linestyle='--', alpha=0.7)\n",
    "    axes[0].set_title('Trade P&L Distribution')\n",
    "    axes[0].set_xlabel('P&L ($)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "\n",
    "    # Trade size distribution\n",
    "    if 'position_size' in trades_df.columns:\n",
    "        trades_df['position_size'].hist(bins=30, ax=axes[1], alpha=0.7)\n",
    "        axes[1].set_title('Position Size Distribution')\n",
    "        axes[1].set_xlabel('Position Size ($)')\n",
    "        axes[1].set_ylabel('Frequency')\n",
    "\n",
    "    # Edge distribution\n",
    "    if 'edge' in trades_df.columns:\n",
    "        trades_df['edge'].hist(bins=30, ax=axes[2], alpha=0.7)\n",
    "        axes[2].axvline(0, color='red', linestyle='--', alpha=0.7)\n",
    "        axes[2].set_title('Edge Distribution')\n",
    "        axes[2].set_xlabel('Edge')\n",
    "        axes[2].set_ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Trade summary stats\n",
    "    print(\"\\nTrade Statistics:\")\n",
    "    print(f\"Winning trades: {len(trades_df[trades_df['pnl'] > 0]):,}\")\n",
    "    print(f\"Losing trades:  {len(trades_df[trades_df['pnl'] < 0]):,}\")\n",
    "    print(f\"Avg win:        ${trades_df[trades_df['pnl'] > 0]['pnl'].mean():.2f}\")\n",
    "    print(f\"Avg loss:       ${trades_df[trades_df['pnl'] < 0]['pnl'].mean():.2f}\")\n",
    "    print(f\"Largest win:    ${trades_df['pnl'].max():.2f}\")\n",
    "    print(f\"Largest loss:   ${trades_df['pnl'].min():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Slicing Analysis\n",
    "\n",
    "Analyze performance across different dimensions: month, temperature regime, lead hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly performance\n",
    "if predictions_df is not None:\n",
    "    predictions_df['month'] = pd.to_datetime(predictions_df['target_date_local']).dt.month\n",
    "    monthly_perf = predictions_df.groupby('month').agg({\n",
    "        'residual': ['mean', 'std', 'count'],\n",
    "        'mae': 'mean',\n",
    "        'rmse': 'mean'\n",
    "    }).round(3)\n",
    "    monthly_perf.columns = ['bias', 'residual_std', 'count', 'mae', 'rmse']\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "    # Monthly bias\n",
    "    axes[0,0].bar(monthly_perf.index, monthly_perf['bias'])\n",
    "    axes[0,0].axhline(0, color='red', linestyle='--', alpha=0.7)\n",
    "    axes[0,0].set_title('Forecast Bias by Month')\n",
    "    axes[0,0].set_xlabel('Month')\n",
    "    axes[0,0].set_ylabel('Bias (°F)')\n",
    "    axes[0,0].set_xticks(range(1, 13))\n",
    "\n",
    "    # Monthly MAE\n",
    "    axes[0,1].bar(monthly_perf.index, monthly_perf['mae'])\n",
    "    axes[0,1].set_title('MAE by Month')\n",
    "    axes[0,1].set_xlabel('Month')\n",
    "    axes[0,1].set_ylabel('MAE (°F)')\n",
    "    axes[0,1].set_xticks(range(1, 13))\n",
    "\n",
    "    # Sample counts\n",
    "    axes[1,0].bar(monthly_perf.index, monthly_perf['count'])\n",
    "    axes[1,0].set_title('Sample Count by Month')\n",
    "    axes[1,0].set_xlabel('Month')\n",
    "    axes[1,0].set_ylabel('Count')\n",
    "    axes[1,0].set_xticks(range(1, 13))\n",
    "\n",
    "    # Temperature distribution by month (if available)\n",
    "    if 'tmax_actual_f' in predictions_df.columns:\n",
    "        monthly_temp = predictions_df.groupby('month')['tmax_actual_f'].agg(['mean', 'std']).round(1)\n",
    "        axes[1,1].bar(monthly_temp.index, monthly_temp['mean'], yerr=monthly_temp['std'], capsize=3)\n",
    "        axes[1,1].set_title('Temperature by Month')\n",
    "        axes[1,1].set_xlabel('Month')\n",
    "        axes[1,1].set_ylabel('Tmax (°F)')\n",
    "        axes[1,1].set_xticks(range(1, 13))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    display(monthly_perf)\n",
    "\n",
    "# Lead hours analysis\n",
    "if predictions_df is not None and 'lead_hours' in predictions_df.columns:\n",
    "    lead_perf = predictions_df.groupby('lead_hours').agg({\n",
    "        'residual': ['mean', 'std', 'count'],\n",
    "        'mae': 'mean',\n",
    "        'rmse': 'mean'\n",
    "    }).round(3)\n",
    "    lead_perf.columns = ['bias', 'residual_std', 'count', 'mae', 'rmse']\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "    axes[0].plot(lead_perf.index, lead_perf['mae'], 'bo-')\n",
    "    axes[0].set_title('MAE by Lead Hours')\n",
    "    axes[0].set_xlabel('Lead Hours')\n",
    "    axes[0].set_ylabel('MAE (°F)')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1].plot(lead_perf.index, lead_perf['bias'], 'ro-')\n",
    "    axes[1].axhline(0, color='black', linestyle='--', alpha=0.7)\n",
    "    axes[1].set_title('Bias by Lead Hours')\n",
    "    axes[1].set_xlabel('Lead Hours')\n",
    "    axes[1].set_ylabel('Bias (°F)')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[2].bar(lead_perf.index, lead_perf['count'])\n",
    "    axes[2].set_title('Sample Count by Lead Hours')\n",
    "    axes[2].set_xlabel('Lead Hours')\n",
    "    axes[2].set_ylabel('Count')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    display(lead_perf)\n",
    "\n",
    "# Temperature regime analysis\n",
    "if predictions_df is not None and 'tmax_actual_f' in predictions_df.columns:\n",
    "    # Create temperature bins\n",
    "    temp_bins = pd.qcut(predictions_df['tmax_actual_f'], q=5, duplicates='drop')\n",
    "    predictions_df['temp_regime'] = temp_bins\n",
    "\n",
    "    regime_perf = predictions_df.groupby('temp_regime').agg({\n",
    "        'residual': ['mean', 'std', 'count'],\n",
    "        'mae': 'mean',\n",
    "        'rmse': 'mean'\n",
    "    }).round(3)\n",
    "    regime_perf.columns = ['bias', 'residual_std', 'count', 'mae', 'rmse']\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    regime_labels = [f\"{interval.left:.0f}-{interval.right:.0f}°F\" for interval in regime_perf.index]\n",
    "\n",
    "    axes[0].bar(range(len(regime_perf)), regime_perf['mae'])\n",
    "    axes[0].set_title('MAE by Temperature Regime')\n",
    "    axes[0].set_xlabel('Temperature Range')\n",
    "    axes[0].set_ylabel('MAE (°F)')\n",
    "    axes[0].set_xticks(range(len(regime_labels)))\n",
    "    axes[0].set_xticklabels(regime_labels, rotation=45, ha='right')\n",
    "\n",
    "    axes[1].bar(range(len(regime_perf)), regime_perf['bias'])\n",
    "    axes[1].axhline(0, color='red', linestyle='--', alpha=0.7)\n",
    "    axes[1].set_title('Bias by Temperature Regime')\n",
    "    axes[1].set_xlabel('Temperature Range')\n",
    "    axes[1].set_ylabel('Bias (°F)')\n",
    "    axes[1].set_xticks(range(len(regime_labels)))\n",
    "    axes[1].set_xticklabels(regime_labels, rotation=45, ha='right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    display(regime_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Multi-Run Comparison\n",
    "\n",
    "Compare metrics and performance across multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multiple runs for comparison\n",
    "def load_run_data(run_id):\n",
    "    \"\"\"Load metrics and key data for a run.\"\"\"\n",
    "    run_dir = PROJECT_ROOT / \"runs\" / run_id\n",
    "    if not run_dir.exists():\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        with open(run_dir / \"metrics.json\") as f:\n",
    "            metrics = json.load(f)\n",
    "\n",
    "        daily_df = pd.read_parquet(run_dir / \"daily_results.parquet\")\n",
    "        return {\n",
    "            'run_id': run_id,\n",
    "            'metrics': metrics,\n",
    "            'daily_df': daily_df\n",
    "        }\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "# Find available runs\n",
    "runs_dir = PROJECT_ROOT / \"runs\"\n",
    "if runs_dir.exists():\n",
    "    available_runs = [d.name for d in runs_dir.iterdir() if d.is_dir()]\n",
    "    print(f\"Available runs: {available_runs}\")\n",
    "\n",
    "    # Load multiple runs (modify this list as needed)\n",
    "    runs_to_compare = available_runs[:3]  # Compare first 3 runs\n",
    "    run_data = []\n",
    "\n",
    "    for run_id in runs_to_compare:\n",
    "        data = load_run_data(run_id)\n",
    "        if data:\n",
    "            run_data.append(data)\n",
    "\n",
    "    if len(run_data) > 1:\n",
    "        print(f\"\\nComparing {len(run_data)} runs: {[r['run_id'] for r in run_data]}\")\n",
    "\n",
    "        # Create comparison table\n",
    "        comparison_data = []\n",
    "        for run in run_data:\n",
    "            m = run['metrics']\n",
    "            row = {\n",
    "                'Run ID': run['run_id'],\n",
    "                'MAE': m.get('forecast', {}).get('mae', 'N/A'),\n",
    "                'Total P&L': m.get('trading', {}).get('total_pnl', 'N/A'),\n",
    "                'Sharpe': m.get('trading', {}).get('sharpe_ratio', 'N/A'),\n",
    "                'Win Rate': m.get('trading', {}).get('win_rate', 'N/A'),\n",
    "                'Max DD %': m.get('trading', {}).get('max_drawdown_pct', 'N/A'),\n",
    "                'Total Trades': m.get('trading', {}).get('total_trades', 'N/A'),\n",
    "            }\n",
    "            comparison_data.append(row)\n",
    "\n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        display(comparison_df)\n",
    "\n",
    "        # Plot equity curves\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for run in run_data:\n",
    "            daily_df = run['daily_df']\n",
    "            if len(daily_df) > 0 and 'date' in daily_df.columns and 'bankroll_end' in daily_df.columns:\n",
    "                plt.plot(daily_df['date'], daily_df['bankroll_end'], label=run['run_id'], linewidth=2)\n",
    "\n",
    "        plt.title('Equity Curves Comparison')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Bankroll ($)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.ticklabel_format(style='plain', axis='y')\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(\"Need at least 2 runs to compare.\")\n",
    "else:\n",
    "    print(\"No runs directory found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Diagnostics & Model Validation\n",
    "\n",
    "Check for overfitting, model stability, and market realism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting diagnostics\n",
    "if 'diagnostics' in metrics:\n",
    "    diag = metrics['diagnostics']\n",
    "    print(\"=\" * 50)\n",
    "    print(\"MODEL DIAGNOSTICS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    if 'overfitting' in diag:\n",
    "        of = diag['overfitting']\n",
    "        print(f\"Train MAE:     {of.get('train_mae', 'N/A'):.2f}°F\")\n",
    "        print(f\"Val MAE:       {of.get('val_mae', 'N/A'):.2f}°F\")\n",
    "        print(f\"Test MAE:      {of.get('test_mae', 'N/A'):.2f}°F\")\n",
    "        print(f\"Overfit ratio: {of.get('overfit_ratio', 'N/A'):.2f}\")\n",
    "\n",
    "        if of.get('overfit_ratio', 1) > 1.5:\n",
    "            print(\"⚠️  Potential overfitting detected\")\n",
    "        else:\n",
    "            print(\"✅ Model appears stable\")\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# Feature importance (if available)\n",
    "if predictions_df is not None:\n",
    "    # Simple correlation analysis\n",
    "    feature_cols = [col for col in predictions_df.columns if col.startswith(('sin_doy', 'cos_doy', 'bias_', 'rmse_'))]\n",
    "    if feature_cols and 'residual' in predictions_df.columns:\n",
    "        correlations = predictions_df[feature_cols + ['residual']].corr()['residual'].drop('residual')\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        correlations.abs().sort_values(ascending=True).plot(kind='barh')\n",
    "        plt.title('Feature Correlation with Residuals')\n",
    "        plt.xlabel('|Correlation|')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\nFeature correlations with residuals:\")\n",
    "        display(correlations.sort_values(key=abs, ascending=False).round(3))\n",
    "\n",
    "# Market realism checks\n",
    "if trades_df is not None:\n",
    "    print(\"\\nMarket Realism Checks:\")\n",
    "\n",
    "    # Check for unrealistic win rates\n",
    "    win_rate = len(trades_df[trades_df['pnl'] > 0]) / len(trades_df)\n",
    "    if win_rate > 0.7:\n",
    "        print(f\"⚠️  Very high win rate ({win_rate:.1%}) - check for overfitting\")\n",
    "    elif win_rate < 0.3:\n",
    "        print(f\"⚠️  Very low win rate ({win_rate:.1%}) - may indicate poor edge\")\n",
    "    else:\n",
    "        print(f\"✅ Reasonable win rate ({win_rate:.1%})\")\n",
    "\n",
    "    # Check Sharpe ratio\n",
    "    if 'trading' in metrics:\n",
    "        sharpe = metrics['trading'].get('sharpe_ratio', 0)\n",
    "        if sharpe > 3:\n",
    "            print(f\"⚠️  Unrealistically high Sharpe ({sharpe:.1f}) - check for bugs\")\n",
    "        elif sharpe < 0.5:\n",
    "            print(f\"⚠️  Low Sharpe ({sharpe:.1f}) - poor risk-adjusted returns\")\n",
    "        else:\n",
    "            print(f\"✅ Reasonable Sharpe ({sharpe:.1f})\")\n",
    "\n",
    "    # Check for position concentration\n",
    "    if 'position_size' in trades_df.columns:\n",
    "        max_position = trades_df['position_size'].max()\n",
    "        mean_position = trades_df['position_size'].mean()\n",
    "        concentration = max_position / mean_position\n",
    "        if concentration > 10:\n",
    "            print(f\"⚠️  High position concentration (max/mean = {concentration:.1f}) - check risk management\")\n",
    "        else:\n",
    "            print(f\"✅ Reasonable position sizing (max/mean = {concentration:.1f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Export Analysis Results\n",
    "\n",
    "Save plots and analysis results outside the run directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figures directory\n",
    "figures_dir = PROJECT_ROOT / \"figures\"\n",
    "figures_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Export key plots\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Example: Save equity curve\n",
    "if daily_df is not None and len(daily_df) > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(daily_df['date'], daily_df['bankroll_end'], 'b-', linewidth=2)\n",
    "    plt.title(f'Equity Curve - {RUN_ID}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Bankroll ($)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "    equity_path = figures_dir / f\"equity_curve_{RUN_ID}_{timestamp}.png\"\n",
    "    plt.savefig(equity_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"✅ Saved equity curve: {equity_path}\")\n",
    "\n",
    "# Save analysis summary\n",
    "summary_path = figures_dir / f\"analysis_summary_{RUN_ID}_{timestamp}.txt\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(f\"Analysis Summary for Run: {RUN_ID}\\n\")\n",
    "    f.write(f\"Generated: {datetime.now()}\\n\\n\")\n",
    "\n",
    "    if 'forecast' in metrics:\n",
    "        fc = metrics['forecast']\n",
    "        f.write(\"FORECAST PERFORMANCE:\\n\")\n",
    "        f.write(f\"  MAE: {fc.get('mae', 'N/A'):.2f}°F\\n\")\n",
    "        f.write(f\"  RMSE: {fc.get('rmse', 'N/A'):.2f}°F\\n\")\n",
    "        f.write(f\"  Bias: {fc.get('bias', 'N/A'):+.2f}°F\\n\")\n",
    "        f.write(f\"  90% PI Coverage: {fc.get('coverage_90', 'N/A'):.1%}\\n\\n\")\n",
    "\n",
    "    if 'trading' in metrics:\n",
    "        tr = metrics['trading']\n",
    "        f.write(\"TRADING PERFORMANCE:\\n\")\n",
    "        f.write(f\"  Total P&L: ${tr.get('total_pnl', 'N/A'):,.2f}\\n\")\n",
    "        f.write(f\"  Return: {tr.get('total_return_pct', 'N/A'):.1%}\\n\")\n",
    "        f.write(f\"  Sharpe: {tr.get('sharpe_ratio', 'N/A'):.2f}\\n\")\n",
    "        f.write(f\"  Win Rate: {tr.get('win_rate', 'N/A'):.1%}\\n\")\n",
    "        f.write(f\"  Max Drawdown: {tr.get('max_drawdown_pct', 'N/A'):.1%}\\n\")\n",
    "\n",
    "print(f\"✅ Saved analysis summary: {summary_path}\")\n",
    "print(f\"\\nAll analysis artifacts saved to: {figures_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
