{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Pipeline Orchestration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed7f29e5"
      },
      "source": [
        "## Setup\n",
        "\n",
        "File setup for project paths and data directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6dc1d05",
        "outputId": "d0e6b31c-b307-4154-a20d-4ad8cf158f4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /content/temp-data-pipeline\n",
            "Data dir: /content/temp-data-pipeline/data\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect Colab\n",
        "IN_COLAB = \"google.colab\" in sys.modules or \"COLAB_GPU\" in os.environ\n",
        "\n",
        "PROJECT_ROOT = None\n",
        "\n",
        "if IN_COLAB:\n",
        "    import subprocess\n",
        "    colab_root = Path(\"/content/temp-data-pipeline\")\n",
        "    if not (colab_root / \"pyproject.toml\").exists():\n",
        "        # Clone repo if not present\n",
        "        subprocess.run(\n",
        "            [\"git\", \"clone\", \"https://github.com/kyler505/temp-data-pipeline.git\", str(colab_root)],\n",
        "            check=True,\n",
        "        )\n",
        "    else:\n",
        "        # Pull latest changes\n",
        "        subprocess.run([\"git\", \"pull\"], cwd=colab_root, check=True)\n",
        "    PROJECT_ROOT = colab_root\n",
        "else:\n",
        "    # Local: search upward for pyproject.toml\n",
        "    cwd = Path.cwd().resolve()\n",
        "    for parent in [cwd] + list(cwd.parents):\n",
        "        if (parent / \"pyproject.toml\").exists():\n",
        "            PROJECT_ROOT = parent\n",
        "            break\n",
        "    # Fallback to common dev location\n",
        "    if PROJECT_ROOT is None:\n",
        "        candidate = Path.home() / \"Documents\" / \"temp-data-pipeline\"\n",
        "        if (candidate / \"pyproject.toml\").exists():\n",
        "            PROJECT_ROOT = candidate\n",
        "\n",
        "if PROJECT_ROOT is None:\n",
        "    raise FileNotFoundError(\"Could not find project root. Set PROJECT_ROOT manually.\")\n",
        "\n",
        "# Add to Python path\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "src_path = PROJECT_ROOT / \"src\"\n",
        "if src_path.exists() and str(src_path) not in sys.path:\n",
        "    sys.path.insert(0, str(src_path))\n",
        "\n",
        "DATA_DIR = PROJECT_ROOT / \"data\"\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"Data dir: {DATA_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install packages\n",
        "\n",
        "Install project dependencies in editable mode if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installed/updated tempdata in editable mode\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "import importlib\n",
        "\n",
        "# Always reinstall in editable mode to pick up any code changes\n",
        "if (PROJECT_ROOT / \"pyproject.toml\").exists():\n",
        "    subprocess.run(\n",
        "        [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-e\", str(PROJECT_ROOT)],\n",
        "        check=True,\n",
        "    )\n",
        "    # Clear cached imports so we get the latest code\n",
        "    for mod_name in list(sys.modules.keys()):\n",
        "        if mod_name.startswith(\"tempdata\"):\n",
        "            del sys.modules[mod_name]\n",
        "    print(\"Installed/updated tempdata in editable mode\")\n",
        "else:\n",
        "    raise FileNotFoundError(\n",
        "        f\"pyproject.toml not found in {PROJECT_ROOT}. \"\n",
        "        \"Update PROJECT_ROOT in the setup cell.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32c96254"
      },
      "source": [
        "## Fetch NOAA hourly data\n",
        "\n",
        "Configure a station and date range, then run the fetcher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a7c82a2",
        "outputId": "77e0a89d-0c4e-40cf-f5f7-ac51c3419431"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[noaa] 2016: rows=13375 coverage=2016-01-01 00:00:00+00:00 -> 2016-12-31 23:51:00+00:00\n",
            "[noaa] 2017: rows=14043 coverage=2017-01-01 00:00:00+00:00 -> 2017-12-31 23:51:00+00:00\n",
            "[noaa] 2018: rows=14280 coverage=2018-01-01 00:00:00+00:00 -> 2018-12-31 23:51:00+00:00\n",
            "[noaa] 2019: rows=14081 coverage=2019-01-01 00:00:00+00:00 -> 2019-12-31 23:51:00+00:00\n",
            "[noaa] 2020: rows=13841 coverage=2020-01-01 00:00:00+00:00 -> 2020-12-31 23:51:00+00:00\n",
            "[noaa] 2021: rows=13565 coverage=2021-01-01 00:00:00+00:00 -> 2021-12-31 23:51:00+00:00\n",
            "[noaa] 2022: rows=13653 coverage=2022-01-01 00:00:00+00:00 -> 2022-12-31 23:51:00+00:00\n",
            "[noaa] 2023: rows=13647 coverage=2023-01-01 00:15:00+00:00 -> 2023-12-31 23:51:00+00:00\n",
            "[noaa] 2024: rows=13414 coverage=2024-01-01 00:00:00+00:00 -> 2024-12-31 23:51:00+00:00\n",
            "[noaa] 2025: rows=8808 coverage=2025-01-01 00:00:00+00:00 -> 2025-08-26 23:51:00+00:00\n",
            "Wrote 10 parquet files:\n",
            "  - /content/temp-data-pipeline/data/raw/noaa_hourly/KLGA/2016.parquet\n",
            "  - /content/temp-data-pipeline/data/raw/noaa_hourly/KLGA/2017.parquet\n",
            "  - /content/temp-data-pipeline/data/raw/noaa_hourly/KLGA/2018.parquet\n",
            "  - /content/temp-data-pipeline/data/raw/noaa_hourly/KLGA/2019.parquet\n",
            "  - /content/temp-data-pipeline/data/raw/noaa_hourly/KLGA/2020.parquet\n",
            "  - /content/temp-data-pipeline/data/raw/noaa_hourly/KLGA/2021.parquet\n",
            "  - /content/temp-data-pipeline/data/raw/noaa_hourly/KLGA/2022.parquet\n",
            "  - /content/temp-data-pipeline/data/raw/noaa_hourly/KLGA/2023.parquet\n",
            "  - /content/temp-data-pipeline/data/raw/noaa_hourly/KLGA/2024.parquet\n",
            "  - /content/temp-data-pipeline/data/raw/noaa_hourly/KLGA/2025.parquet\n"
          ]
        }
      ],
      "source": [
        "from tempdata.fetch.noaa_hourly import fetch_noaa_hourly\n",
        "\n",
        "STATION_ID = \"KLGA\"\n",
        "START_DATE = \"2016-01-01\"\n",
        "END_DATE = \"2025-08-27\"  # exclusive\n",
        "\n",
        "OUTPUT_DIR = DATA_DIR / \"raw\" / \"noaa_hourly\" / STATION_ID\n",
        "CACHE_DIR = DATA_DIR / \"cache\" / \"isd_csv\" / STATION_ID\n",
        "\n",
        "written = fetch_noaa_hourly(\n",
        "    station_id=STATION_ID,\n",
        "    start_date=START_DATE,\n",
        "    end_date=END_DATE,\n",
        "    out_dir=OUTPUT_DIR,\n",
        "    cache_dir=CACHE_DIR,\n",
        ")\n",
        "\n",
        "print(f\"Wrote {len(written)} parquet files:\")\n",
        "for path in written:\n",
        "    print(f\"  - {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b91ad96"
      },
      "source": [
        "## Verify outputs\n",
        "\n",
        "Load one parquet file to confirm the fetch results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96884b12",
        "outputId": "5b9e7269-f77c-468d-b084-4ee71786ed93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 13494 rows from 2010.parquet\n",
            "Loaded 13955 rows from 2011.parquet\n",
            "Loaded 13704 rows from 2012.parquet\n",
            "Loaded 13659 rows from 2013.parquet\n",
            "Loaded 13790 rows from 2014.parquet\n",
            "Loaded 13668 rows from 2015.parquet\n",
            "Loaded 13375 rows from 2016.parquet\n",
            "Loaded 14043 rows from 2017.parquet\n",
            "Loaded 14280 rows from 2018.parquet\n",
            "Loaded 14081 rows from 2019.parquet\n",
            "Loaded 13841 rows from 2020.parquet\n",
            "Loaded 13565 rows from 2021.parquet\n",
            "Loaded 13653 rows from 2022.parquet\n",
            "Loaded 13647 rows from 2023.parquet\n",
            "Loaded 13414 rows from 2024.parquet\n",
            "Loaded 8808 rows from 2025.parquet\n",
            "\n",
            "Total: 214977 rows from 16 files\n",
            "Schema validation passed\n",
            "                     ts_utc station_id       lat       lon  temp_c source  \\\n",
            "0 2010-01-01 00:00:00+00:00       KLGA  40.77944 -73.88035     1.1   noaa   \n",
            "1 2010-01-01 00:51:00+00:00       KLGA  40.77944 -73.88035     1.1   noaa   \n",
            "2 2010-01-01 01:36:00+00:00       KLGA  40.77944 -73.88035     1.0   noaa   \n",
            "3 2010-01-01 01:51:00+00:00       KLGA  40.77944 -73.88035     1.1   noaa   \n",
            "4 2010-01-01 02:01:00+00:00       KLGA  40.77944 -73.88035     1.0   noaa   \n",
            "\n",
            "   qc_flags  \n",
            "0         0  \n",
            "1         0  \n",
            "2         0  \n",
            "3         0  \n",
            "4         0  \n",
            "Date range: 2010-01-01 00:00:00+00:00 to 2025-08-26 23:51:00+00:00\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from tempdata.schemas import validate_hourly_obs\n",
        "\n",
        "parquet_files = sorted(OUTPUT_DIR.glob(\"*.parquet\"))\n",
        "if not parquet_files:\n",
        "    raise FileNotFoundError(f\"No parquet files found in {OUTPUT_DIR}\")\n",
        "\n",
        "# Load ALL parquet files and concatenate\n",
        "dfs = []\n",
        "for pf in parquet_files:\n",
        "    df_year = pd.read_parquet(pf)\n",
        "    dfs.append(df_year)\n",
        "    print(f\"Loaded {len(df_year)} rows from {pf.name}\")\n",
        "\n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "print(f\"\\nTotal: {len(df)} rows from {len(parquet_files)} files\")\n",
        "\n",
        "# Validate schema (will raise if invalid)\n",
        "validate_hourly_obs(df, require_unique_keys=False)\n",
        "print(\"Schema validation passed\")\n",
        "\n",
        "print(df.head())\n",
        "print(f\"Date range: {df['ts_utc'].min()} to {df['ts_utc'].max()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean hourly data\n",
        "\n",
        "Apply the cleaning pipeline to the fetched data:\n",
        "- Validate input schema (early fail on malformed data)\n",
        "- Sort and deduplicate by (ts_utc, station_id)\n",
        "- Flag missing temperature values\n",
        "- Flag and nullify out-of-range temperatures\n",
        "- Detect hour-to-hour spikes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[clean] Cleaning summary:\n",
            "  Total rows: 214977 -> 214686 (291 duplicates removed)\n",
            "  Rows with QC flags: 5769\n",
            "    QC_MISSING_VALUE: 5768\n",
            "    QC_SPIKE_DETECTED: 1\n",
            "  Temp range (valid): -17.2C to 39.4C\n",
            "\n",
            "Cleaned DataFrame shape: (214686, 7)\n",
            "                     ts_utc station_id       lat       lon  temp_c source  \\\n",
            "0 2010-01-01 00:00:00+00:00       KLGA  40.77944 -73.88035     1.1   noaa   \n",
            "1 2010-01-01 00:51:00+00:00       KLGA  40.77944 -73.88035     1.1   noaa   \n",
            "2 2010-01-01 01:36:00+00:00       KLGA  40.77944 -73.88035     1.0   noaa   \n",
            "3 2010-01-01 01:51:00+00:00       KLGA  40.77944 -73.88035     1.1   noaa   \n",
            "4 2010-01-01 02:01:00+00:00       KLGA  40.77944 -73.88035     1.0   noaa   \n",
            "\n",
            "   qc_flags  \n",
            "0         0  \n",
            "1         0  \n",
            "2         0  \n",
            "3         0  \n",
            "4         0  \n"
          ]
        }
      ],
      "source": [
        "from tempdata.clean import clean_hourly_obs\n",
        "\n",
        "# Clean the fetched data\n",
        "# This applies: deduplication, missing value flags, out-of-range handling, spike detection\n",
        "df_clean = clean_hourly_obs(df)\n",
        "\n",
        "print(f\"\\nCleaned DataFrame shape: {df_clean.shape}\")\n",
        "print(df_clean.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5911ef1"
      },
      "source": [
        "## Aggregate to Daily Tmax\n",
        "\n",
        "Convert cleaned hourly observations to daily maximum temperature (Tmax).\n",
        "\n",
        "Key design principles:\n",
        "- **Market-aligned**: Tmax is computed per station-local calendar day, not UTC\n",
        "- **QC-aware**: Hours with `QC_OUT_OF_RANGE` are excluded from Tmax calculation\n",
        "- **Spike-inclusive**: Spike-flagged values ARE included (to avoid removing real heat spikes)\n",
        "- **Transparent**: Every day carries `coverage_hours` and propagated `qc_flags`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "310d8877",
        "outputId": "80bfcb75-9439-43eb-83d0-b8509766ffbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Daily Tmax schema validation passed\n",
            "\n",
            "Aggregated 214686 hourly obs -> 5718 daily records\n",
            "Date range: 2009-12-31 to 2025-08-26\n",
            "\n",
            "Daily Tmax summary:\n",
            "                 date_local  tmax_c  tmax_f  coverage_hours  qc_flags\n",
            "0 2009-12-31 00:00:00-05:00     1.1    34.0               5        17\n",
            "1 2010-01-01 00:00:00-05:00     3.9    39.0              24         1\n",
            "2 2010-01-02 00:00:00-05:00     0.0    32.0              24         1\n",
            "3 2010-01-03 00:00:00-05:00    -5.0    23.0              24         1\n",
            "4 2010-01-04 00:00:00-05:00    -0.6    30.9              24         1\n",
            "5 2010-01-05 00:00:00-05:00    -0.6    30.9              24         1\n",
            "6 2010-01-06 00:00:00-05:00     1.1    34.0              24         1\n",
            "7 2010-01-07 00:00:00-05:00     3.3    37.9              24         1\n",
            "8 2010-01-08 00:00:00-05:00     1.1    34.0              24         1\n",
            "9 2010-01-09 00:00:00-05:00    -0.6    30.9              24         1\n"
          ]
        }
      ],
      "source": [
        "from tempdata.aggregate.build_daily_tmax import build_daily_tmax\n",
        "from tempdata.schemas.daily_tmax import validate_daily_tmax\n",
        "\n",
        "# Station timezone (KLGA is in Eastern time)\n",
        "STATION_TZ = \"America/New_York\"\n",
        "\n",
        "# Build daily Tmax from cleaned hourly data\n",
        "df_daily = build_daily_tmax(df_clean, station_tz=STATION_TZ)\n",
        "\n",
        "# Validate the output schema\n",
        "validate_daily_tmax(df_daily)\n",
        "print(\"Daily Tmax schema validation passed\")\n",
        "\n",
        "print(f\"\\nAggregated {len(df_clean)} hourly obs -> {len(df_daily)} daily records\")\n",
        "print(f\"Date range: {df_daily['date_local'].min().date()} to {df_daily['date_local'].max().date()}\")\n",
        "\n",
        "print(\"\\nDaily Tmax summary:\")\n",
        "print(df_daily[[\"date_local\", \"tmax_c\", \"tmax_f\", \"coverage_hours\", \"qc_flags\"]].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Coverage and Quality Analysis\n",
        "\n",
        "Check data quality metrics for the aggregated daily Tmax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coverage Statistics:\n",
            "  Min coverage: 5 hours\n",
            "  Max coverage: 24 hours\n",
            "  Mean coverage: 24.0 hours\n",
            "  Days with 24h coverage: 5695\n",
            "\n",
            "QC Flag Analysis:\n",
            "  Days with QC_LOW_COVERAGE: 1\n",
            "  Days with QC_INCOMPLETE_DAY: 0\n",
            "  Days with QC_SPIKE_DETECTED: 1\n",
            "  Days with no QC issues: 20\n",
            "\n",
            "Temperature Range:\n",
            "  Min Tmax: -10.0°C (14.0°F)\n",
            "  Max Tmax: 39.4°C (102.9°F)\n",
            "  Mean Tmax: 17.6°C (63.6°F)\n"
          ]
        }
      ],
      "source": [
        "from tempdata.schemas.qc_flags import QC_LOW_COVERAGE, QC_INCOMPLETE_DAY, QC_SPIKE_DETECTED\n",
        "\n",
        "# Coverage statistics\n",
        "print(\"Coverage Statistics:\")\n",
        "print(f\"  Min coverage: {df_daily['coverage_hours'].min()} hours\")\n",
        "print(f\"  Max coverage: {df_daily['coverage_hours'].max()} hours\")\n",
        "print(f\"  Mean coverage: {df_daily['coverage_hours'].mean():.1f} hours\")\n",
        "print(f\"  Days with 24h coverage: {(df_daily['coverage_hours'] == 24).sum()}\")\n",
        "\n",
        "# QC flag breakdown\n",
        "print(\"\\nQC Flag Analysis:\")\n",
        "low_coverage_days = ((df_daily['qc_flags'] & QC_LOW_COVERAGE) != 0).sum()\n",
        "incomplete_days = ((df_daily['qc_flags'] & QC_INCOMPLETE_DAY) != 0).sum()\n",
        "spike_days = ((df_daily['qc_flags'] & QC_SPIKE_DETECTED) != 0).sum()\n",
        "\n",
        "print(f\"  Days with QC_LOW_COVERAGE: {low_coverage_days}\")\n",
        "print(f\"  Days with QC_INCOMPLETE_DAY: {incomplete_days}\")\n",
        "print(f\"  Days with QC_SPIKE_DETECTED: {spike_days}\")\n",
        "print(f\"  Days with no QC issues: {(df_daily['qc_flags'] == 0).sum()}\")\n",
        "\n",
        "# Temperature range\n",
        "print(\"\\nTemperature Range:\")\n",
        "print(f\"  Min Tmax: {df_daily['tmax_c'].min():.1f}°C ({df_daily['tmax_f'].min():.1f}°F)\")\n",
        "print(f\"  Max Tmax: {df_daily['tmax_c'].max():.1f}°C ({df_daily['tmax_f'].max():.1f}°F)\")\n",
        "print(f\"  Mean Tmax: {df_daily['tmax_c'].mean():.1f}°C ({df_daily['tmax_f'].mean():.1f}°F)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c81173fe"
      },
      "source": [
        "## Save Daily Tmax\n",
        "\n",
        "Write the daily Tmax data to parquet for downstream use (backtesting, model training, trading validation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb605429",
        "outputId": "55df9858-bba2-4163-e3c5-dc548b36e0e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[aggregate] Wrote 1 rows to /content/temp-data-pipeline/data/clean/daily_tmax/KLGA/2009.parquet\n",
            "[aggregate] Wrote 365 rows to /content/temp-data-pipeline/data/clean/daily_tmax/KLGA/2010.parquet\n",
            "[aggregate] Wrote 365 rows to /content/temp-data-pipeline/data/clean/daily_tmax/KLGA/2011.parquet\n",
            "[aggregate] Wrote 366 rows to /content/temp-data-pipeline/data/clean/daily_tmax/KLGA/2012.parquet\n",
            "[aggregate] Wrote 365 rows to /content/temp-data-pipeline/data/clean/daily_tmax/KLGA/2013.parquet\n",
            "[aggregate] Wrote 365 rows to /content/temp-data-pipeline/data/clean/daily_tmax/KLGA/2014.parquet\n",
            "[aggregate] Wrote 365 rows to /content/temp-data-pipeline/data/clean/daily_tmax/KLGA/2015.parquet\n",
            "[aggregate] Wrote 366 rows to /content/temp-data-pipeline/data/clean/daily_tmax/KLGA/2016.parquet\n",
            "[aggregate] Wrote 365 rows to /content/temp-data-pipeline/data/clean/daily_tmax/KLGA/2017.parquet\n",
            "[aggregate] Wrote 365 rows to /content/temp-data-pipeline/data/clean/daily_tmax/KLGA/2018.parquet\n",
            "[aggregate] Wrote 365 rows to /content/temp-data-pipeline/data/clean/daily_tmax/KLGA/2019.parquet\n",
            "[aggregate] Wrote 366 rows to /content/temp-data-pipeline/data/clean/daily_tmax/KLGA/2020.parquet\n",
            "[aggregate] Wrote 365 rows to /content/temp-data-pipeline/data/clean/daily_tmax/KLGA/2021.parquet\n",
            "[aggregate] Wrote 365 rows to /content/temp-data-pipeline/data/clean/daily_tmax/KLGA/2022.parquet\n",
            "[aggregate] Wrote 365 rows to /content/temp-data-pipeline/data/clean/daily_tmax/KLGA/2023.parquet\n",
            "[aggregate] Wrote 366 rows to /content/temp-data-pipeline/data/clean/daily_tmax/KLGA/2024.parquet\n",
            "[aggregate] Wrote 238 rows to /content/temp-data-pipeline/data/clean/daily_tmax/KLGA/2025.parquet\n",
            "[clean] Wrote 13481 rows to /content/temp-data-pipeline/data/clean/hourly_obs/KLGA/2010.parquet\n",
            "[clean] Wrote 13942 rows to /content/temp-data-pipeline/data/clean/hourly_obs/KLGA/2011.parquet\n",
            "[clean] Wrote 13690 rows to /content/temp-data-pipeline/data/clean/hourly_obs/KLGA/2012.parquet\n",
            "[clean] Wrote 13639 rows to /content/temp-data-pipeline/data/clean/hourly_obs/KLGA/2013.parquet\n",
            "[clean] Wrote 13771 rows to /content/temp-data-pipeline/data/clean/hourly_obs/KLGA/2014.parquet\n",
            "[clean] Wrote 13645 rows to /content/temp-data-pipeline/data/clean/hourly_obs/KLGA/2015.parquet\n",
            "[clean] Wrote 13359 rows to /content/temp-data-pipeline/data/clean/hourly_obs/KLGA/2016.parquet\n",
            "[clean] Wrote 14020 rows to /content/temp-data-pipeline/data/clean/hourly_obs/KLGA/2017.parquet\n",
            "[clean] Wrote 14262 rows to /content/temp-data-pipeline/data/clean/hourly_obs/KLGA/2018.parquet\n",
            "[clean] Wrote 14061 rows to /content/temp-data-pipeline/data/clean/hourly_obs/KLGA/2019.parquet\n",
            "[clean] Wrote 13819 rows to /content/temp-data-pipeline/data/clean/hourly_obs/KLGA/2020.parquet\n",
            "[clean] Wrote 13547 rows to /content/temp-data-pipeline/data/clean/hourly_obs/KLGA/2021.parquet\n",
            "[clean] Wrote 13637 rows to /content/temp-data-pipeline/data/clean/hourly_obs/KLGA/2022.parquet\n",
            "[clean] Wrote 13624 rows to /content/temp-data-pipeline/data/clean/hourly_obs/KLGA/2023.parquet\n",
            "[clean] Wrote 13394 rows to /content/temp-data-pipeline/data/clean/hourly_obs/KLGA/2024.parquet\n",
            "[clean] Wrote 8795 rows to /content/temp-data-pipeline/data/clean/hourly_obs/KLGA/2025.parquet\n",
            "\n",
            "Pipeline complete!\n",
            "  Daily Tmax: /content/temp-data-pipeline/data/clean/daily_tmax/KLGA\n",
            "  Cleaned hourly: /content/temp-data-pipeline/data/clean/hourly_obs/KLGA\n"
          ]
        }
      ],
      "source": [
        "from tempdata.aggregate.build_daily_tmax import write_daily_tmax\n",
        "\n",
        "# Output paths - partition daily Tmax by year like hourly data\n",
        "DAILY_TMAX_DIR = DATA_DIR / \"clean\" / \"daily_tmax\" / STATION_ID\n",
        "DAILY_TMAX_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Determine year range for partitioning\n",
        "years = df_daily[\"date_local\"].dt.year.unique()\n",
        "for year in years:\n",
        "    year_df = df_daily[df_daily[\"date_local\"].dt.year == year]\n",
        "    year_path = DAILY_TMAX_DIR / f\"{year}.parquet\"\n",
        "    year_df.to_parquet(year_path, index=False)\n",
        "    print(f\"[aggregate] Wrote {len(year_df)} rows to {year_path}\")\n",
        "\n",
        "# Also save cleaned hourly data for reference\n",
        "HOURLY_CLEAN_DIR = DATA_DIR / \"clean\" / \"hourly_obs\" / STATION_ID\n",
        "HOURLY_CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Determine year range for partitioning\n",
        "years = df_clean[\"ts_utc\"].dt.year.unique()\n",
        "for year in years:\n",
        "    year_df = df_clean[df_clean[\"ts_utc\"].dt.year == year]\n",
        "    year_path = HOURLY_CLEAN_DIR / f\"{year}.parquet\"\n",
        "    year_df.to_parquet(year_path, index=False)\n",
        "    print(f\"[clean] Wrote {len(year_df)} rows to {year_path}\")\n",
        "\n",
        "print(f\"\\nPipeline complete!\")\n",
        "print(f\"  Daily Tmax: {DAILY_TMAX_DIR}\")\n",
        "print(f\"  Cleaned hourly: {HOURLY_CLEAN_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verify Saved Data\n",
        "\n",
        "Reload the saved parquet to confirm it was written correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 365 rows from 2021.parquet\n",
            "Loaded 365 rows from 2018.parquet\n",
            "Loaded 365 rows from 2022.parquet\n",
            "Loaded 366 rows from 2012.parquet\n",
            "Loaded 365 rows from 2014.parquet\n",
            "Loaded 238 rows from 2025.parquet\n",
            "Loaded 366 rows from 2024.parquet\n",
            "Loaded 365 rows from 2010.parquet\n",
            "Loaded 1 rows from 2009.parquet\n",
            "Loaded 365 rows from 2011.parquet\n",
            "Loaded 366 rows from 2016.parquet\n",
            "Loaded 365 rows from 2015.parquet\n",
            "Loaded 365 rows from 2013.parquet\n",
            "Loaded 365 rows from 2019.parquet\n",
            "Loaded 366 rows from 2020.parquet\n",
            "Loaded 365 rows from 2023.parquet\n",
            "Loaded 365 rows from 2017.parquet\n",
            "\n",
            "Total: 5718 daily records from 17 files\n",
            "Schema validation passed\n",
            "\n",
            "Daily Tmax Data (first 10 rows):\n",
            "               date_local station_id  tmax_c  tmax_f  coverage_hours   source  qc_flags                   updated_at_utc\n",
            "2021-01-01 00:00:00-05:00       KLGA     5.0    41.0              24 noaa_isd         1 2026-01-19 19:36:15.075180+00:00\n",
            "2021-01-02 00:00:00-05:00       KLGA    11.1    52.0              24 noaa_isd         1 2026-01-19 19:36:15.075180+00:00\n",
            "2021-01-03 00:00:00-05:00       KLGA     4.0    39.2              24 noaa_isd         1 2026-01-19 19:36:15.075180+00:00\n",
            "2021-01-04 00:00:00-05:00       KLGA     6.7    44.1              24 noaa_isd         1 2026-01-19 19:36:15.075180+00:00\n",
            "2021-01-05 00:00:00-05:00       KLGA     5.6    42.1              24 noaa_isd         1 2026-01-19 19:36:15.075180+00:00\n",
            "2021-01-06 00:00:00-05:00       KLGA     5.6    42.1              24 noaa_isd         1 2026-01-19 19:36:15.075180+00:00\n",
            "2021-01-07 00:00:00-05:00       KLGA     6.1    43.0              24 noaa_isd         1 2026-01-19 19:36:15.075180+00:00\n",
            "2021-01-08 00:00:00-05:00       KLGA     3.9    39.0              24 noaa_isd         1 2026-01-19 19:36:15.075180+00:00\n",
            "2021-01-09 00:00:00-05:00       KLGA     2.8    37.0              24 noaa_isd         1 2026-01-19 19:36:15.075180+00:00\n",
            "2021-01-10 00:00:00-05:00       KLGA     6.1    43.0              24 noaa_isd         1 2026-01-19 19:36:15.075180+00:00\n"
          ]
        }
      ],
      "source": [
        "# Reload and verify the saved daily Tmax data (partitioned by year)\n",
        "daily_tmax_files = list(DAILY_TMAX_DIR.glob(\"*.parquet\"))\n",
        "daily_tmax_dfs = []\n",
        "for f in daily_tmax_files:\n",
        "    df_year = pd.read_parquet(f)\n",
        "    daily_tmax_dfs.append(df_year)\n",
        "    print(f\"Loaded {len(df_year)} rows from {f.name}\")\n",
        "\n",
        "df_verify = pd.concat(daily_tmax_dfs, ignore_index=True)\n",
        "print(f\"\\nTotal: {len(df_verify)} daily records from {len(daily_tmax_files)} files\")\n",
        "\n",
        "# Validate schema\n",
        "validate_daily_tmax(df_verify)\n",
        "print(\"Schema validation passed\")\n",
        "\n",
        "# Show sample of dataset\n",
        "print(\"\\nDaily Tmax Data (first 10 rows):\")\n",
        "print(df_verify.head(10).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fetch Open-Meteo Historical Forecasts\n",
        "\n",
        "Ingest **historical** daily Tmax forecasts from Open-Meteo for the same station and date range as the truth data.\n",
        "\n",
        "This creates the **feature-side** dataset: \"What did the forecast say at issue time about a target local date?\"\n",
        "\n",
        "Key concepts:\n",
        "- **Issue time**: when the forecast was issued (simulated as midnight UTC of the day before target)\n",
        "- **Target date**: the station-local calendar date being forecasted\n",
        "- **Lead hours**: hours from issue time to target date midnight in station timezone\n",
        "\n",
        "Using historical forecasts allows us to join forecasts to truth data for model training and backtesting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching historical forecasts for KLGA\n",
            "Date range: 2016-01-01 to 2025-08-26\n",
            "\n",
            "Wrote 2 files:\n",
            "  - /content/temp-data-pipeline/data/raw/forecasts/openmeteo/KLGA/historical_2016-01-01_to_2025-08-26.json\n",
            "  - /content/temp-data-pipeline/data/clean/forecasts/openmeteo/KLGA/historical_2016-01-01_to_2025-08-26.parquet\n"
          ]
        }
      ],
      "source": [
        "from tempdata.fetch.openmeteo_daily_forecast import fetch_openmeteo_historical_forecasts\n",
        "from tempdata.schemas.daily_tmax_forecast import validate_daily_tmax_forecast\n",
        "\n",
        "# Use the same date range as the truth data (from NOAA fetch)\n",
        "FORECAST_START_DATE = START_DATE  # e.g., \"2024-01-01\"\n",
        "FORECAST_END_DATE = END_DATE      # e.g., \"2024-02-01\" (exclusive for NOAA, but inclusive for Open-Meteo)\n",
        "\n",
        "# Adjust end date: NOAA uses exclusive end, Open-Meteo uses inclusive\n",
        "# Subtract 1 day from END_DATE to match the truth data range\n",
        "from datetime import datetime, timedelta\n",
        "end_dt = datetime.strptime(FORECAST_END_DATE, \"%Y-%m-%d\") - timedelta(days=1)\n",
        "forecast_end_date = end_dt.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "print(f\"Fetching historical forecasts for {STATION_ID}\")\n",
        "print(f\"Date range: {FORECAST_START_DATE} to {forecast_end_date}\")\n",
        "\n",
        "# Output directories\n",
        "FORECAST_RAW_DIR = DATA_DIR / \"raw\" / \"forecasts\" / \"openmeteo\" / STATION_ID\n",
        "FORECAST_CLEAN_DIR = DATA_DIR / \"clean\" / \"forecasts\" / \"openmeteo\" / STATION_ID\n",
        "\n",
        "forecast_files, df_forecast = fetch_openmeteo_historical_forecasts(\n",
        "    station_id=STATION_ID,\n",
        "    start_date=FORECAST_START_DATE,\n",
        "    end_date=forecast_end_date,\n",
        "    out_raw_dir=FORECAST_RAW_DIR,\n",
        "    out_parquet_dir=FORECAST_CLEAN_DIR,\n",
        "    write_raw=True,  # Save raw JSON for debugging\n",
        ")\n",
        "\n",
        "print(f\"\\nWrote {len(forecast_files)} files:\")\n",
        "for path in forecast_files:\n",
        "    print(f\"  - {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verify Forecast Data\n",
        "\n",
        "Load and validate the forecast parquet, then display a summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 2796 forecast rows\n",
            "Schema validation passed\n",
            "\n",
            "Forecast Summary:\n",
            "  Target dates: 2017-12-31 to 2025-08-26\n",
            "  Lead hours range: 28 to 29\n",
            "  Tmax (C): -12.1 to 39.4\n",
            "  Tmax (F): 10.2 to 102.9\n",
            "\n",
            "Forecast Data (first 10 rows):\n",
            "target_date_local  tmax_pred_c  tmax_pred_f  lead_hours\n",
            "       2017-12-31        -12.1        10.22          29\n",
            "       2018-01-01         -7.7        18.14          29\n",
            "       2018-01-02         -3.6        25.52          29\n",
            "       2018-01-03         -2.0        28.40          29\n",
            "       2018-01-04         -2.8        26.96          29\n",
            "       2018-01-05         -7.2        19.04          29\n",
            "       2018-01-06        -10.5        13.10          29\n",
            "       2018-01-07         -7.9        17.78          29\n",
            "       2018-01-08         -0.5        31.10          29\n",
            "       2018-01-09          5.4        41.72          29\n"
          ]
        }
      ],
      "source": [
        "# df_forecast is already returned from fetch_openmeteo_historical_forecasts\n",
        "print(f\"Loaded {len(df_forecast)} forecast rows\")\n",
        "\n",
        "# Validate schema (already validated in fetch, but double-check)\n",
        "validate_daily_tmax_forecast(df_forecast)\n",
        "print(\"Schema validation passed\")\n",
        "\n",
        "# Display summary\n",
        "print(f\"\\nForecast Summary:\")\n",
        "print(f\"  Target dates: {df_forecast['target_date_local'].min().date()} to {df_forecast['target_date_local'].max().date()}\")\n",
        "print(f\"  Lead hours range: {df_forecast['lead_hours'].min()} to {df_forecast['lead_hours'].max()}\")\n",
        "print(f\"  Tmax (C): {df_forecast['tmax_pred_c'].min():.1f} to {df_forecast['tmax_pred_c'].max():.1f}\")\n",
        "print(f\"  Tmax (F): {df_forecast['tmax_pred_f'].min():.1f} to {df_forecast['tmax_pred_f'].max():.1f}\")\n",
        "\n",
        "print(\"\\nForecast Data (first 10 rows):\")\n",
        "print(df_forecast[[\"target_date_local\", \"tmax_pred_c\", \"tmax_pred_f\", \"lead_hours\"]].head(10).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering for Daily Tmax\n",
        "\n",
        "Transform forecasts and truth data into a **model-ready training dataset**.\n",
        "\n",
        "This section uses:\n",
        "- **Real truth data** (`df_daily`) from the NOAA aggregation step above\n",
        "- **Real historical forecasts** (`df_forecast`) from the Open-Meteo fetch above\n",
        "\n",
        "The feature engineering pipeline:\n",
        "1. Joins forecasts to truth on `(station_id, target_date_local)`\n",
        "2. Filters low-quality truth days by coverage\n",
        "3. Adds seasonal encodings: `sin_doy`, `cos_doy`, `month`\n",
        "4. Computes rolling bias/error statistics: `bias_7d`, `bias_14d`, `bias_30d`, `rmse_14d`, `rmse_30d`, `sigma_lead`\n",
        "\n",
        "All rolling features use `.shift(1)` to ensure **no lookahead** — each row's features are computed only from prior data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using real truth data: 5718 days\n",
            "  Date range: 2009-12-31 to 2025-08-26\n",
            "\n",
            "Using real Open-Meteo historical forecasts:\n",
            "  Forecast rows: 2796\n",
            "  Lead times: [np.int64(28), np.int64(29)] hours\n",
            "  Target date range: 2017-12-31 to 2025-08-26\n"
          ]
        }
      ],
      "source": [
        "# Use real data from previous cells:\n",
        "# - df_daily: NOAA observations aggregated to daily Tmax (truth)\n",
        "# - df_forecast: Open-Meteo historical forecasts\n",
        "\n",
        "# Prepare truth data for feature engineering\n",
        "df_truth_for_features = df_daily.copy()\n",
        "\n",
        "print(f\"Using real truth data: {len(df_truth_for_features)} days\")\n",
        "print(f\"  Date range: {df_truth_for_features['date_local'].min().date()} to {df_truth_for_features['date_local'].max().date()}\")\n",
        "\n",
        "# Use the real Open-Meteo historical forecasts\n",
        "df_forecast_for_features = df_forecast.copy()\n",
        "\n",
        "print(f\"\\nUsing real Open-Meteo historical forecasts:\")\n",
        "print(f\"  Forecast rows: {len(df_forecast_for_features)}\")\n",
        "print(f\"  Lead times: {sorted(df_forecast_for_features['lead_hours'].unique())} hours\")\n",
        "print(f\"  Target date range: {df_forecast_for_features['target_date_local'].min().date()} to {df_forecast_for_features['target_date_local'].max().date()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build Training Dataset\n",
        "\n",
        "Run the feature engineering pipeline to create model-ready features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training dataset: 2796 rows\n",
            "Columns: ['station_id', 'issue_time_utc', 'target_date_local', 'tmax_pred_f', 'lead_hours', 'forecast_source', 'sin_doy', 'cos_doy', 'month', 'bias_7d', 'bias_14d', 'bias_30d', 'rmse_14d', 'rmse_30d', 'sigma_lead', 'tmax_actual_f']\n",
            "\n",
            "Column types:\n",
            "  station_id: object\n",
            "  issue_time_utc: datetime64[ns, UTC]\n",
            "  target_date_local: datetime64[ns]\n",
            "  tmax_pred_f: float64\n",
            "  lead_hours: int64\n",
            "  forecast_source: object\n",
            "  sin_doy: float64\n",
            "  cos_doy: float64\n",
            "  month: int32\n",
            "  bias_7d: float64\n",
            "  bias_14d: float64\n",
            "  bias_30d: float64\n",
            "  rmse_14d: float64\n",
            "  rmse_30d: float64\n",
            "  sigma_lead: float64\n",
            "  tmax_actual_f: float64\n"
          ]
        }
      ],
      "source": [
        "from tempdata.features.build_train_daily_tmax import build_train_daily_tmax\n",
        "from tempdata.schemas.train_daily_tmax import validate_train_daily_tmax, TRAIN_DAILY_TMAX_FIELDS\n",
        "\n",
        "# Build the training dataset using real truth data + real Open-Meteo forecasts\n",
        "# This performs: join, seasonal features, rolling bias/error stats, validation\n",
        "df_train = build_train_daily_tmax(\n",
        "    forecast_df=df_forecast_for_features,\n",
        "    truth_df=df_truth_for_features,\n",
        "    min_coverage_hours=18,  # Filter low-quality truth days\n",
        "    drop_warmup_nulls=False,  # Keep warm-up rows (they have NaN in rolling features)\n",
        "    validate=True,\n",
        ")\n",
        "\n",
        "print(f\"Training dataset: {len(df_train)} rows\")\n",
        "print(f\"Columns: {list(df_train.columns)}\")\n",
        "print(f\"\\nColumn types:\")\n",
        "for col in df_train.columns:\n",
        "    print(f\"  {col}: {df_train[col].dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspect Features\n",
        "\n",
        "Examine the generated features, focusing on rolling statistics and seasonal encodings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Core Features & Seasonal Encodings (first 10 rows):\n",
            "target_date_local  lead_hours  tmax_pred_f  tmax_actual_f  sin_doy  cos_doy  month\n",
            "       2018-03-12          28        43.52           42.1 0.939570 0.342357      3\n",
            "       2018-03-13          28        41.00           41.0 0.945320 0.326144      3\n",
            "       2018-03-14          28        40.82           42.1 0.950790 0.309835      3\n",
            "       2018-03-15          28        46.94           48.0 0.955979 0.293434      3\n",
            "       2018-03-16          28        39.56           39.0 0.960885 0.276946      3\n",
            "       2018-03-17          28        47.66           48.0 0.965507 0.260376      3\n",
            "       2018-03-18          28        43.70           44.1 0.969843 0.243730      3\n",
            "       2018-03-19          28        46.58           46.9 0.973892 0.227011      3\n",
            "       2018-03-20          28        39.74           39.0 0.977653 0.210225      3\n",
            "       2018-03-21          28        39.56           39.9 0.981125 0.193376      3\n",
            "\n",
            "\n",
            "Rolling Bias & Error Features (rows 10-20, after warm-up):\n",
            "target_date_local  lead_hours   bias_7d  bias_14d  bias_30d  rmse_14d  rmse_30d  sigma_lead\n",
            "       2018-03-22          28 -0.165714 -0.102000 -0.102000  0.783147  0.783147    0.818478\n",
            "       2018-03-23          28 -0.840000 -0.618182 -0.618182  1.895967  1.895967    1.879839\n",
            "       2018-03-24          28 -0.817143 -0.506667 -0.506667  1.827111  1.827111    1.833513\n",
            "       2018-03-25          28 -0.794286 -0.481538 -0.481538  1.756141  1.756141    1.757791\n",
            "       2018-03-26          28 -0.817143 -0.487143 -0.487143  1.698865  1.698865    1.688962\n",
            "       2018-03-27          28 -1.025714 -0.715714 -0.573333  1.722913  1.704394    1.661405\n",
            "       2018-03-28          28 -1.188571 -0.744286 -0.562500  1.726226  1.653300    1.605655\n",
            "       2018-03-29          28 -1.425714 -0.795714 -0.647059  1.774413  1.675681    1.593282\n",
            "       2018-03-30          28 -0.757143 -0.798571 -0.672222  1.776152  1.648979    1.549393\n",
            "       2018-03-31          28 -0.548571 -0.682857 -0.522105  1.863269  1.681115    1.641773\n"
          ]
        }
      ],
      "source": [
        "# Display core features and seasonal encodings\n",
        "display_cols = [\n",
        "    \"target_date_local\", \"lead_hours\", \"tmax_pred_f\", \"tmax_actual_f\",\n",
        "    \"sin_doy\", \"cos_doy\", \"month\"\n",
        "]\n",
        "print(\"Core Features & Seasonal Encodings (first 10 rows):\")\n",
        "print(df_train[display_cols].head(10).to_string(index=False))\n",
        "\n",
        "# Display rolling bias/error features\n",
        "rolling_cols = [\n",
        "    \"target_date_local\", \"lead_hours\", \"bias_7d\", \"bias_14d\", \"bias_30d\",\n",
        "    \"rmse_14d\", \"rmse_30d\", \"sigma_lead\"\n",
        "]\n",
        "print(\"\\n\\nRolling Bias & Error Features (rows 10-20, after warm-up):\")\n",
        "print(df_train[rolling_cols].iloc[10:20].to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verify No-Lookahead Property\n",
        "\n",
        "Confirm that rolling features are computed correctly with `.shift(1)` — each row's features should only use prior data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First row per (station_id, lead_hours) group — bias_7d should be NaN:\n",
            "                       bias_7d  bias_14d  bias_30d\n",
            "station_id lead_hours                             \n",
            "KLGA       28             1.42      1.42      1.42\n",
            "           29           -10.78    -10.78    -10.78\n",
            "\n",
            "\n",
            "Verification for row 10:\n",
            "  Actual residual (pred - actual): -5.78°F\n",
            "  bias_7d (computed from PRIOR 7 days): -0.17°F\n",
            "  These should be different values (bias excludes current row)\n",
            "\n",
            "  Residual == bias_7d? False\n"
          ]
        }
      ],
      "source": [
        "# Verify no-lookahead: The first row for each (station, lead_hours) group should have NaN bias\n",
        "# because there's no prior data to compute rolling stats from\n",
        "\n",
        "first_rows = df_train.groupby([\"station_id\", \"lead_hours\"]).first()\n",
        "print(\"First row per (station_id, lead_hours) group — bias_7d should be NaN:\")\n",
        "print(first_rows[[\"bias_7d\", \"bias_14d\", \"bias_30d\"]].to_string())\n",
        "\n",
        "# Compute actual residual for a specific row and verify it's NOT in its own bias\n",
        "# Pick a row after warm-up period\n",
        "if len(df_train) > 10:\n",
        "    test_idx = 10\n",
        "    test_row = df_train.iloc[test_idx]\n",
        "    actual_residual = test_row[\"tmax_pred_f\"] - test_row[\"tmax_actual_f\"]\n",
        "\n",
        "    print(f\"\\n\\nVerification for row {test_idx}:\")\n",
        "    print(f\"  Actual residual (pred - actual): {actual_residual:.2f}°F\")\n",
        "    print(f\"  bias_7d (computed from PRIOR 7 days): {test_row['bias_7d']:.2f}°F\")\n",
        "    print(f\"  These should be different values (bias excludes current row)\")\n",
        "\n",
        "    # The bias_7d should NOT equal the current residual (unless by coincidence)\n",
        "    print(f\"\\n  Residual == bias_7d? {abs(actual_residual - test_row['bias_7d']) < 0.01}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyze Forecast Bias by Lead Time\n",
        "\n",
        "The rolling bias features capture systematic forecast errors that vary by lead time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Forecast Error Analysis by Lead Time:\n",
            "============================================================\n",
            "            Mean Error (°F)  Std Dev (°F)  Count  Avg bias_30d  Avg sigma_lead\n",
            "lead_hours                                                                    \n",
            "28                    -0.09          1.60   1836         -0.10            1.65\n",
            "29                    -0.46          1.46    960         -0.51            1.68\n",
            "\n",
            "\n",
            "Key Insight:\n",
            "  - sigma_lead captures uncertainty and can be used for confidence intervals\n",
            "  - bias_30d provides a rolling estimate that adapts to recent forecast performance\n"
          ]
        }
      ],
      "source": [
        "# Compute actual residuals for analysis\n",
        "df_train[\"residual\"] = df_train[\"tmax_pred_f\"] - df_train[\"tmax_actual_f\"]\n",
        "\n",
        "# Analyze bias by lead time\n",
        "print(\"Forecast Error Analysis by Lead Time:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "bias_by_lead = df_train.groupby(\"lead_hours\").agg({\n",
        "    \"residual\": [\"mean\", \"std\", \"count\"],\n",
        "    \"bias_30d\": \"mean\",  # Average of the rolling bias feature\n",
        "    \"sigma_lead\": \"mean\",  # Average sigma_lead\n",
        "}).round(2)\n",
        "\n",
        "bias_by_lead.columns = [\"Mean Error (°F)\", \"Std Dev (°F)\", \"Count\", \"Avg bias_30d\", \"Avg sigma_lead\"]\n",
        "print(bias_by_lead.to_string())\n",
        "\n",
        "print(\"\\n\\nKey Insight:\")\n",
        "print(\"  - sigma_lead captures uncertainty and can be used for confidence intervals\")\n",
        "print(\"  - bias_30d provides a rolling estimate that adapts to recent forecast performance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Training Dataset\n",
        "\n",
        "Write the feature-engineered training dataset to parquet for model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows after dropping warm-up NaNs: 2792 (was 2796)\n",
            "[features] wrote 2792 rows to /content/temp-data-pipeline/data/train/daily_tmax/KLGA/train_daily_tmax.parquet\n",
            "\n",
            "Training dataset saved to: /content/temp-data-pipeline/data/train/daily_tmax/KLGA/train_daily_tmax.parquet\n"
          ]
        }
      ],
      "source": [
        "from tempdata.features.build_train_daily_tmax import write_train_daily_tmax\n",
        "\n",
        "# Create output directory\n",
        "TRAIN_DIR = DATA_DIR / \"train\" / \"daily_tmax\" / STATION_ID\n",
        "TRAIN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Drop rows with NaN rolling features for clean training data\n",
        "df_train_clean = df_train.dropna(subset=[\"bias_7d\", \"bias_14d\", \"bias_30d\", \"rmse_14d\", \"rmse_30d\", \"sigma_lead\"])\n",
        "print(f\"Rows after dropping warm-up NaNs: {len(df_train_clean)} (was {len(df_train)})\")\n",
        "\n",
        "# Select only the schema columns (drop residual which was for analysis)\n",
        "df_train_final = df_train_clean[TRAIN_DAILY_TMAX_FIELDS].copy()\n",
        "\n",
        "# Write to parquet\n",
        "output_path = TRAIN_DIR / \"train_daily_tmax.parquet\"\n",
        "write_train_daily_tmax(df_train_final, output_path)\n",
        "\n",
        "print(f\"\\nTraining dataset saved to: {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering Summary\n",
        "\n",
        "The `train_daily_tmax` dataset is now ready for model training with:\n",
        "\n",
        "**Input Data:**\n",
        "- Truth: Real NOAA observations aggregated to daily Tmax\n",
        "- Forecasts: Real Open-Meteo historical forecasts\n",
        "\n",
        "**Core Features:**\n",
        "- `tmax_pred_f`: Raw forecast (the baseline to beat)\n",
        "- `lead_hours`: Forecast horizon (longer = more uncertainty)\n",
        "- `forecast_source`: Model identifier for multi-model ensembles\n",
        "\n",
        "**Seasonal Encodings:**\n",
        "- `sin_doy`, `cos_doy`: Capture annual temperature cycles\n",
        "- `month`: Coarse seasonal regime\n",
        "\n",
        "**Rolling Bias/Error Statistics (key value-add):**\n",
        "- `bias_7d`, `bias_14d`, `bias_30d`: Recent forecast bias (forecast - observed)\n",
        "- `rmse_14d`, `rmse_30d`: Recent forecast error magnitude\n",
        "- `sigma_lead`: Historical uncertainty for this lead time\n",
        "\n",
        "**Label:**\n",
        "- `tmax_actual_f`: Observed maximum temperature (ground truth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipeline outputs:\n",
            "  - Raw NOAA hourly data: /content/temp-data-pipeline/data/raw/noaa_hourly/KLGA\n",
            "  - Cleaned hourly observations: /content/temp-data-pipeline/data/clean/hourly_obs/KLGA\n",
            "  - Daily Tmax (truth): /content/temp-data-pipeline/data/clean/daily_tmax/KLGA/KLGA.parquet\n",
            "  - Open-Meteo forecasts: /content/temp-data-pipeline/data/clean/forecasts/openmeteo/KLGA\n",
            "  - Training dataset: /content/temp-data-pipeline/data/train/daily_tmax/KLGA/train_daily_tmax.parquet\n"
          ]
        }
      ],
      "source": [
        "# Pipeline complete - summary of outputs\n",
        "print(\"Pipeline outputs:\")\n",
        "print(f\"  - Raw NOAA hourly data: {OUTPUT_DIR}\")\n",
        "print(f\"  - Cleaned hourly observations: {HOURLY_CLEAN_DIR}\")\n",
        "print(f\"  - Daily Tmax (truth): {DAILY_TMAX_DIR / f'{STATION_ID}.parquet'}\")\n",
        "print(f\"  - Open-Meteo forecasts: {FORECAST_CLEAN_DIR}\")\n",
        "print(f\"  - Training dataset: {output_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
