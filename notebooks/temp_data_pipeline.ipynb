{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "universal-colab-setup"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from types import ModuleType\n",
    "import importlib\n",
    "\n",
    "# --- 1. Mount Google Drive ---\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# --- 2. Setup Workspace ---\n",
    "if IN_COLAB:\n",
    "    REPO_NAME = \"temp-data-pipeline\"\n",
    "    WORKSPACE_DIR = Path(f\"/content/drive/MyDrive/{REPO_NAME}\")\n",
    "    REPO_URL = f\"https://github.com/kyler505/{REPO_NAME}.git\"\n",
    "    \n",
    "    # Clone if missing\n",
    "    if not WORKSPACE_DIR.exists():\n",
    "        print(f\"Cloning {REPO_NAME} to Drive...\")\n",
    "        !git clone {REPO_URL} {str(WORKSPACE_DIR)}\n",
    "    \n",
    "    os.chdir(WORKSPACE_DIR)\n",
    "    if str(WORKSPACE_DIR) not in sys.path:\n",
    "        sys.path.insert(0, str(WORKSPACE_DIR))\n",
    "else:\n",
    "    # Local development setup\n",
    "    cwd = Path.cwd().resolve()\n",
    "    project_root = None\n",
    "    for parent in [cwd] + list(cwd.parents):\n",
    "        if (parent / \"pyproject.toml\").exists():\n",
    "            project_root = parent\n",
    "            break\n",
    "    if project_root:\n",
    "        os.chdir(project_root)\n",
    "        if str(project_root) not in sys.path:\n",
    "            sys.path.insert(0, str(project_root))\n",
    "        print(f\"Local environment ready: {project_root}\")\n",
    "\n",
    "# --- 3. Python 3.12 Compatibility Shim (imp module) ---\n",
    "try:\n",
    "    import imp\n",
    "except ImportError:\n",
    "    print(\"Applying 'imp' module shim...\")\n",
    "    imp_shim = ModuleType(\"imp\")\n",
    "    imp_shim.reload = importlib.reload\n",
    "    sys.modules[\"imp\"] = imp_shim\n",
    "\n",
    "# --- 4. Run Bootstrap ---\n",
    "try:\n",
    "    from tempdata.utils.colab import bootstrap\n",
    "    bootstrap(use_wandb=True)\n",
    "except ImportError:\n",
    "    print(\"Failed to import bootstrap utility. Ensure repo is on sys.path.\")\n",
    "\n",
    "print(\"Environment initialization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDS credentials already configured at /Users/kcao/.cdsapirc\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# CDS API credentials\n",
    "# Get your key from: https://cds.climate.copernicus.eu/profile\n",
    "CDS_API_KEY = \"6ac45a56-7542-47fb-acf8-61975dbca72d\"  # <-- Replace with your key (format UID:KEY)\n",
    "CDS_API_URL = \"https://cds.climate.copernicus.eu/api\"\n",
    "\n",
    "# Write ~/.cdsapirc if not exists\n",
    "cdsapirc_path = Path.home() / \".cdsapirc\"\n",
    "if not cdsapirc_path.exists() and CDS_API_KEY != \"YOUR_CDS_API_KEY\":\n",
    "    cdsapirc_path.write_text(f\"url: {CDS_API_URL}\\nkey: {CDS_API_KEY}\\n\")\n",
    "    print(f\"Created {cdsapirc_path}\")\n",
    "elif cdsapirc_path.exists():\n",
    "    print(f\"CDS credentials already configured at {cdsapirc_path}\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Set CDS_API_KEY above to configure ERA5 access\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32c96254"
   },
   "source": [
    "## Fetch hourly data\n",
    "\n",
    "Configure a station and date range, then run the fetcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1a7c82a2",
    "outputId": "77e0a89d-0c4e-40cf-f5f7-ac51c3419431"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[isd] 2010: rows=13494 coverage=2010-01-01 00:00:00+00:00 -> 2010-12-31 23:51:00+00:00\n",
      "[isd] 2011: rows=13955 coverage=2011-01-01 00:00:00+00:00 -> 2011-12-31 23:51:00+00:00\n",
      "[isd] 2012: rows=13704 coverage=2012-01-01 00:00:00+00:00 -> 2012-12-31 23:51:00+00:00\n",
      "[isd] 2013: rows=13659 coverage=2013-01-01 00:00:00+00:00 -> 2013-12-31 23:51:00+00:00\n",
      "[isd] 2014: rows=13790 coverage=2014-01-01 00:00:00+00:00 -> 2014-12-31 23:51:00+00:00\n",
      "[isd] 2015: rows=13668 coverage=2015-01-01 00:00:00+00:00 -> 2015-12-31 23:51:00+00:00\n",
      "[isd] 2016: rows=13375 coverage=2016-01-01 00:00:00+00:00 -> 2016-12-31 23:51:00+00:00\n",
      "[isd] 2017: rows=14043 coverage=2017-01-01 00:00:00+00:00 -> 2017-12-31 23:51:00+00:00\n",
      "[isd] 2018: rows=14280 coverage=2018-01-01 00:00:00+00:00 -> 2018-12-31 23:51:00+00:00\n",
      "[isd] 2019: rows=14081 coverage=2019-01-01 00:00:00+00:00 -> 2019-12-31 23:51:00+00:00\n",
      "[isd] 2020: rows=13841 coverage=2020-01-01 00:00:00+00:00 -> 2020-12-31 23:51:00+00:00\n",
      "[isd] 2021: rows=13565 coverage=2021-01-01 00:00:00+00:00 -> 2021-12-31 23:51:00+00:00\n",
      "[isd] 2022: rows=13653 coverage=2022-01-01 00:00:00+00:00 -> 2022-12-31 23:51:00+00:00\n",
      "[isd] 2023: rows=13647 coverage=2023-01-01 00:15:00+00:00 -> 2023-12-31 23:51:00+00:00\n",
      "[isd] 2024: rows=13414 coverage=2024-01-01 00:00:00+00:00 -> 2024-12-31 23:51:00+00:00\n",
      "Wrote 15 parquet files:\n",
      "  - /Users/kcao/Documents/temp-data-pipeline/data/raw/noaa_hourly/KLGA/isd_2010.parquet\n",
      "  - /Users/kcao/Documents/temp-data-pipeline/data/raw/noaa_hourly/KLGA/isd_2011.parquet\n",
      "  - /Users/kcao/Documents/temp-data-pipeline/data/raw/noaa_hourly/KLGA/isd_2012.parquet\n",
      "  - /Users/kcao/Documents/temp-data-pipeline/data/raw/noaa_hourly/KLGA/isd_2013.parquet\n",
      "  - /Users/kcao/Documents/temp-data-pipeline/data/raw/noaa_hourly/KLGA/isd_2014.parquet\n",
      "  - /Users/kcao/Documents/temp-data-pipeline/data/raw/noaa_hourly/KLGA/isd_2015.parquet\n",
      "  - /Users/kcao/Documents/temp-data-pipeline/data/raw/noaa_hourly/KLGA/isd_2016.parquet\n",
      "  - /Users/kcao/Documents/temp-data-pipeline/data/raw/noaa_hourly/KLGA/isd_2017.parquet\n",
      "  - /Users/kcao/Documents/temp-data-pipeline/data/raw/noaa_hourly/KLGA/isd_2018.parquet\n",
      "  - /Users/kcao/Documents/temp-data-pipeline/data/raw/noaa_hourly/KLGA/isd_2019.parquet\n",
      "  - /Users/kcao/Documents/temp-data-pipeline/data/raw/noaa_hourly/KLGA/isd_2020.parquet\n",
      "  - /Users/kcao/Documents/temp-data-pipeline/data/raw/noaa_hourly/KLGA/isd_2021.parquet\n",
      "  - /Users/kcao/Documents/temp-data-pipeline/data/raw/noaa_hourly/KLGA/isd_2022.parquet\n",
      "  - /Users/kcao/Documents/temp-data-pipeline/data/raw/noaa_hourly/KLGA/isd_2023.parquet\n",
      "  - /Users/kcao/Documents/temp-data-pipeline/data/raw/noaa_hourly/KLGA/isd_2024.parquet\n"
     ]
    }
   ],
   "source": [
    "from tempdata.fetch.noaa_hourly import fetch_noaa_hourly\n",
    "\n",
    "STATION_ID = \"KLGA\"\n",
    "START_DATE = \"2010-01-01\"\n",
    "END_DATE = \"2025-01-01\"  # exclusive\n",
    "\n",
    "OUTPUT_DIR = DATA_DIR / \"raw\" / \"noaa_hourly\" / STATION_ID\n",
    "CACHE_DIR = DATA_DIR / \"cache\" / \"isd_csv\" / STATION_ID\n",
    "\n",
    "written = fetch_noaa_hourly(\n",
    "    station_id=STATION_ID,\n",
    "    start_date=START_DATE,\n",
    "    end_date=END_DATE,\n",
    "    out_dir=OUTPUT_DIR,\n",
    "    cache_dir=CACHE_DIR,\n",
    ")\n",
    "\n",
    "print(f\"Wrote {len(written)} parquet files:\")\n",
    "for path in written:\n",
    "    print(f\"  - {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4b91ad96"
   },
   "source": [
    "## Verify outputs\n",
    "\n",
    "Load one parquet file to confirm the fetch results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96884b12",
    "outputId": "5b9e7269-f77c-468d-b084-4ee71786ed93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13375 rows from 2016.parquet\n",
      "Loaded 14043 rows from 2017.parquet\n",
      "Loaded 14280 rows from 2018.parquet\n",
      "Loaded 14081 rows from 2019.parquet\n",
      "Loaded 13841 rows from 2020.parquet\n",
      "Loaded 13565 rows from 2021.parquet\n",
      "Loaded 13653 rows from 2022.parquet\n",
      "Loaded 13647 rows from 2023.parquet\n",
      "Loaded 13414 rows from 2024.parquet\n",
      "Loaded 8808 rows from 2025.parquet\n",
      "Loaded 13494 rows from isd_2010.parquet\n",
      "Loaded 13955 rows from isd_2011.parquet\n",
      "Loaded 13704 rows from isd_2012.parquet\n",
      "Loaded 13659 rows from isd_2013.parquet\n",
      "Loaded 13790 rows from isd_2014.parquet\n",
      "Loaded 13668 rows from isd_2015.parquet\n",
      "Loaded 13375 rows from isd_2016.parquet\n",
      "Loaded 14043 rows from isd_2017.parquet\n",
      "Loaded 14280 rows from isd_2018.parquet\n",
      "Loaded 14081 rows from isd_2019.parquet\n",
      "Loaded 13841 rows from isd_2020.parquet\n",
      "Loaded 13565 rows from isd_2021.parquet\n",
      "Loaded 13653 rows from isd_2022.parquet\n",
      "Loaded 13647 rows from isd_2023.parquet\n",
      "Loaded 13414 rows from isd_2024.parquet\n",
      "\n",
      "Total: 338876 rows from 25 files\n",
      "Schema validation passed\n",
      "                     ts_utc station_id       lat       lon  temp_c source  \\\n",
      "0 2016-01-01 00:00:00+00:00       KLGA  40.77944 -73.88035     7.8   noaa   \n",
      "1 2016-01-01 00:51:00+00:00       KLGA  40.77944 -73.88035     7.8   noaa   \n",
      "2 2016-01-01 01:51:00+00:00       KLGA  40.77944 -73.88035     7.2   noaa   \n",
      "3 2016-01-01 02:51:00+00:00       KLGA  40.77944 -73.88035     7.2   noaa   \n",
      "4 2016-01-01 03:00:00+00:00       KLGA  40.77944 -73.88035     7.2   noaa   \n",
      "\n",
      "   qc_flags  \n",
      "0         0  \n",
      "1         0  \n",
      "2         0  \n",
      "3         0  \n",
      "4         0  \n",
      "Date range: 2010-01-01 00:00:00+00:00 to 2025-08-26 23:51:00+00:00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tempdata.schemas import validate_hourly_obs\n",
    "\n",
    "parquet_files = sorted(OUTPUT_DIR.glob(\"*.parquet\"))\n",
    "if not parquet_files:\n",
    "    raise FileNotFoundError(f\"No parquet files found in {OUTPUT_DIR}\")\n",
    "\n",
    "# Load ALL parquet files and concatenate\n",
    "dfs = []\n",
    "for pf in parquet_files:\n",
    "    df_year = pd.read_parquet(pf)\n",
    "    dfs.append(df_year)\n",
    "    print(f\"Loaded {len(df_year)} rows from {pf.name}\")\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"\\nTotal: {len(df)} rows from {len(parquet_files)} files\")\n",
    "\n",
    "# Validate schema (will raise if invalid)\n",
    "validate_hourly_obs(df, require_unique_keys=False)\n",
    "print(\"Schema validation passed\")\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Date range: {df['ts_utc'].min()} to {df['ts_utc'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean hourly data\n",
    "\n",
    "Apply the cleaning pipeline to the fetched data:\n",
    "- Validate input schema (early fail on malformed data)\n",
    "- Sort and deduplicate by (ts_utc, station_id)\n",
    "- Flag missing temperature values\n",
    "- Flag and nullify out-of-range temperatures\n",
    "- Detect hour-to-hour spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[clean] Cleaning summary:\n",
      "  Total rows: 338876 -> 214686 (124190 duplicates removed)\n",
      "  Rows with QC flags: 5770\n",
      "    QC_MISSING_VALUE: 5769\n",
      "    QC_SPIKE_DETECTED: 1\n",
      "  Temp range (valid): -17.2C to 39.4C\n",
      "\n",
      "Cleaned DataFrame shape: (214686, 7)\n",
      "                     ts_utc station_id       lat       lon  temp_c source  \\\n",
      "0 2010-01-01 00:00:00+00:00       KLGA  40.77944 -73.88035     1.1    isd   \n",
      "1 2010-01-01 00:51:00+00:00       KLGA  40.77944 -73.88035     1.1    isd   \n",
      "2 2010-01-01 01:36:00+00:00       KLGA  40.77944 -73.88035     1.0    isd   \n",
      "3 2010-01-01 01:51:00+00:00       KLGA  40.77944 -73.88035     1.1    isd   \n",
      "4 2010-01-01 02:01:00+00:00       KLGA  40.77944 -73.88035     1.0    isd   \n",
      "\n",
      "   qc_flags  \n",
      "0         0  \n",
      "1         0  \n",
      "2         0  \n",
      "3         0  \n",
      "4         0  \n"
     ]
    }
   ],
   "source": [
    "from tempdata.clean import clean_hourly_obs\n",
    "\n",
    "# Clean the fetched data\n",
    "# This applies: deduplication, missing value flags, out-of-range handling, spike detection\n",
    "df_clean = clean_hourly_obs(df)\n",
    "\n",
    "print(f\"\\nCleaned DataFrame shape: {df_clean.shape}\")\n",
    "print(df_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5911ef1"
   },
   "source": [
    "## Aggregate to Daily Tmax\n",
    "\n",
    "Convert cleaned hourly observations to daily maximum temperature (Tmax).\n",
    "\n",
    "Key design principles:\n",
    "- **Market-aligned**: Tmax is computed per station-local calendar day, not UTC\n",
    "- **QC-aware**: Hours with `QC_OUT_OF_RANGE` are excluded from Tmax calculation\n",
    "- **Spike-inclusive**: Spike-flagged values ARE included (to avoid removing real heat spikes)\n",
    "- **Transparent**: Every day carries `coverage_hours` and propagated `qc_flags`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "310d8877",
    "outputId": "80bfcb75-9439-43eb-83d0-b8509766ffbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily Tmax schema validation passed\n",
      "\n",
      "Aggregated 214686 hourly obs -> 5718 daily records\n",
      "Date range: 2009-12-31 to 2025-08-26\n",
      "\n",
      "Daily Tmax summary:\n",
      "                 date_local  tmax_c  tmax_f  coverage_hours  qc_flags\n",
      "0 2009-12-31 00:00:00-05:00     1.1    34.0               5        17\n",
      "1 2010-01-01 00:00:00-05:00     3.9    39.0              24         1\n",
      "2 2010-01-02 00:00:00-05:00     0.0    32.0              24         1\n",
      "3 2010-01-03 00:00:00-05:00    -5.0    23.0              24         1\n",
      "4 2010-01-04 00:00:00-05:00    -0.6    30.9              24         1\n",
      "5 2010-01-05 00:00:00-05:00    -0.6    30.9              24         1\n",
      "6 2010-01-06 00:00:00-05:00     1.1    34.0              24         1\n",
      "7 2010-01-07 00:00:00-05:00     3.3    37.9              24         1\n",
      "8 2010-01-08 00:00:00-05:00     1.1    34.0              24         1\n",
      "9 2010-01-09 00:00:00-05:00    -0.6    30.9              24         1\n"
     ]
    }
   ],
   "source": [
    "from tempdata.aggregate.build_daily_tmax import build_daily_tmax\n",
    "from tempdata.schemas.daily_tmax import validate_daily_tmax\n",
    "\n",
    "# Station timezone (KLGA is in Eastern time)\n",
    "STATION_TZ = \"America/New_York\"\n",
    "\n",
    "# Build daily Tmax from cleaned hourly data\n",
    "df_daily = build_daily_tmax(df_clean, station_tz=STATION_TZ)\n",
    "\n",
    "# Validate the output schema\n",
    "validate_daily_tmax(df_daily)\n",
    "print(\"Daily Tmax schema validation passed\")\n",
    "\n",
    "print(f\"\\nAggregated {len(df_clean)} hourly obs -> {len(df_daily)} daily records\")\n",
    "print(f\"Date range: {df_daily['date_local'].min().date()} to {df_daily['date_local'].max().date()}\")\n",
    "\n",
    "print(\"\\nDaily Tmax summary:\")\n",
    "print(df_daily[[\"date_local\", \"tmax_c\", \"tmax_f\", \"coverage_hours\", \"qc_flags\"]].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage and Quality Analysis\n",
    "\n",
    "Check data quality metrics for the aggregated daily Tmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage Statistics:\n",
      "  Min coverage: 5 hours\n",
      "  Max coverage: 24 hours\n",
      "  Mean coverage: 24.0 hours\n",
      "  Days with 24h coverage: 5696\n",
      "\n",
      "QC Flag Analysis:\n",
      "  Days with QC_LOW_COVERAGE: 1\n",
      "  Days with QC_INCOMPLETE_DAY: 0\n",
      "  Days with QC_SPIKE_DETECTED: 1\n",
      "  Days with no QC issues: 19\n",
      "\n",
      "Temperature Range:\n",
      "  Min Tmax: -10.0\u00b0C (14.0\u00b0F)\n",
      "  Max Tmax: 39.4\u00b0C (102.9\u00b0F)\n",
      "  Mean Tmax: 17.6\u00b0C (63.6\u00b0F)\n"
     ]
    }
   ],
   "source": [
    "from tempdata.schemas.qc_flags import QC_LOW_COVERAGE, QC_INCOMPLETE_DAY, QC_SPIKE_DETECTED\n",
    "\n",
    "# Coverage statistics\n",
    "print(\"Coverage Statistics:\")\n",
    "print(f\"  Min coverage: {df_daily['coverage_hours'].min()} hours\")\n",
    "print(f\"  Max coverage: {df_daily['coverage_hours'].max()} hours\")\n",
    "print(f\"  Mean coverage: {df_daily['coverage_hours'].mean():.1f} hours\")\n",
    "print(f\"  Days with 24h coverage: {(df_daily['coverage_hours'] == 24).sum()}\")\n",
    "\n",
    "# QC flag breakdown\n",
    "print(\"\\nQC Flag Analysis:\")\n",
    "low_coverage_days = ((df_daily['qc_flags'] & QC_LOW_COVERAGE) != 0).sum()\n",
    "incomplete_days = ((df_daily['qc_flags'] & QC_INCOMPLETE_DAY) != 0).sum()\n",
    "spike_days = ((df_daily['qc_flags'] & QC_SPIKE_DETECTED) != 0).sum()\n",
    "\n",
    "print(f\"  Days with QC_LOW_COVERAGE: {low_coverage_days}\")\n",
    "print(f\"  Days with QC_INCOMPLETE_DAY: {incomplete_days}\")\n",
    "print(f\"  Days with QC_SPIKE_DETECTED: {spike_days}\")\n",
    "print(f\"  Days with no QC issues: {(df_daily['qc_flags'] == 0).sum()}\")\n",
    "\n",
    "# Temperature range\n",
    "print(\"\\nTemperature Range:\")\n",
    "print(f\"  Min Tmax: {df_daily['tmax_c'].min():.1f}\u00b0C ({df_daily['tmax_f'].min():.1f}\u00b0F)\")\n",
    "print(f\"  Max Tmax: {df_daily['tmax_c'].max():.1f}\u00b0C ({df_daily['tmax_f'].max():.1f}\u00b0F)\")\n",
    "print(f\"  Mean Tmax: {df_daily['tmax_c'].mean():.1f}\u00b0C ({df_daily['tmax_f'].mean():.1f}\u00b0F)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c81173fe"
   },
   "source": [
    "## Save Daily Tmax\n",
    "\n",
    "Write the daily Tmax data to parquet for downstream use (backtesting, model training, trading validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eb605429",
    "outputId": "55df9858-bba2-4163-e3c5-dc548b36e0e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[aggregate] Wrote 1 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/daily_tmax/KLGA/2009.parquet\n",
      "[aggregate] Wrote 365 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/daily_tmax/KLGA/2010.parquet\n",
      "[aggregate] Wrote 365 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/daily_tmax/KLGA/2011.parquet\n",
      "[aggregate] Wrote 366 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/daily_tmax/KLGA/2012.parquet\n",
      "[aggregate] Wrote 365 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/daily_tmax/KLGA/2013.parquet\n",
      "[aggregate] Wrote 365 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/daily_tmax/KLGA/2014.parquet\n",
      "[aggregate] Wrote 365 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/daily_tmax/KLGA/2015.parquet\n",
      "[aggregate] Wrote 366 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/daily_tmax/KLGA/2016.parquet\n",
      "[aggregate] Wrote 365 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/daily_tmax/KLGA/2017.parquet\n",
      "[aggregate] Wrote 365 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/daily_tmax/KLGA/2018.parquet\n",
      "[aggregate] Wrote 365 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/daily_tmax/KLGA/2019.parquet\n",
      "[aggregate] Wrote 366 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/daily_tmax/KLGA/2020.parquet\n",
      "[aggregate] Wrote 365 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/daily_tmax/KLGA/2021.parquet\n",
      "[aggregate] Wrote 365 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/daily_tmax/KLGA/2022.parquet\n",
      "[aggregate] Wrote 365 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/daily_tmax/KLGA/2023.parquet\n",
      "[aggregate] Wrote 366 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/daily_tmax/KLGA/2024.parquet\n",
      "[aggregate] Wrote 238 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/daily_tmax/KLGA/2025.parquet\n",
      "[clean] Wrote 13481 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/hourly_obs/KLGA/2010.parquet\n",
      "[clean] Wrote 13942 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/hourly_obs/KLGA/2011.parquet\n",
      "[clean] Wrote 13690 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/hourly_obs/KLGA/2012.parquet\n",
      "[clean] Wrote 13639 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/hourly_obs/KLGA/2013.parquet\n",
      "[clean] Wrote 13771 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/hourly_obs/KLGA/2014.parquet\n",
      "[clean] Wrote 13645 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/hourly_obs/KLGA/2015.parquet\n",
      "[clean] Wrote 13359 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/hourly_obs/KLGA/2016.parquet\n",
      "[clean] Wrote 14020 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/hourly_obs/KLGA/2017.parquet\n",
      "[clean] Wrote 14262 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/hourly_obs/KLGA/2018.parquet\n",
      "[clean] Wrote 14061 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/hourly_obs/KLGA/2019.parquet\n",
      "[clean] Wrote 13819 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/hourly_obs/KLGA/2020.parquet\n",
      "[clean] Wrote 13547 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/hourly_obs/KLGA/2021.parquet\n",
      "[clean] Wrote 13637 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/hourly_obs/KLGA/2022.parquet\n",
      "[clean] Wrote 13624 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/hourly_obs/KLGA/2023.parquet\n",
      "[clean] Wrote 13394 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/hourly_obs/KLGA/2024.parquet\n",
      "[clean] Wrote 8795 rows to /Users/kcao/Documents/temp-data-pipeline/data/clean/hourly_obs/KLGA/2025.parquet\n",
      "\n",
      "Pipeline complete!\n",
      "  Daily Tmax: /Users/kcao/Documents/temp-data-pipeline/data/clean/daily_tmax/KLGA\n",
      "  Cleaned hourly: /Users/kcao/Documents/temp-data-pipeline/data/clean/hourly_obs/KLGA\n"
     ]
    }
   ],
   "source": [
    "from tempdata.aggregate.build_daily_tmax import write_daily_tmax\n",
    "\n",
    "# Output paths - partition daily Tmax by year like hourly data\n",
    "DAILY_TMAX_DIR = DATA_DIR / \"clean\" / \"daily_tmax\" / STATION_ID\n",
    "DAILY_TMAX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Determine year range for partitioning\n",
    "years = df_daily[\"date_local\"].dt.year.unique()\n",
    "for year in years:\n",
    "    year_df = df_daily[df_daily[\"date_local\"].dt.year == year]\n",
    "    year_path = DAILY_TMAX_DIR / f\"{year}.parquet\"\n",
    "    year_df.to_parquet(year_path, index=False)\n",
    "    print(f\"[aggregate] Wrote {len(year_df)} rows to {year_path}\")\n",
    "\n",
    "# Also save cleaned hourly data for reference\n",
    "HOURLY_CLEAN_DIR = DATA_DIR / \"clean\" / \"hourly_obs\" / STATION_ID\n",
    "HOURLY_CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Determine year range for partitioning\n",
    "years = df_clean[\"ts_utc\"].dt.year.unique()\n",
    "for year in years:\n",
    "    year_df = df_clean[df_clean[\"ts_utc\"].dt.year == year]\n",
    "    year_path = HOURLY_CLEAN_DIR / f\"{year}.parquet\"\n",
    "    year_df.to_parquet(year_path, index=False)\n",
    "    print(f\"[clean] Wrote {len(year_df)} rows to {year_path}\")\n",
    "\n",
    "print(f\"\\nPipeline complete!\")\n",
    "print(f\"  Daily Tmax: {DAILY_TMAX_DIR}\")\n",
    "print(f\"  Cleaned hourly: {HOURLY_CLEAN_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Saved Data\n",
    "\n",
    "Reload the saved parquet to confirm it was written correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 365 rows from 2023.parquet\n",
      "Loaded 365 rows from 2015.parquet\n",
      "Loaded 365 rows from 2014.parquet\n",
      "Loaded 365 rows from 2022.parquet\n",
      "Loaded 366 rows from 2016.parquet\n",
      "Loaded 366 rows from 2020.parquet\n",
      "Loaded 365 rows from 2021.parquet\n",
      "Loaded 365 rows from 2017.parquet\n",
      "Loaded 366 rows from 2012.parquet\n",
      "Loaded 366 rows from 2024.parquet\n",
      "Loaded 238 rows from 2025.parquet\n",
      "Loaded 365 rows from 2013.parquet\n",
      "Loaded 365 rows from 2018.parquet\n",
      "Loaded 365 rows from 2011.parquet\n",
      "Loaded 365 rows from 2010.parquet\n",
      "Loaded 1 rows from 2009.parquet\n",
      "Loaded 365 rows from 2019.parquet\n",
      "\n",
      "Total: 5718 daily records from 17 files\n",
      "Schema validation passed\n",
      "\n",
      "Daily Tmax Data (first 10 rows):\n",
      "               date_local station_id  tmax_c  tmax_f  coverage_hours   source  qc_flags                   updated_at_utc\n",
      "2023-01-01 00:00:00-05:00       KLGA    12.2    54.0              24 noaa_isd         1 2026-01-21 17:37:26.131952+00:00\n",
      "2023-01-02 00:00:00-05:00       KLGA    13.3    55.9              24 noaa_isd         1 2026-01-21 17:37:26.131952+00:00\n",
      "2023-01-03 00:00:00-05:00       KLGA    12.8    55.0              24 noaa_isd         1 2026-01-21 17:37:26.131952+00:00\n",
      "2023-01-04 00:00:00-05:00       KLGA    18.3    64.9              24 noaa_isd         1 2026-01-21 17:37:26.131952+00:00\n",
      "2023-01-05 00:00:00-05:00       KLGA     8.3    46.9              24 noaa_isd         1 2026-01-21 17:37:26.131952+00:00\n",
      "2023-01-06 00:00:00-05:00       KLGA     8.9    48.0              24 noaa_isd         1 2026-01-21 17:37:26.131952+00:00\n",
      "2023-01-07 00:00:00-05:00       KLGA     6.7    44.1              24 noaa_isd         1 2026-01-21 17:37:26.131952+00:00\n",
      "2023-01-08 00:00:00-05:00       KLGA     4.4    39.9              24 noaa_isd         1 2026-01-21 17:37:26.131952+00:00\n",
      "2023-01-09 00:00:00-05:00       KLGA     6.7    44.1              24 noaa_isd         1 2026-01-21 17:37:26.131952+00:00\n",
      "2023-01-10 00:00:00-05:00       KLGA     5.0    41.0              24 noaa_isd         1 2026-01-21 17:37:26.131952+00:00\n"
     ]
    }
   ],
   "source": [
    "# Reload and verify the saved daily Tmax data (partitioned by year)\n",
    "daily_tmax_files = list(DAILY_TMAX_DIR.glob(\"*.parquet\"))\n",
    "daily_tmax_dfs = []\n",
    "for f in daily_tmax_files:\n",
    "    df_year = pd.read_parquet(f)\n",
    "    daily_tmax_dfs.append(df_year)\n",
    "    print(f\"Loaded {len(df_year)} rows from {f.name}\")\n",
    "\n",
    "df_verify = pd.concat(daily_tmax_dfs, ignore_index=True)\n",
    "print(f\"\\nTotal: {len(df_verify)} daily records from {len(daily_tmax_files)} files\")\n",
    "\n",
    "# Validate schema\n",
    "validate_daily_tmax(df_verify)\n",
    "print(\"Schema validation passed\")\n",
    "\n",
    "# Show sample of dataset\n",
    "print(\"\\nDaily Tmax Data (first 10 rows):\")\n",
    "print(df_verify.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Open-Meteo Historical Forecasts\n",
    "\n",
    "Ingest **historical** daily Tmax forecasts from Open-Meteo for the same station and date range as the truth data.\n",
    "\n",
    "This creates the **feature-side** dataset: \"What did the forecast say at issue time about a target local date?\"\n",
    "\n",
    "Key concepts:\n",
    "- **Issue time**: when the forecast was issued (simulated as midnight UTC of the day before target)\n",
    "- **Target date**: the station-local calendar date being forecasted\n",
    "- **Lead hours**: hours from issue time to target date midnight in station timezone\n",
    "\n",
    "Using historical forecasts allows us to join forecasts to truth data for model training and backtesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch ERA5 Data (Deep Historical)\n",
    "\n",
    "For dates before 2016, use ERA5 reanalysis data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching ERA5 data for 2010-01-01 to 2016-01-01\n",
      "[era5] 2010: fetching full year...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-21 11:37:27,827 INFO [2025-12-11T00:00:00] Please note that a dedicated catalogue entry for this dataset, post-processed and stored in Analysis Ready Cloud Optimized (ARCO) format (Zarr), is available for optimised time-series retrievals (i.e. for retrieving data from selected variables for a single point over an extended period of time in an efficient way). You can discover it [here](https://cds.climate.copernicus.eu/datasets/reanalysis-era5-single-levels-timeseries?tab=overview)\n",
      "2026-01-21 11:37:27,828 INFO Request ID is a90b6b6e-5083-4793-8873-ea401274adef\n",
      "2026-01-21 11:37:28,003 INFO status has been updated to accepted\n",
      "2026-01-21 11:37:42,073 INFO status has been updated to running\n",
      "2026-01-21 11:47:54,399 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9359514c024b45d9b5a67bf5fd353cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1fdc087550656c98380dc0c7acf13853.nc:   0%|          | 0.00/1.02M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[era5] 2010: wrote 8760 rows -> /Users/kcao/Documents/temp-data-pipeline/data/raw/era5/KLGA/era5_2010.parquet\n",
      "[era5] 2011: fetching full year...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-21 11:47:59,201 INFO [2025-12-11T00:00:00] Please note that a dedicated catalogue entry for this dataset, post-processed and stored in Analysis Ready Cloud Optimized (ARCO) format (Zarr), is available for optimised time-series retrievals (i.e. for retrieving data from selected variables for a single point over an extended period of time in an efficient way). You can discover it [here](https://cds.climate.copernicus.eu/datasets/reanalysis-era5-single-levels-timeseries?tab=overview)\n",
      "2026-01-21 11:47:59,203 INFO Request ID is 84beb07c-4401-4d0d-9d33-b386c9f4b165\n",
      "2026-01-21 11:47:59,404 INFO status has been updated to accepted\n",
      "2026-01-21 11:48:08,449 INFO status has been updated to running\n",
      "2026-01-21 12:00:23,933 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a55cad1e3f24396958842e83647f723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "6f71ecf4f417d0c237b3bdffea3d1f9.nc:   0%|          | 0.00/1.02M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[era5] 2011: wrote 8760 rows -> /Users/kcao/Documents/temp-data-pipeline/data/raw/era5/KLGA/era5_2011.parquet\n",
      "[era5] 2012: fetching full year...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-21 12:00:35,598 INFO [2025-12-11T00:00:00] Please note that a dedicated catalogue entry for this dataset, post-processed and stored in Analysis Ready Cloud Optimized (ARCO) format (Zarr), is available for optimised time-series retrievals (i.e. for retrieving data from selected variables for a single point over an extended period of time in an efficient way). You can discover it [here](https://cds.climate.copernicus.eu/datasets/reanalysis-era5-single-levels-timeseries?tab=overview)\n",
      "2026-01-21 12:00:35,599 INFO Request ID is 5084c997-cd2b-4e7d-b64a-4d65d6d5ca35\n",
      "2026-01-21 12:00:35,778 INFO status has been updated to accepted\n",
      "2026-01-21 12:00:50,072 INFO status has been updated to running\n",
      "2026-01-21 12:11:03,860 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd0f5c995f74c9c93f3db9c0877b21b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "7573ff022761d8f448066e28f846a569.nc:   0%|          | 0.00/1.02M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[era5] 2012: wrote 8784 rows -> /Users/kcao/Documents/temp-data-pipeline/data/raw/era5/KLGA/era5_2012.parquet\n",
      "[era5] 2013: fetching full year...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-21 12:11:06,838 INFO [2025-12-11T00:00:00] Please note that a dedicated catalogue entry for this dataset, post-processed and stored in Analysis Ready Cloud Optimized (ARCO) format (Zarr), is available for optimised time-series retrievals (i.e. for retrieving data from selected variables for a single point over an extended period of time in an efficient way). You can discover it [here](https://cds.climate.copernicus.eu/datasets/reanalysis-era5-single-levels-timeseries?tab=overview)\n",
      "2026-01-21 12:11:06,839 INFO Request ID is ced1c057-3b26-407c-9319-058f31b4b32c\n",
      "2026-01-21 12:11:07,025 INFO status has been updated to accepted\n",
      "2026-01-21 12:11:15,927 INFO status has been updated to running\n",
      "Recovering from connection error [('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))], attempt 1 of 500\n",
      "Retrying in 120 seconds\n",
      "Recovering from connection error [HTTPSConnectionPool(host='cds.climate.copernicus.eu', port=443): Max retries exceeded with url: /api/retrieve/v1/jobs/ced1c057-3b26-407c-9319-058f31b4b32c?log=True&request=True (Caused by NameResolutionError(\"HTTPSConnection(host='cds.climate.copernicus.eu', port=443): Failed to resolve 'cds.climate.copernicus.eu' ([Errno 8] nodename nor servname provided, or not known)\"))], attempt 2 of 500\n",
      "Retrying in 120 seconds\n",
      "Recovering from connection error [HTTPSConnectionPool(host='cds.climate.copernicus.eu', port=443): Max retries exceeded with url: /api/retrieve/v1/jobs/ced1c057-3b26-407c-9319-058f31b4b32c?log=True&request=True (Caused by NameResolutionError(\"HTTPSConnection(host='cds.climate.copernicus.eu', port=443): Failed to resolve 'cds.climate.copernicus.eu' ([Errno 8] nodename nor servname provided, or not known)\"))], attempt 3 of 500\n",
      "Retrying in 120 seconds\n",
      "2026-01-21 12:57:00,500 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f14fac78b69400da32c1b17e2df5f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "9df9e41cdc70afaf89e88d36ce4482a3.nc:   0%|          | 0.00/1.02M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recovering from connection error [HTTPSConnectionPool(host='object-store.os-api.cci2.ecmwf.int', port=443): Read timed out. (read timeout=60)], attempt 1 of 500\n",
      "Retrying in 120 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284206f84e8b4bed869054b0c98e2dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "9df9e41cdc70afaf89e88d36ce4482a3.nc:   0%|          | 0.00/1.02M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[era5] 2013: wrote 8760 rows -> /Users/kcao/Documents/temp-data-pipeline/data/raw/era5/KLGA/era5_2013.parquet\n",
      "[era5] 2014: fetching full year...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recovering from connection error [HTTPSConnectionPool(host='cds.climate.copernicus.eu', port=443): Read timed out. (read timeout=60)], attempt 1 of 500\n",
      "Retrying in 120 seconds\n",
      "2026-01-21 13:49:35,662 INFO [2025-12-11T00:00:00] Please note that a dedicated catalogue entry for this dataset, post-processed and stored in Analysis Ready Cloud Optimized (ARCO) format (Zarr), is available for optimised time-series retrievals (i.e. for retrieving data from selected variables for a single point over an extended period of time in an efficient way). You can discover it [here](https://cds.climate.copernicus.eu/datasets/reanalysis-era5-single-levels-timeseries?tab=overview)\n",
      "2026-01-21 13:49:35,664 INFO Request ID is 3ea7ab89-ef7c-42d8-88a9-c6c459f8b2da\n",
      "2026-01-21 13:49:35,841 INFO status has been updated to accepted\n",
      "2026-01-21 13:49:49,456 INFO status has been updated to running\n",
      "Recovering from connection error [HTTPSConnectionPool(host='cds.climate.copernicus.eu', port=443): Read timed out. (read timeout=60)], attempt 1 of 500\n",
      "Retrying in 120 seconds\n",
      "2026-01-21 14:20:36,962 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908aef6fca844d4299e5d1d7da4d2aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aaa5e4d3880eecd97408ff09ed9a0b58.nc:   0%|          | 0.00/1.02M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[era5] 2014: wrote 8760 rows -> /Users/kcao/Documents/temp-data-pipeline/data/raw/era5/KLGA/era5_2014.parquet\n",
      "[era5] 2015: using cached /Users/kcao/Documents/temp-data-pipeline/data/raw/era5/KLGA/era5_2015.parquet\n",
      "Wrote 6 ERA5 parquet files\n"
     ]
    }
   ],
   "source": [
    "from tempdata.fetch.era5_hourly import fetch_era5_hourly\n",
    "import pandas as pd\n",
    "\n",
    "ERA5_CUTOFF = \"2016-01-01\"\n",
    "if pd.Timestamp(START_DATE) < pd.Timestamp(ERA5_CUTOFF):\n",
    "    era5_end = min(pd.Timestamp(END_DATE), pd.Timestamp(ERA5_CUTOFF)).strftime(\"%Y-%m-%d\")\n",
    "    print(f\"Fetching ERA5 data for {START_DATE} to {era5_end}\")\n",
    "\n",
    "    try:\n",
    "        era5_files = fetch_era5_hourly(\n",
    "            station_id=STATION_ID,\n",
    "            start_date=START_DATE,\n",
    "            end_date=era5_end,\n",
    "            out_dir=DATA_DIR / \"raw\" / \"era5\" / STATION_ID,\n",
    "        )\n",
    "        print(f\"Wrote {len(era5_files)} ERA5 parquet files\")\n",
    "    except ImportError:\n",
    "        print(\"Skipping ERA5: dependencies not installed (run setup cells above)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping ERA5: {e}\")\n",
    "else:\n",
    "    print(\"No ERA5 data needed (start date >= 2016)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[notebook] Adjusting Open-Meteo start date from 2010-01-01 to 2016-01-01 (API limit)\n",
      "Fetching historical forecasts for KLGA\n",
      "Date range: 2016-01-01 to 2024-12-31\n",
      "\n",
      "Wrote 2 files:\n",
      "  - /Users/kcao/Documents/temp-data-pipeline/data/raw/forecasts/openmeteo/KLGA/historical_2016-01-01_to_2024-12-31.json\n",
      "  - /Users/kcao/Documents/temp-data-pipeline/data/clean/forecasts/openmeteo/KLGA/historical_2016-01-01_to_2024-12-31.parquet\n"
     ]
    }
   ],
   "source": [
    "from tempdata.fetch.openmeteo_daily_forecast import fetch_openmeteo_historical_forecasts\n",
    "from tempdata.schemas.daily_tmax_forecast import validate_daily_tmax_forecast\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# Use the same date range as the truth data (from NOAA fetch)\n",
    "OPENMETEO_MIN_DATE = \"2016-01-01\"\n",
    "if pd.Timestamp(START_DATE) < pd.Timestamp(OPENMETEO_MIN_DATE):\n",
    "    print(f\"[notebook] Adjusting Open-Meteo start date from {START_DATE} to {OPENMETEO_MIN_DATE} (API limit)\")\n",
    "    FORECAST_START_DATE = OPENMETEO_MIN_DATE\n",
    "else:\n",
    "    FORECAST_START_DATE = START_DATE\n",
    "\n",
    "FORECAST_END_DATE = END_DATE\n",
    "\n",
    "# Adjust end date: NOAA uses exclusive end, Open-Meteo uses inclusive\n",
    "end_dt = datetime.strptime(FORECAST_END_DATE, \"%Y-%m-%d\") - timedelta(days=1)\n",
    "forecast_end_date = end_dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "if pd.Timestamp(FORECAST_START_DATE) <= pd.Timestamp(forecast_end_date):\n",
    "    print(f\"Fetching historical forecasts for {STATION_ID}\")\n",
    "    print(f\"Date range: {FORECAST_START_DATE} to {forecast_end_date}\")\n",
    "\n",
    "    # Output directories\n",
    "    FORECAST_RAW_DIR = DATA_DIR / \"raw\" / \"forecasts\" / \"openmeteo\" / STATION_ID\n",
    "    FORECAST_CLEAN_DIR = DATA_DIR / \"clean\" / \"forecasts\" / \"openmeteo\" / STATION_ID\n",
    "\n",
    "    forecast_files, df_forecast_om = fetch_openmeteo_historical_forecasts(\n",
    "        station_id=STATION_ID,\n",
    "        start_date=FORECAST_START_DATE,\n",
    "        end_date=forecast_end_date,\n",
    "        out_raw_dir=FORECAST_RAW_DIR,\n",
    "        out_parquet_dir=FORECAST_CLEAN_DIR,\n",
    "        write_raw=True,  # Save raw JSON for debugging\n",
    "    )\n",
    "\n",
    "    print(f\"\\nWrote {len(forecast_files)} files:\")\n",
    "    for path in forecast_files:\n",
    "        print(f\"  - {path}\")\n",
    "else:\n",
    "    print(\"Open-Meteo forecast range is empty (dates are pre-2016)\")\n",
    "    df_forecast_om = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 54000 hourly ERA5 rows\n",
      "Aggregated ERA5 to 2192 daily Tmax records\n",
      "Synthesizing ERA5 forecasts for lead hours: [np.int64(28), np.int64(29)]\n",
      "Added 4384 synthentic ERA5 forecast rows\n",
      "Loaded 2558 Open-Meteo forecast rows\n",
      "Final df_forecast: 6942 rows\n"
     ]
    }
   ],
   "source": [
    "# Consolidate forecasts (ERA5 + Open-Meteo)\n",
    "from tempdata.aggregate.build_daily_tmax import build_daily_tmax\n",
    "from datetime import timezone\n",
    "\n",
    "forecast_dfs = []\n",
    "\n",
    "# 1. Add ERA5 data if available (as perfect forecast proxy)\n",
    "era5_dir = DATA_DIR / \"raw\" / \"era5\" / STATION_ID\n",
    "if era5_dir.exists():\n",
    "    era5_files = list(era5_dir.glob(\"*.parquet\"))\n",
    "    if era5_files:\n",
    "        # Load ERA5 (hourly)\n",
    "        df_era5_hourly = pd.concat([pd.read_parquet(f) for f in era5_files])\n",
    "        print(f\"Loaded {len(df_era5_hourly)} hourly ERA5 rows\")\n",
    "\n",
    "        # Aggregate to Daily Tmax\n",
    "        # Note: We rely on STATION_TZ being defined in previous cells (Cell 32)\n",
    "        if 'STATION_TZ' not in locals():\n",
    "            STATION_TZ = \"America/New_York\"\n",
    "            print(f\"Warning: STATION_TZ was not defined, defaulting to {STATION_TZ}\")\n",
    "\n",
    "        df_era5_daily = build_daily_tmax(df_era5_hourly, station_tz=STATION_TZ)\n",
    "        print(f\"Aggregated ERA5 to {len(df_era5_daily)} daily Tmax records\")\n",
    "\n",
    "        # Transform to Forecast Schema\n",
    "        # We need to replicate the 'lead_hours' structure from the actual forecasts\n",
    "        # to ensure the model sees consistent features.\n",
    "\n",
    "        # Get lead hours from Open-Meteo data if available, otherwise default to [28, 29]\n",
    "        if 'df_forecast_om' in locals() and not df_forecast_om.empty:\n",
    "            lead_hours_list = sorted(df_forecast_om['lead_hours'].unique())\n",
    "        else:\n",
    "            lead_hours_list = [28, 29] # Default for daily tmax (issue ~midnight UTC prev day)\n",
    "\n",
    "        print(f\"Synthesizing ERA5 forecasts for lead hours: {lead_hours_list}\")\n",
    "\n",
    "        era5_forecasts = []\n",
    "        now_utc = pd.Timestamp.now(tz=timezone.utc)\n",
    "\n",
    "        # Get station metadata from first row/file if possible, or use STATION_ID\n",
    "        # The hourly data has lat/lon\n",
    "        lat = df_era5_hourly['lat'].iloc[0] if not df_era5_hourly.empty else 0.0\n",
    "        lon = df_era5_hourly['lon'].iloc[0] if not df_era5_hourly.empty else 0.0\n",
    "\n",
    "        for lead in lead_hours_list:\n",
    "            # Create a copy for this lead time\n",
    "            df_lead = df_era5_daily.copy()\n",
    "\n",
    "            # Map columns\n",
    "            # daily_tmax has: date_local, tmax_c, tmax_f, coverage_hours, qc_flags, source, updated_at_utc\n",
    "            # forecast has: station_id, lat, lon, issue_time_utc, target_date_local, tmax_pred_c, tmax_pred_f, lead_hours, model, source, ingested_at_utc\n",
    "\n",
    "            df_lead['station_id'] = STATION_ID\n",
    "            df_lead['lat'] = lat\n",
    "            df_lead['lon'] = lon\n",
    "            df_lead['target_date_local'] = df_lead['date_local'].dt.tz_localize(None) # Remove tz for schema\n",
    "\n",
    "            # Calculate issue time: target_date (midnight local) - lead_hours\n",
    "            # We need to be careful with timezones.\n",
    "            # The simple approximation:\n",
    "            # target_date_local (as UTC timestamp representation) - lead hours?\n",
    "            # No, standard is issue_time_utc.\n",
    "            # Let's reverse engineering: issue_time + lead_hours = target_date (in station tz)\n",
    "            # So issue_time = target_date (station tz) - lead_hours\n",
    "\n",
    "            # df_lead['date_local'] is tz-aware (STATION_TZ)\n",
    "            df_lead['issue_time_utc'] = df_lead['date_local'] - pd.to_timedelta(lead, unit='h')\n",
    "            df_lead['issue_time_utc'] = df_lead['issue_time_utc'].dt.tz_convert('UTC')\n",
    "\n",
    "            df_lead['tmax_pred_c'] = df_lead['tmax_c']\n",
    "            df_lead['tmax_pred_f'] = df_lead['tmax_f']\n",
    "            df_lead['lead_hours'] = lead\n",
    "            df_lead['model'] = 'era5'\n",
    "            df_lead['source'] = 'era5' # Overwrite 'noaa_isd' from aggregation\n",
    "            df_lead['ingested_at_utc'] = now_utc\n",
    "\n",
    "            # Select columns\n",
    "            from tempdata.schemas.daily_tmax_forecast import DAILY_TMAX_FORECAST_FIELDS\n",
    "            era5_forecasts.append(df_lead[DAILY_TMAX_FORECAST_FIELDS])\n",
    "\n",
    "        if era5_forecasts:\n",
    "            df_era5_final = pd.concat(era5_forecasts, ignore_index=True)\n",
    "            forecast_dfs.append(df_era5_final)\n",
    "            print(f\"Added {len(df_era5_final)} synthentic ERA5 forecast rows\")\n",
    "\n",
    "# 2. Add Open-Meteo data\n",
    "if 'df_forecast_om' in locals() and not df_forecast_om.empty:\n",
    "    forecast_dfs.append(df_forecast_om)\n",
    "    print(f\"Loaded {len(df_forecast_om)} Open-Meteo forecast rows\")\n",
    "\n",
    "if forecast_dfs:\n",
    "    df_forecast = pd.concat(forecast_dfs, ignore_index=True)\n",
    "    # Sort for tidiness\n",
    "    df_forecast.sort_values(['station_id', 'target_date_local', 'lead_hours'], inplace=True)\n",
    "else:\n",
    "    print(\"Warning: No forecast data found. Creating empty DataFrame.\")\n",
    "    from tempdata.schemas.daily_tmax_forecast import DAILY_TMAX_FORECAST_FIELDS\n",
    "    df_forecast = pd.DataFrame(columns=DAILY_TMAX_FORECAST_FIELDS)\n",
    "\n",
    "print(f\"Final df_forecast: {len(df_forecast)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Forecast Data\n",
    "\n",
    "Load and validate the forecast parquet, then display a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6942 forecast rows\n",
      "Schema validation passed\n",
      "\n",
      "Forecast Summary:\n",
      "  Target dates: 2009-12-31 to 2024-12-31\n",
      "  Lead hours range: 28 to 29\n",
      "  Tmax (C): -12.1 to 38.2\n",
      "  Tmax (F): 10.2 to 100.8\n",
      "\n",
      "Forecast Data (first 10 rows):\n",
      "target_date_local  tmax_pred_c  tmax_pred_f  lead_hours\n",
      "       2009-12-31     0.515778         32.9          28\n",
      "       2009-12-31     0.515778         32.9          29\n",
      "       2010-01-01     3.700043         38.7          28\n",
      "       2010-01-01     3.700043         38.7          29\n",
      "       2010-01-02     1.040802         33.9          28\n",
      "       2010-01-02     1.040802         33.9          29\n",
      "       2010-01-03    -5.446259         22.2          28\n",
      "       2010-01-03    -5.446259         22.2          29\n",
      "       2010-01-04    -0.885223         30.4          28\n",
      "       2010-01-04    -0.885223         30.4          29\n"
     ]
    }
   ],
   "source": [
    "# df_forecast is already returned from fetch_openmeteo_historical_forecasts\n",
    "print(f\"Loaded {len(df_forecast)} forecast rows\")\n",
    "\n",
    "# Validate schema (already validated in fetch, but double-check)\n",
    "validate_daily_tmax_forecast(df_forecast)\n",
    "print(\"Schema validation passed\")\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\nForecast Summary:\")\n",
    "print(f\"  Target dates: {df_forecast['target_date_local'].min().date()} to {df_forecast['target_date_local'].max().date()}\")\n",
    "print(f\"  Lead hours range: {df_forecast['lead_hours'].min()} to {df_forecast['lead_hours'].max()}\")\n",
    "print(f\"  Tmax (C): {df_forecast['tmax_pred_c'].min():.1f} to {df_forecast['tmax_pred_c'].max():.1f}\")\n",
    "print(f\"  Tmax (F): {df_forecast['tmax_pred_f'].min():.1f} to {df_forecast['tmax_pred_f'].max():.1f}\")\n",
    "\n",
    "print(\"\\nForecast Data (first 10 rows):\")\n",
    "print(df_forecast[[\"target_date_local\", \"tmax_pred_c\", \"tmax_pred_f\", \"lead_hours\"]].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering for Daily Tmax\n",
    "\n",
    "Transform forecasts and truth data into a **model-ready training dataset**.\n",
    "\n",
    "This section uses:\n",
    "- **Real truth data** (`df_daily`) from the NOAA aggregation step above\n",
    "- **Real historical forecasts** (`df_forecast`) from the Open-Meteo fetch above\n",
    "\n",
    "The feature engineering pipeline:\n",
    "1. Joins forecasts to truth on `(station_id, target_date_local)`\n",
    "2. Filters low-quality truth days by coverage\n",
    "3. Adds seasonal encodings: `sin_doy`, `cos_doy`, `month`\n",
    "4. Computes rolling bias/error statistics: `bias_7d`, `bias_14d`, `bias_30d`, `rmse_14d`, `rmse_30d`, `sigma_lead`\n",
    "\n",
    "All rolling features use `.shift(1)` to ensure **no lookahead** \u2014 each row's features are computed only from prior data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real truth data: 5718 days\n",
      "  Date range: 2009-12-31 to 2025-08-26\n",
      "\n",
      "Using real Open-Meteo historical forecasts:\n",
      "  Forecast rows: 6942\n",
      "  Lead times: [np.int64(28), np.int64(29)] hours\n",
      "  Target date range: 2009-12-31 to 2024-12-31\n"
     ]
    }
   ],
   "source": [
    "# Use real data from previous cells:\n",
    "# - df_daily: NOAA observations aggregated to daily Tmax (truth)\n",
    "# - df_forecast: Open-Meteo historical forecasts\n",
    "\n",
    "# Prepare truth data for feature engineering\n",
    "df_truth_for_features = df_daily.copy()\n",
    "\n",
    "print(f\"Using real truth data: {len(df_truth_for_features)} days\")\n",
    "print(f\"  Date range: {df_truth_for_features['date_local'].min().date()} to {df_truth_for_features['date_local'].max().date()}\")\n",
    "\n",
    "# Use the real Open-Meteo historical forecasts\n",
    "df_forecast_for_features = df_forecast.copy()\n",
    "\n",
    "print(f\"\\nUsing real Open-Meteo historical forecasts:\")\n",
    "print(f\"  Forecast rows: {len(df_forecast_for_features)}\")\n",
    "print(f\"  Lead times: {sorted(df_forecast_for_features['lead_hours'].unique())} hours\")\n",
    "print(f\"  Target date range: {df_forecast_for_features['target_date_local'].min().date()} to {df_forecast_for_features['target_date_local'].max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Training Dataset\n",
    "\n",
    "Run the feature engineering pipeline to create model-ready features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset: 6940 rows\n",
      "Columns: ['station_id', 'issue_time_utc', 'target_date_local', 'tmax_pred_f', 'lead_hours', 'forecast_source', 'sin_doy', 'cos_doy', 'month', 'bias_7d', 'bias_14d', 'bias_30d', 'rmse_14d', 'rmse_30d', 'sigma_lead', 'tmax_actual_f']\n",
      "\n",
      "Column types:\n",
      "  station_id: object\n",
      "  issue_time_utc: datetime64[ns, UTC]\n",
      "  target_date_local: datetime64[ns]\n",
      "  tmax_pred_f: float64\n",
      "  lead_hours: int64\n",
      "  forecast_source: object\n",
      "  sin_doy: float64\n",
      "  cos_doy: float64\n",
      "  month: int32\n",
      "  bias_7d: float64\n",
      "  bias_14d: float64\n",
      "  bias_30d: float64\n",
      "  rmse_14d: float64\n",
      "  rmse_30d: float64\n",
      "  sigma_lead: float64\n",
      "  tmax_actual_f: float64\n"
     ]
    }
   ],
   "source": [
    "from tempdata.features.build_train_daily_tmax import build_train_daily_tmax\n",
    "from tempdata.schemas.train_daily_tmax import validate_train_daily_tmax, TRAIN_DAILY_TMAX_FIELDS\n",
    "\n",
    "# Build the training dataset using real truth data + real Open-Meteo forecasts\n",
    "# This performs: join, seasonal features, rolling bias/error stats, validation\n",
    "df_train = build_train_daily_tmax(\n",
    "    forecast_df=df_forecast_for_features,\n",
    "    truth_df=df_truth_for_features,\n",
    "    min_coverage_hours=18,  # Filter low-quality truth days\n",
    "    drop_warmup_nulls=False,  # Keep warm-up rows (they have NaN in rolling features)\n",
    "    validate=True,\n",
    ")\n",
    "\n",
    "print(f\"Training dataset: {len(df_train)} rows\")\n",
    "print(f\"Columns: {list(df_train.columns)}\")\n",
    "print(f\"\\nColumn types:\")\n",
    "for col in df_train.columns:\n",
    "    print(f\"  {col}: {df_train[col].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Features\n",
    "\n",
    "Examine the generated features, focusing on rolling statistics and seasonal encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core Features & Seasonal Encodings (first 10 rows):\n",
      "target_date_local  lead_hours  tmax_pred_f  tmax_actual_f  sin_doy  cos_doy  month\n",
      "       2010-01-01          28         38.7           39.0 0.017202 0.999852      1\n",
      "       2010-01-02          28         33.9           32.0 0.034398 0.999408      1\n",
      "       2010-01-03          28         22.2           23.0 0.051584 0.998669      1\n",
      "       2010-01-04          28         30.4           30.9 0.068755 0.997634      1\n",
      "       2010-01-05          28         30.3           30.9 0.085906 0.996303      1\n",
      "       2010-01-06          28         33.8           34.0 0.103031 0.994678      1\n",
      "       2010-01-07          28         38.0           37.9 0.120126 0.992759      1\n",
      "       2010-01-08          28         32.4           34.0 0.137185 0.990545      1\n",
      "       2010-01-09          28         28.9           30.9 0.154204 0.988039      1\n",
      "       2010-01-10          28         27.9           28.9 0.171177 0.985240      1\n",
      "\n",
      "\n",
      "Rolling Bias & Error Features (rows 10-20, after warm-up):\n",
      "target_date_local  lead_hours   bias_7d  bias_14d  bias_30d  rmse_14d  rmse_30d  sigma_lead\n",
      "       2010-01-11          28 -0.828571 -0.500000 -0.500000  1.120714  1.120714    1.057250\n",
      "       2010-01-12          28 -0.814286 -0.490909 -0.490909  1.075343  1.075343    1.003449\n",
      "       2010-01-13          28 -0.700000 -0.433333 -0.433333  1.031181  1.031181    0.977319\n",
      "       2010-01-14          28 -0.957143 -0.553846 -0.553846  1.135443  1.135443    1.031678\n",
      "       2010-01-15          28 -1.214286 -0.635714 -0.635714  1.184724  1.184724    1.037458\n",
      "       2010-01-16          28 -1.642857 -0.942857 -0.900000  1.705453  1.649444    1.430784\n",
      "       2010-01-17          28 -1.814286 -1.307143 -1.043750  1.839060  1.786232    1.497094\n",
      "       2010-01-18          28 -1.657143 -1.242857 -0.976471  1.826785  1.733069    1.475860\n",
      "       2010-01-19          28 -1.785714 -1.300000 -0.994444  1.854724  1.711887    1.433823\n",
      "       2010-01-20          28 -2.300000 -1.500000 -1.121053  2.059126  1.839765    1.498732\n"
     ]
    }
   ],
   "source": [
    "# Display core features and seasonal encodings\n",
    "display_cols = [\n",
    "    \"target_date_local\", \"lead_hours\", \"tmax_pred_f\", \"tmax_actual_f\",\n",
    "    \"sin_doy\", \"cos_doy\", \"month\"\n",
    "]\n",
    "print(\"Core Features & Seasonal Encodings (first 10 rows):\")\n",
    "print(df_train[display_cols].head(10).to_string(index=False))\n",
    "\n",
    "# Display rolling bias/error features\n",
    "rolling_cols = [\n",
    "    \"target_date_local\", \"lead_hours\", \"bias_7d\", \"bias_14d\", \"bias_30d\",\n",
    "    \"rmse_14d\", \"rmse_30d\", \"sigma_lead\"\n",
    "]\n",
    "print(\"\\n\\nRolling Bias & Error Features (rows 10-20, after warm-up):\")\n",
    "print(df_train[rolling_cols].iloc[10:20].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify No-Lookahead Property\n",
    "\n",
    "Confirm that rolling features are computed correctly with `.shift(1)` \u2014 each row's features should only use prior data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row per (station_id, lead_hours) group \u2014 bias_7d should be NaN:\n",
      "                       bias_7d  bias_14d  bias_30d\n",
      "station_id lead_hours                             \n",
      "KLGA       28             -0.3      -0.3      -0.3\n",
      "           29             -0.3      -0.3      -0.3\n",
      "\n",
      "\n",
      "Verification for row 10:\n",
      "  Actual residual (pred - actual): -0.40\u00b0F\n",
      "  bias_7d (computed from PRIOR 7 days): -0.83\u00b0F\n",
      "  These should be different values (bias excludes current row)\n",
      "\n",
      "  Residual == bias_7d? False\n"
     ]
    }
   ],
   "source": [
    "# Verify no-lookahead: The first row for each (station, lead_hours) group should have NaN bias\n",
    "# because there's no prior data to compute rolling stats from\n",
    "\n",
    "first_rows = df_train.groupby([\"station_id\", \"lead_hours\"]).first()\n",
    "print(\"First row per (station_id, lead_hours) group \u2014 bias_7d should be NaN:\")\n",
    "print(first_rows[[\"bias_7d\", \"bias_14d\", \"bias_30d\"]].to_string())\n",
    "\n",
    "# Compute actual residual for a specific row and verify it's NOT in its own bias\n",
    "# Pick a row after warm-up period\n",
    "if len(df_train) > 10:\n",
    "    test_idx = 10\n",
    "    test_row = df_train.iloc[test_idx]\n",
    "    actual_residual = test_row[\"tmax_pred_f\"] - test_row[\"tmax_actual_f\"]\n",
    "\n",
    "    print(f\"\\n\\nVerification for row {test_idx}:\")\n",
    "    print(f\"  Actual residual (pred - actual): {actual_residual:.2f}\u00b0F\")\n",
    "    print(f\"  bias_7d (computed from PRIOR 7 days): {test_row['bias_7d']:.2f}\u00b0F\")\n",
    "    print(f\"  These should be different values (bias excludes current row)\")\n",
    "\n",
    "    # The bias_7d should NOT equal the current residual (unless by coincidence)\n",
    "    print(f\"\\n  Residual == bias_7d? {abs(actual_residual - test_row['bias_7d']) < 0.01}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Forecast Bias by Lead Time\n",
    "\n",
    "The rolling bias features capture systematic forecast errors that vary by lead time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast Error Analysis by Lead Time:\n",
      "============================================================\n",
      "            Mean Error (\u00b0F)  Std Dev (\u00b0F)  Count  Avg bias_30d  Avg sigma_lead\n",
      "lead_hours                                                                    \n",
      "28                    -1.08          1.95   3857         -1.09            1.93\n",
      "29                    -1.42          1.87   3083         -1.42            1.93\n",
      "\n",
      "\n",
      "Key Insight:\n",
      "  - sigma_lead captures uncertainty and can be used for confidence intervals\n",
      "  - bias_30d provides a rolling estimate that adapts to recent forecast performance\n"
     ]
    }
   ],
   "source": [
    "# Compute actual residuals for analysis\n",
    "df_train[\"residual\"] = df_train[\"tmax_pred_f\"] - df_train[\"tmax_actual_f\"]\n",
    "\n",
    "# Analyze bias by lead time\n",
    "print(\"Forecast Error Analysis by Lead Time:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "bias_by_lead = df_train.groupby(\"lead_hours\").agg({\n",
    "    \"residual\": [\"mean\", \"std\", \"count\"],\n",
    "    \"bias_30d\": \"mean\",  # Average of the rolling bias feature\n",
    "    \"sigma_lead\": \"mean\",  # Average sigma_lead\n",
    "}).round(2)\n",
    "\n",
    "bias_by_lead.columns = [\"Mean Error (\u00b0F)\", \"Std Dev (\u00b0F)\", \"Count\", \"Avg bias_30d\", \"Avg sigma_lead\"]\n",
    "print(bias_by_lead.to_string())\n",
    "\n",
    "print(\"\\n\\nKey Insight:\")\n",
    "print(\"  - sigma_lead captures uncertainty and can be used for confidence intervals\")\n",
    "print(\"  - bias_30d provides a rolling estimate that adapts to recent forecast performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Training Dataset\n",
    "\n",
    "Write the feature-engineered training dataset to parquet for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after dropping warm-up NaNs: 6936 (was 6940)\n",
      "[features] wrote 6936 rows to /Users/kcao/Documents/temp-data-pipeline/data/train/daily_tmax/KLGA/train_daily_tmax.parquet\n",
      "\n",
      "Training dataset saved to: /Users/kcao/Documents/temp-data-pipeline/data/train/daily_tmax/KLGA/train_daily_tmax.parquet\n"
     ]
    }
   ],
   "source": [
    "from tempdata.features.build_train_daily_tmax import write_train_daily_tmax\n",
    "\n",
    "# Create output directory\n",
    "TRAIN_DIR = DATA_DIR / \"train\" / \"daily_tmax\" / STATION_ID\n",
    "TRAIN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Drop rows with NaN rolling features for clean training data\n",
    "df_train_clean = df_train.dropna(subset=[\"bias_7d\", \"bias_14d\", \"bias_30d\", \"rmse_14d\", \"rmse_30d\", \"sigma_lead\"])\n",
    "print(f\"Rows after dropping warm-up NaNs: {len(df_train_clean)} (was {len(df_train)})\")\n",
    "\n",
    "# Select only the schema columns (drop residual which was for analysis)\n",
    "df_train_final = df_train_clean[TRAIN_DAILY_TMAX_FIELDS].copy()\n",
    "\n",
    "# Write to parquet\n",
    "output_path = TRAIN_DIR / \"train_daily_tmax.parquet\"\n",
    "write_train_daily_tmax(df_train_final, output_path)\n",
    "\n",
    "print(f\"\\nTraining dataset saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Summary\n",
    "\n",
    "The `train_daily_tmax` dataset is now ready for model training with:\n",
    "\n",
    "**Input Data:**\n",
    "- Truth: Real NOAA observations aggregated to daily Tmax\n",
    "- Forecasts: Real Open-Meteo historical forecasts\n",
    "\n",
    "**Core Features:**\n",
    "- `tmax_pred_f`: Raw forecast (the baseline to beat)\n",
    "- `lead_hours`: Forecast horizon (longer = more uncertainty)\n",
    "- `forecast_source`: Model identifier for multi-model ensembles\n",
    "\n",
    "**Seasonal Encodings:**\n",
    "- `sin_doy`, `cos_doy`: Capture annual temperature cycles\n",
    "- `month`: Coarse seasonal regime\n",
    "\n",
    "**Rolling Bias/Error Statistics (key value-add):**\n",
    "- `bias_7d`, `bias_14d`, `bias_30d`: Recent forecast bias (forecast - observed)\n",
    "- `rmse_14d`, `rmse_30d`: Recent forecast error magnitude\n",
    "- `sigma_lead`: Historical uncertainty for this lead time\n",
    "\n",
    "**Label:**\n",
    "- `tmax_actual_f`: Observed maximum temperature (ground truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline outputs:\n",
      "  - Raw NOAA hourly data: /Users/kcao/Documents/temp-data-pipeline/data/raw/noaa_hourly/KLGA\n",
      "  - Cleaned hourly observations: /Users/kcao/Documents/temp-data-pipeline/data/clean/hourly_obs/KLGA\n",
      "  - Daily Tmax (truth): /Users/kcao/Documents/temp-data-pipeline/data/clean/daily_tmax/KLGA/KLGA.parquet\n",
      "  - Open-Meteo forecasts: /Users/kcao/Documents/temp-data-pipeline/data/clean/forecasts/openmeteo/KLGA\n",
      "  - Training dataset: /Users/kcao/Documents/temp-data-pipeline/data/train/daily_tmax/KLGA/train_daily_tmax.parquet\n"
     ]
    }
   ],
   "source": [
    "# Pipeline complete - summary of outputs\n",
    "print(\"Pipeline outputs:\")\n",
    "print(f\"  - Raw NOAA hourly data: {OUTPUT_DIR}\")\n",
    "print(f\"  - Cleaned hourly observations: {HOURLY_CLEAN_DIR}\")\n",
    "print(f\"  - Daily Tmax (truth): {DAILY_TMAX_DIR / f'{STATION_ID}.parquet'}\")\n",
    "print(f\"  - Open-Meteo forecasts: {FORECAST_CLEAN_DIR}\")\n",
    "print(f\"  - Training dataset: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}