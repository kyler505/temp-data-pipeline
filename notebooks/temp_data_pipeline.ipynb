{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Pipeline Orchestration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed7f29e5"
      },
      "source": [
        "## Setup\n",
        "\n",
        "File setup for project paths and data directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6dc1d05",
        "outputId": "d0e6b31c-b307-4154-a20d-4ad8cf158f4a"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect Colab\n",
        "IN_COLAB = \"google.colab\" in sys.modules or \"COLAB_GPU\" in os.environ\n",
        "\n",
        "PROJECT_ROOT = None\n",
        "\n",
        "if IN_COLAB:\n",
        "    import subprocess\n",
        "    colab_root = Path(\"/content/temp-data-pipeline\")\n",
        "    if not (colab_root / \"pyproject.toml\").exists():\n",
        "        # Clone repo if not present\n",
        "        subprocess.run(\n",
        "            [\"git\", \"clone\", \"https://github.com/kyler505/temp-data-pipeline.git\", str(colab_root)],\n",
        "            check=True,\n",
        "        )\n",
        "    else:\n",
        "        # Pull latest changes\n",
        "        subprocess.run([\"git\", \"pull\"], cwd=colab_root, check=True)\n",
        "    PROJECT_ROOT = colab_root\n",
        "else:\n",
        "    # Local: search upward for pyproject.toml\n",
        "    cwd = Path.cwd().resolve()\n",
        "    for parent in [cwd] + list(cwd.parents):\n",
        "        if (parent / \"pyproject.toml\").exists():\n",
        "            PROJECT_ROOT = parent\n",
        "            break\n",
        "    # Fallback to common dev location\n",
        "    if PROJECT_ROOT is None:\n",
        "        candidate = Path.home() / \"Documents\" / \"temp-data-pipeline\"\n",
        "        if (candidate / \"pyproject.toml\").exists():\n",
        "            PROJECT_ROOT = candidate\n",
        "\n",
        "if PROJECT_ROOT is None:\n",
        "    raise FileNotFoundError(\"Could not find project root. Set PROJECT_ROOT manually.\")\n",
        "\n",
        "# Add to Python path\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "src_path = PROJECT_ROOT / \"src\"\n",
        "if src_path.exists() and str(src_path) not in sys.path:\n",
        "    sys.path.insert(0, str(src_path))\n",
        "\n",
        "DATA_DIR = PROJECT_ROOT / \"data\"\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"Data dir: {DATA_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install packages\n",
        "\n",
        "Install project dependencies in editable mode if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "import importlib\n",
        "\n",
        "# Always reinstall in editable mode to pick up any code changes\n",
        "if (PROJECT_ROOT / \"pyproject.toml\").exists():\n",
        "    subprocess.run(\n",
        "        [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-e\", str(PROJECT_ROOT)],\n",
        "        check=True,\n",
        "    )\n",
        "    # Clear cached imports so we get the latest code\n",
        "    for mod_name in list(sys.modules.keys()):\n",
        "        if mod_name.startswith(\"tempdata\"):\n",
        "            del sys.modules[mod_name]\n",
        "    print(\"Installed/updated tempdata in editable mode\")\n",
        "else:\n",
        "    raise FileNotFoundError(\n",
        "        f\"pyproject.toml not found in {PROJECT_ROOT}. \"\n",
        "        \"Update PROJECT_ROOT in the setup cell.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32c96254"
      },
      "source": [
        "## Fetch NOAA hourly data\n",
        "\n",
        "Configure a station and date range, then run the fetcher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a7c82a2",
        "outputId": "77e0a89d-0c4e-40cf-f5f7-ac51c3419431"
      },
      "outputs": [],
      "source": [
        "from tempdata.fetch.noaa_hourly import fetch_noaa_hourly\n",
        "\n",
        "STATION_ID = \"KLGA\"\n",
        "START_DATE = \"2016-01-01\"\n",
        "END_DATE = \"2025-08-27\"  # exclusive\n",
        "\n",
        "OUTPUT_DIR = DATA_DIR / \"raw\" / \"noaa_hourly\" / STATION_ID\n",
        "CACHE_DIR = DATA_DIR / \"cache\" / \"isd_csv\" / STATION_ID\n",
        "\n",
        "written = fetch_noaa_hourly(\n",
        "    station_id=STATION_ID,\n",
        "    start_date=START_DATE,\n",
        "    end_date=END_DATE,\n",
        "    out_dir=OUTPUT_DIR,\n",
        "    cache_dir=CACHE_DIR,\n",
        ")\n",
        "\n",
        "print(f\"Wrote {len(written)} parquet files:\")\n",
        "for path in written:\n",
        "    print(f\"  - {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b91ad96"
      },
      "source": [
        "## Verify outputs\n",
        "\n",
        "Load one parquet file to confirm the fetch results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96884b12",
        "outputId": "5b9e7269-f77c-468d-b084-4ee71786ed93"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tempdata.schemas import validate_hourly_obs\n",
        "\n",
        "parquet_files = sorted(OUTPUT_DIR.glob(\"*.parquet\"))\n",
        "if not parquet_files:\n",
        "    raise FileNotFoundError(f\"No parquet files found in {OUTPUT_DIR}\")\n",
        "\n",
        "# Load ALL parquet files and concatenate\n",
        "dfs = []\n",
        "for pf in parquet_files:\n",
        "    df_year = pd.read_parquet(pf)\n",
        "    dfs.append(df_year)\n",
        "    print(f\"Loaded {len(df_year)} rows from {pf.name}\")\n",
        "\n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "print(f\"\\nTotal: {len(df)} rows from {len(parquet_files)} files\")\n",
        "\n",
        "# Validate schema (will raise if invalid)\n",
        "validate_hourly_obs(df, require_unique_keys=False)\n",
        "print(\"Schema validation passed\")\n",
        "\n",
        "print(df.head())\n",
        "print(f\"Date range: {df['ts_utc'].min()} to {df['ts_utc'].max()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean hourly data\n",
        "\n",
        "Apply the cleaning pipeline to the fetched data:\n",
        "- Validate input schema (early fail on malformed data)\n",
        "- Sort and deduplicate by (ts_utc, station_id)\n",
        "- Flag missing temperature values\n",
        "- Flag and nullify out-of-range temperatures\n",
        "- Detect hour-to-hour spikes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tempdata.clean import clean_hourly_obs\n",
        "\n",
        "# Clean the fetched data\n",
        "# This applies: deduplication, missing value flags, out-of-range handling, spike detection\n",
        "df_clean = clean_hourly_obs(df)\n",
        "\n",
        "print(f\"\\nCleaned DataFrame shape: {df_clean.shape}\")\n",
        "print(df_clean.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5911ef1"
      },
      "source": [
        "## Aggregate to Daily Tmax\n",
        "\n",
        "Convert cleaned hourly observations to daily maximum temperature (Tmax).\n",
        "\n",
        "Key design principles:\n",
        "- **Market-aligned**: Tmax is computed per station-local calendar day, not UTC\n",
        "- **QC-aware**: Hours with `QC_OUT_OF_RANGE` are excluded from Tmax calculation\n",
        "- **Spike-inclusive**: Spike-flagged values ARE included (to avoid removing real heat spikes)\n",
        "- **Transparent**: Every day carries `coverage_hours` and propagated `qc_flags`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "310d8877",
        "outputId": "80bfcb75-9439-43eb-83d0-b8509766ffbf"
      },
      "outputs": [],
      "source": [
        "from tempdata.aggregate.build_daily_tmax import build_daily_tmax\n",
        "from tempdata.schemas.daily_tmax import validate_daily_tmax\n",
        "\n",
        "# Station timezone (KLGA is in Eastern time)\n",
        "STATION_TZ = \"America/New_York\"\n",
        "\n",
        "# Build daily Tmax from cleaned hourly data\n",
        "df_daily = build_daily_tmax(df_clean, station_tz=STATION_TZ)\n",
        "\n",
        "# Validate the output schema\n",
        "validate_daily_tmax(df_daily)\n",
        "print(\"Daily Tmax schema validation passed\")\n",
        "\n",
        "print(f\"\\nAggregated {len(df_clean)} hourly obs -> {len(df_daily)} daily records\")\n",
        "print(f\"Date range: {df_daily['date_local'].min().date()} to {df_daily['date_local'].max().date()}\")\n",
        "\n",
        "print(\"\\nDaily Tmax summary:\")\n",
        "print(df_daily[[\"date_local\", \"tmax_c\", \"tmax_f\", \"coverage_hours\", \"qc_flags\"]].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Coverage and Quality Analysis\n",
        "\n",
        "Check data quality metrics for the aggregated daily Tmax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tempdata.schemas.qc_flags import QC_LOW_COVERAGE, QC_INCOMPLETE_DAY, QC_SPIKE_DETECTED\n",
        "\n",
        "# Coverage statistics\n",
        "print(\"Coverage Statistics:\")\n",
        "print(f\"  Min coverage: {df_daily['coverage_hours'].min()} hours\")\n",
        "print(f\"  Max coverage: {df_daily['coverage_hours'].max()} hours\")\n",
        "print(f\"  Mean coverage: {df_daily['coverage_hours'].mean():.1f} hours\")\n",
        "print(f\"  Days with 24h coverage: {(df_daily['coverage_hours'] == 24).sum()}\")\n",
        "\n",
        "# QC flag breakdown\n",
        "print(\"\\nQC Flag Analysis:\")\n",
        "low_coverage_days = ((df_daily['qc_flags'] & QC_LOW_COVERAGE) != 0).sum()\n",
        "incomplete_days = ((df_daily['qc_flags'] & QC_INCOMPLETE_DAY) != 0).sum()\n",
        "spike_days = ((df_daily['qc_flags'] & QC_SPIKE_DETECTED) != 0).sum()\n",
        "\n",
        "print(f\"  Days with QC_LOW_COVERAGE: {low_coverage_days}\")\n",
        "print(f\"  Days with QC_INCOMPLETE_DAY: {incomplete_days}\")\n",
        "print(f\"  Days with QC_SPIKE_DETECTED: {spike_days}\")\n",
        "print(f\"  Days with no QC issues: {(df_daily['qc_flags'] == 0).sum()}\")\n",
        "\n",
        "# Temperature range\n",
        "print(\"\\nTemperature Range:\")\n",
        "print(f\"  Min Tmax: {df_daily['tmax_c'].min():.1f}°C ({df_daily['tmax_f'].min():.1f}°F)\")\n",
        "print(f\"  Max Tmax: {df_daily['tmax_c'].max():.1f}°C ({df_daily['tmax_f'].max():.1f}°F)\")\n",
        "print(f\"  Mean Tmax: {df_daily['tmax_c'].mean():.1f}°C ({df_daily['tmax_f'].mean():.1f}°F)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c81173fe"
      },
      "source": [
        "## Save Daily Tmax\n",
        "\n",
        "Write the daily Tmax data to parquet for downstream use (backtesting, model training, trading validation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb605429",
        "outputId": "55df9858-bba2-4163-e3c5-dc548b36e0e1"
      },
      "outputs": [],
      "source": [
        "from tempdata.aggregate.build_daily_tmax import write_daily_tmax\n",
        "\n",
        "# Output paths - partition daily Tmax by year like hourly data\n",
        "DAILY_TMAX_DIR = DATA_DIR / \"clean\" / \"daily_tmax\" / STATION_ID\n",
        "DAILY_TMAX_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Determine year range for partitioning\n",
        "years = df_daily[\"date_local\"].dt.year.unique()\n",
        "for year in years:\n",
        "    year_df = df_daily[df_daily[\"date_local\"].dt.year == year]\n",
        "    year_path = DAILY_TMAX_DIR / f\"{year}.parquet\"\n",
        "    year_df.to_parquet(year_path, index=False)\n",
        "    print(f\"[aggregate] Wrote {len(year_df)} rows to {year_path}\")\n",
        "\n",
        "# Also save cleaned hourly data for reference\n",
        "HOURLY_CLEAN_DIR = DATA_DIR / \"clean\" / \"hourly_obs\" / STATION_ID\n",
        "HOURLY_CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Determine year range for partitioning\n",
        "years = df_clean[\"ts_utc\"].dt.year.unique()\n",
        "for year in years:\n",
        "    year_df = df_clean[df_clean[\"ts_utc\"].dt.year == year]\n",
        "    year_path = HOURLY_CLEAN_DIR / f\"{year}.parquet\"\n",
        "    year_df.to_parquet(year_path, index=False)\n",
        "    print(f\"[clean] Wrote {len(year_df)} rows to {year_path}\")\n",
        "\n",
        "print(f\"\\nPipeline complete!\")\n",
        "print(f\"  Daily Tmax: {DAILY_TMAX_DIR}\")\n",
        "print(f\"  Cleaned hourly: {HOURLY_CLEAN_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verify Saved Data\n",
        "\n",
        "Reload the saved parquet to confirm it was written correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload and verify the saved daily Tmax data (partitioned by year)\n",
        "daily_tmax_files = list(DAILY_TMAX_DIR.glob(\"*.parquet\"))\n",
        "daily_tmax_dfs = []\n",
        "for f in daily_tmax_files:\n",
        "    df_year = pd.read_parquet(f)\n",
        "    daily_tmax_dfs.append(df_year)\n",
        "    print(f\"Loaded {len(df_year)} rows from {f.name}\")\n",
        "\n",
        "df_verify = pd.concat(daily_tmax_dfs, ignore_index=True)\n",
        "print(f\"\\nTotal: {len(df_verify)} daily records from {len(daily_tmax_files)} files\")\n",
        "\n",
        "# Validate schema\n",
        "validate_daily_tmax(df_verify)\n",
        "print(\"Schema validation passed\")\n",
        "\n",
        "# Show sample of dataset\n",
        "print(\"\\nDaily Tmax Data (first 10 rows):\")\n",
        "print(df_verify.head(10).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fetch Open-Meteo Historical Forecasts\n",
        "\n",
        "Ingest **historical** daily Tmax forecasts from Open-Meteo for the same station and date range as the truth data.\n",
        "\n",
        "This creates the **feature-side** dataset: \"What did the forecast say at issue time about a target local date?\"\n",
        "\n",
        "Key concepts:\n",
        "- **Issue time**: when the forecast was issued (simulated as midnight UTC of the day before target)\n",
        "- **Target date**: the station-local calendar date being forecasted\n",
        "- **Lead hours**: hours from issue time to target date midnight in station timezone\n",
        "\n",
        "Using historical forecasts allows us to join forecasts to truth data for model training and backtesting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tempdata.fetch.openmeteo_daily_forecast import fetch_openmeteo_historical_forecasts\n",
        "from tempdata.schemas.daily_tmax_forecast import validate_daily_tmax_forecast\n",
        "\n",
        "# Use the same date range as the truth data (from NOAA fetch)\n",
        "FORECAST_START_DATE = START_DATE  # e.g., \"2024-01-01\"\n",
        "FORECAST_END_DATE = END_DATE      # e.g., \"2024-02-01\" (exclusive for NOAA, but inclusive for Open-Meteo)\n",
        "\n",
        "# Adjust end date: NOAA uses exclusive end, Open-Meteo uses inclusive\n",
        "# Subtract 1 day from END_DATE to match the truth data range\n",
        "from datetime import datetime, timedelta\n",
        "end_dt = datetime.strptime(FORECAST_END_DATE, \"%Y-%m-%d\") - timedelta(days=1)\n",
        "forecast_end_date = end_dt.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "print(f\"Fetching historical forecasts for {STATION_ID}\")\n",
        "print(f\"Date range: {FORECAST_START_DATE} to {forecast_end_date}\")\n",
        "\n",
        "# Output directories\n",
        "FORECAST_RAW_DIR = DATA_DIR / \"raw\" / \"forecasts\" / \"openmeteo\" / STATION_ID\n",
        "FORECAST_CLEAN_DIR = DATA_DIR / \"clean\" / \"forecasts\" / \"openmeteo\" / STATION_ID\n",
        "\n",
        "forecast_files, df_forecast = fetch_openmeteo_historical_forecasts(\n",
        "    station_id=STATION_ID,\n",
        "    start_date=FORECAST_START_DATE,\n",
        "    end_date=forecast_end_date,\n",
        "    out_raw_dir=FORECAST_RAW_DIR,\n",
        "    out_parquet_dir=FORECAST_CLEAN_DIR,\n",
        "    write_raw=True,  # Save raw JSON for debugging\n",
        ")\n",
        "\n",
        "print(f\"\\nWrote {len(forecast_files)} files:\")\n",
        "for path in forecast_files:\n",
        "    print(f\"  - {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verify Forecast Data\n",
        "\n",
        "Load and validate the forecast parquet, then display a summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df_forecast is already returned from fetch_openmeteo_historical_forecasts\n",
        "print(f\"Loaded {len(df_forecast)} forecast rows\")\n",
        "\n",
        "# Validate schema (already validated in fetch, but double-check)\n",
        "validate_daily_tmax_forecast(df_forecast)\n",
        "print(\"Schema validation passed\")\n",
        "\n",
        "# Display summary\n",
        "print(f\"\\nForecast Summary:\")\n",
        "print(f\"  Target dates: {df_forecast['target_date_local'].min().date()} to {df_forecast['target_date_local'].max().date()}\")\n",
        "print(f\"  Lead hours range: {df_forecast['lead_hours'].min()} to {df_forecast['lead_hours'].max()}\")\n",
        "print(f\"  Tmax (C): {df_forecast['tmax_pred_c'].min():.1f} to {df_forecast['tmax_pred_c'].max():.1f}\")\n",
        "print(f\"  Tmax (F): {df_forecast['tmax_pred_f'].min():.1f} to {df_forecast['tmax_pred_f'].max():.1f}\")\n",
        "\n",
        "print(\"\\nForecast Data (first 10 rows):\")\n",
        "print(df_forecast[[\"target_date_local\", \"tmax_pred_c\", \"tmax_pred_f\", \"lead_hours\"]].head(10).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering for Daily Tmax\n",
        "\n",
        "Transform forecasts and truth data into a **model-ready training dataset**.\n",
        "\n",
        "This section uses:\n",
        "- **Real truth data** (`df_daily`) from the NOAA aggregation step above\n",
        "- **Real historical forecasts** (`df_forecast`) from the Open-Meteo fetch above\n",
        "\n",
        "The feature engineering pipeline:\n",
        "1. Joins forecasts to truth on `(station_id, target_date_local)`\n",
        "2. Filters low-quality truth days by coverage\n",
        "3. Adds seasonal encodings: `sin_doy`, `cos_doy`, `month`\n",
        "4. Computes rolling bias/error statistics: `bias_7d`, `bias_14d`, `bias_30d`, `rmse_14d`, `rmse_30d`, `sigma_lead`\n",
        "\n",
        "All rolling features use `.shift(1)` to ensure **no lookahead** — each row's features are computed only from prior data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use real data from previous cells:\n",
        "# - df_daily: NOAA observations aggregated to daily Tmax (truth)\n",
        "# - df_forecast: Open-Meteo historical forecasts\n",
        "\n",
        "# Prepare truth data for feature engineering\n",
        "df_truth_for_features = df_daily.copy()\n",
        "\n",
        "print(f\"Using real truth data: {len(df_truth_for_features)} days\")\n",
        "print(f\"  Date range: {df_truth_for_features['date_local'].min().date()} to {df_truth_for_features['date_local'].max().date()}\")\n",
        "\n",
        "# Use the real Open-Meteo historical forecasts\n",
        "df_forecast_for_features = df_forecast.copy()\n",
        "\n",
        "print(f\"\\nUsing real Open-Meteo historical forecasts:\")\n",
        "print(f\"  Forecast rows: {len(df_forecast_for_features)}\")\n",
        "print(f\"  Lead times: {sorted(df_forecast_for_features['lead_hours'].unique())} hours\")\n",
        "print(f\"  Target date range: {df_forecast_for_features['target_date_local'].min().date()} to {df_forecast_for_features['target_date_local'].max().date()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build Training Dataset\n",
        "\n",
        "Run the feature engineering pipeline to create model-ready features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tempdata.features.build_train_daily_tmax import build_train_daily_tmax\n",
        "from tempdata.schemas.train_daily_tmax import validate_train_daily_tmax, TRAIN_DAILY_TMAX_FIELDS\n",
        "\n",
        "# Build the training dataset using real truth data + real Open-Meteo forecasts\n",
        "# This performs: join, seasonal features, rolling bias/error stats, validation\n",
        "df_train = build_train_daily_tmax(\n",
        "    forecast_df=df_forecast_for_features,\n",
        "    truth_df=df_truth_for_features,\n",
        "    min_coverage_hours=18,  # Filter low-quality truth days\n",
        "    drop_warmup_nulls=False,  # Keep warm-up rows (they have NaN in rolling features)\n",
        "    validate=True,\n",
        ")\n",
        "\n",
        "print(f\"Training dataset: {len(df_train)} rows\")\n",
        "print(f\"Columns: {list(df_train.columns)}\")\n",
        "print(f\"\\nColumn types:\")\n",
        "for col in df_train.columns:\n",
        "    print(f\"  {col}: {df_train[col].dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspect Features\n",
        "\n",
        "Examine the generated features, focusing on rolling statistics and seasonal encodings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display core features and seasonal encodings\n",
        "display_cols = [\n",
        "    \"target_date_local\", \"lead_hours\", \"tmax_pred_f\", \"tmax_actual_f\",\n",
        "    \"sin_doy\", \"cos_doy\", \"month\"\n",
        "]\n",
        "print(\"Core Features & Seasonal Encodings (first 10 rows):\")\n",
        "print(df_train[display_cols].head(10).to_string(index=False))\n",
        "\n",
        "# Display rolling bias/error features\n",
        "rolling_cols = [\n",
        "    \"target_date_local\", \"lead_hours\", \"bias_7d\", \"bias_14d\", \"bias_30d\",\n",
        "    \"rmse_14d\", \"rmse_30d\", \"sigma_lead\"\n",
        "]\n",
        "print(\"\\n\\nRolling Bias & Error Features (rows 10-20, after warm-up):\")\n",
        "print(df_train[rolling_cols].iloc[10:20].to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verify No-Lookahead Property\n",
        "\n",
        "Confirm that rolling features are computed correctly with `.shift(1)` — each row's features should only use prior data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify no-lookahead: The first row for each (station, lead_hours) group should have NaN bias\n",
        "# because there's no prior data to compute rolling stats from\n",
        "\n",
        "first_rows = df_train.groupby([\"station_id\", \"lead_hours\"]).first()\n",
        "print(\"First row per (station_id, lead_hours) group — bias_7d should be NaN:\")\n",
        "print(first_rows[[\"bias_7d\", \"bias_14d\", \"bias_30d\"]].to_string())\n",
        "\n",
        "# Compute actual residual for a specific row and verify it's NOT in its own bias\n",
        "# Pick a row after warm-up period\n",
        "if len(df_train) > 10:\n",
        "    test_idx = 10\n",
        "    test_row = df_train.iloc[test_idx]\n",
        "    actual_residual = test_row[\"tmax_pred_f\"] - test_row[\"tmax_actual_f\"]\n",
        "\n",
        "    print(f\"\\n\\nVerification for row {test_idx}:\")\n",
        "    print(f\"  Actual residual (pred - actual): {actual_residual:.2f}°F\")\n",
        "    print(f\"  bias_7d (computed from PRIOR 7 days): {test_row['bias_7d']:.2f}°F\")\n",
        "    print(f\"  These should be different values (bias excludes current row)\")\n",
        "\n",
        "    # The bias_7d should NOT equal the current residual (unless by coincidence)\n",
        "    print(f\"\\n  Residual == bias_7d? {abs(actual_residual - test_row['bias_7d']) < 0.01}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyze Forecast Bias by Lead Time\n",
        "\n",
        "The rolling bias features capture systematic forecast errors that vary by lead time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute actual residuals for analysis\n",
        "df_train[\"residual\"] = df_train[\"tmax_pred_f\"] - df_train[\"tmax_actual_f\"]\n",
        "\n",
        "# Analyze bias by lead time\n",
        "print(\"Forecast Error Analysis by Lead Time:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "bias_by_lead = df_train.groupby(\"lead_hours\").agg({\n",
        "    \"residual\": [\"mean\", \"std\", \"count\"],\n",
        "    \"bias_30d\": \"mean\",  # Average of the rolling bias feature\n",
        "    \"sigma_lead\": \"mean\",  # Average sigma_lead\n",
        "}).round(2)\n",
        "\n",
        "bias_by_lead.columns = [\"Mean Error (°F)\", \"Std Dev (°F)\", \"Count\", \"Avg bias_30d\", \"Avg sigma_lead\"]\n",
        "print(bias_by_lead.to_string())\n",
        "\n",
        "print(\"\\n\\nKey Insight:\")\n",
        "print(\"  - sigma_lead captures uncertainty and can be used for confidence intervals\")\n",
        "print(\"  - bias_30d provides a rolling estimate that adapts to recent forecast performance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Training Dataset\n",
        "\n",
        "Write the feature-engineered training dataset to parquet for model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tempdata.features.build_train_daily_tmax import write_train_daily_tmax\n",
        "\n",
        "# Create output directory\n",
        "TRAIN_DIR = DATA_DIR / \"train\" / \"daily_tmax\" / STATION_ID\n",
        "TRAIN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Drop rows with NaN rolling features for clean training data\n",
        "df_train_clean = df_train.dropna(subset=[\"bias_7d\", \"bias_14d\", \"bias_30d\", \"rmse_14d\", \"rmse_30d\", \"sigma_lead\"])\n",
        "print(f\"Rows after dropping warm-up NaNs: {len(df_train_clean)} (was {len(df_train)})\")\n",
        "\n",
        "# Select only the schema columns (drop residual which was for analysis)\n",
        "df_train_final = df_train_clean[TRAIN_DAILY_TMAX_FIELDS].copy()\n",
        "\n",
        "# Write to parquet\n",
        "output_path = TRAIN_DIR / \"train_daily_tmax.parquet\"\n",
        "write_train_daily_tmax(df_train_final, output_path)\n",
        "\n",
        "print(f\"\\nTraining dataset saved to: {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering Summary\n",
        "\n",
        "The `train_daily_tmax` dataset is now ready for model training with:\n",
        "\n",
        "**Input Data:**\n",
        "- Truth: Real NOAA observations aggregated to daily Tmax\n",
        "- Forecasts: Real Open-Meteo historical forecasts\n",
        "\n",
        "**Core Features:**\n",
        "- `tmax_pred_f`: Raw forecast (the baseline to beat)\n",
        "- `lead_hours`: Forecast horizon (longer = more uncertainty)\n",
        "- `forecast_source`: Model identifier for multi-model ensembles\n",
        "\n",
        "**Seasonal Encodings:**\n",
        "- `sin_doy`, `cos_doy`: Capture annual temperature cycles\n",
        "- `month`: Coarse seasonal regime\n",
        "\n",
        "**Rolling Bias/Error Statistics (key value-add):**\n",
        "- `bias_7d`, `bias_14d`, `bias_30d`: Recent forecast bias (forecast - observed)\n",
        "- `rmse_14d`, `rmse_30d`: Recent forecast error magnitude\n",
        "- `sigma_lead`: Historical uncertainty for this lead time\n",
        "\n",
        "**Label:**\n",
        "- `tmax_actual_f`: Observed maximum temperature (ground truth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pipeline complete - summary of outputs\n",
        "print(\"Pipeline outputs:\")\n",
        "print(f\"  - Raw NOAA hourly data: {OUTPUT_DIR}\")\n",
        "print(f\"  - Cleaned hourly observations: {HOURLY_CLEAN_DIR}\")\n",
        "print(f\"  - Daily Tmax (truth): {DAILY_TMAX_DIR / f'{STATION_ID}.parquet'}\")\n",
        "print(f\"  - Open-Meteo forecasts: {FORECAST_CLEAN_DIR}\")\n",
        "print(f\"  - Training dataset: {output_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
