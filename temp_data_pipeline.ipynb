{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed7f29e5"
      },
      "source": [
        "# Setup\n",
        "\n",
        "File setup for project paths and data directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6dc1d05",
        "outputId": "d0e6b31c-b307-4154-a20d-4ad8cf158f4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /content/temp-data-pipeline\n",
            "Data dir: /content/temp-data-pipeline/data\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect Colab\n",
        "IN_COLAB = \"google.colab\" in sys.modules or \"COLAB_GPU\" in os.environ\n",
        "\n",
        "PROJECT_ROOT = None\n",
        "\n",
        "if IN_COLAB:\n",
        "    import subprocess\n",
        "    colab_root = Path(\"/content/temp-data-pipeline\")\n",
        "    if not (colab_root / \"pyproject.toml\").exists():\n",
        "        # Clone repo if not present\n",
        "        subprocess.run(\n",
        "            [\"git\", \"clone\", \"https://github.com/kyler505/temp-data-pipeline.git\", str(colab_root)],\n",
        "            check=True,\n",
        "        )\n",
        "    else:\n",
        "        # Pull latest changes\n",
        "        subprocess.run([\"git\", \"pull\"], cwd=colab_root, check=True)\n",
        "    PROJECT_ROOT = colab_root\n",
        "else:\n",
        "    # Local: search upward for pyproject.toml\n",
        "    cwd = Path.cwd().resolve()\n",
        "    for parent in [cwd] + list(cwd.parents):\n",
        "        if (parent / \"pyproject.toml\").exists():\n",
        "            PROJECT_ROOT = parent\n",
        "            break\n",
        "    # Fallback to common dev location\n",
        "    if PROJECT_ROOT is None:\n",
        "        candidate = Path.home() / \"Documents\" / \"temp-data-pipeline\"\n",
        "        if (candidate / \"pyproject.toml\").exists():\n",
        "            PROJECT_ROOT = candidate\n",
        "\n",
        "if PROJECT_ROOT is None:\n",
        "    raise FileNotFoundError(\"Could not find project root. Set PROJECT_ROOT manually.\")\n",
        "\n",
        "# Add to Python path\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "src_path = PROJECT_ROOT / \"src\"\n",
        "if src_path.exists() and str(src_path) not in sys.path:\n",
        "    sys.path.insert(0, str(src_path))\n",
        "\n",
        "DATA_DIR = PROJECT_ROOT / \"data\"\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"Data dir: {DATA_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Install packages\n",
        "\n",
        "Install project dependencies in editable mode if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installed/updated tempdata in editable mode\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "import importlib\n",
        "\n",
        "# Always reinstall in editable mode to pick up any code changes\n",
        "if (PROJECT_ROOT / \"pyproject.toml\").exists():\n",
        "    subprocess.run(\n",
        "        [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-e\", str(PROJECT_ROOT)],\n",
        "        check=True,\n",
        "    )\n",
        "    # Clear cached imports so we get the latest code\n",
        "    for mod_name in list(sys.modules.keys()):\n",
        "        if mod_name.startswith(\"tempdata\"):\n",
        "            del sys.modules[mod_name]\n",
        "    print(\"Installed/updated tempdata in editable mode\")\n",
        "else:\n",
        "    raise FileNotFoundError(\n",
        "        f\"pyproject.toml not found in {PROJECT_ROOT}. \"\n",
        "        \"Update PROJECT_ROOT in the setup cell.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32c96254"
      },
      "source": [
        "# Fetch NOAA hourly data\n",
        "\n",
        "Configure a station and date range, then run the fetcher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a7c82a2",
        "outputId": "77e0a89d-0c4e-40cf-f5f7-ac51c3419431"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[noaa] 2024: rows=1236 coverage=2024-01-01 00:00:00+00:00 -> 2024-01-31 23:51:00+00:00\n",
            "Wrote 1 parquet files:\n",
            "  - /content/temp-data-pipeline/data/raw/noaa_hourly/KLGA/2024.parquet\n"
          ]
        }
      ],
      "source": [
        "from tempdata.fetch.noaa_hourly import fetch_noaa_hourly\n",
        "\n",
        "STATION_ID = \"KLGA\"\n",
        "START_DATE = \"2024-01-01\"\n",
        "END_DATE = \"2024-02-01\"  # exclusive\n",
        "\n",
        "OUTPUT_DIR = DATA_DIR / \"raw\" / \"noaa_hourly\" / STATION_ID\n",
        "CACHE_DIR = DATA_DIR / \"cache\" / \"isd_csv\" / STATION_ID\n",
        "\n",
        "written = fetch_noaa_hourly(\n",
        "    station_id=STATION_ID,\n",
        "    start_date=START_DATE,\n",
        "    end_date=END_DATE,\n",
        "    out_dir=OUTPUT_DIR,\n",
        "    cache_dir=CACHE_DIR,\n",
        ")\n",
        "\n",
        "print(f\"Wrote {len(written)} parquet files:\")\n",
        "for path in written:\n",
        "    print(f\"  - {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b91ad96"
      },
      "source": [
        "# Verify outputs\n",
        "\n",
        "Load one parquet file to confirm the fetch results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96884b12",
        "outputId": "5b9e7269-f77c-468d-b084-4ee71786ed93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1236 rows from 2024.parquet\n",
            "Schema validation passed\n",
            "                     ts_utc station_id       lat       lon  temp_c source  \\\n",
            "0 2024-01-01 00:00:00+00:00       KLGA  40.77945 -73.88027     6.1   noaa   \n",
            "1 2024-01-01 00:51:00+00:00       KLGA  40.77945 -73.88027     6.1   noaa   \n",
            "2 2024-01-01 01:51:00+00:00       KLGA  40.77945 -73.88027     6.1   noaa   \n",
            "3 2024-01-01 02:51:00+00:00       KLGA  40.77945 -73.88027     6.1   noaa   \n",
            "4 2024-01-01 03:00:00+00:00       KLGA  40.77945 -73.88027     6.1   noaa   \n",
            "\n",
            "   qc_flags  \n",
            "0         0  \n",
            "1         0  \n",
            "2         0  \n",
            "3         0  \n",
            "4         0  \n",
            "Date range: 2024-01-01 00:00:00+00:00 to 2024-01-31 23:51:00+00:00\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from tempdata.schemas import validate_hourly_obs\n",
        "\n",
        "parquet_files = sorted(OUTPUT_DIR.glob(\"*.parquet\"))\n",
        "if not parquet_files:\n",
        "    raise FileNotFoundError(f\"No parquet files found in {OUTPUT_DIR}\")\n",
        "\n",
        "df = pd.read_parquet(parquet_files[0])\n",
        "print(f\"Loaded {len(df)} rows from {parquet_files[0].name}\")\n",
        "\n",
        "# Validate schema (will raise if invalid)\n",
        "validate_hourly_obs(df, require_unique_keys=False)\n",
        "print(\"Schema validation passed\")\n",
        "\n",
        "print(df.head())\n",
        "print(f\"Date range: {df['ts_utc'].min()} to {df['ts_utc'].max()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Clean hourly data\n",
        "\n",
        "Apply the cleaning pipeline to the fetched data:\n",
        "- Validate input schema (early fail on malformed data)\n",
        "- Sort and deduplicate by (ts_utc, station_id)\n",
        "- Flag missing temperature values\n",
        "- Flag and nullify out-of-range temperatures\n",
        "- Detect hour-to-hour spikes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[clean] Cleaning summary:\n",
            "  Total rows: 1236 -> 1234 (2 duplicates removed)\n",
            "  Rows with QC flags: 31\n",
            "    QC_MISSING_VALUE: 31\n",
            "  Temp range (valid): -7.2C to 15.0C\n",
            "\n",
            "Cleaned DataFrame shape: (1234, 7)\n",
            "                     ts_utc station_id       lat       lon  temp_c source  \\\n",
            "0 2024-01-01 00:00:00+00:00       KLGA  40.77945 -73.88027     6.1   noaa   \n",
            "1 2024-01-01 00:51:00+00:00       KLGA  40.77945 -73.88027     6.1   noaa   \n",
            "2 2024-01-01 01:51:00+00:00       KLGA  40.77945 -73.88027     6.1   noaa   \n",
            "3 2024-01-01 02:51:00+00:00       KLGA  40.77945 -73.88027     6.1   noaa   \n",
            "4 2024-01-01 03:00:00+00:00       KLGA  40.77945 -73.88027     6.1   noaa   \n",
            "\n",
            "   qc_flags  \n",
            "0         0  \n",
            "1         0  \n",
            "2         0  \n",
            "3         0  \n",
            "4         0  \n"
          ]
        }
      ],
      "source": [
        "from tempdata.clean import clean_hourly_obs\n",
        "\n",
        "# Clean the fetched data\n",
        "# This applies: deduplication, missing value flags, out-of-range handling, spike detection\n",
        "df_clean = clean_hourly_obs(df)\n",
        "\n",
        "print(f\"\\nCleaned DataFrame shape: {df_clean.shape}\")\n",
        "print(df_clean.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5911ef1"
      },
      "source": [
        "# Aggregate to Daily Tmax\n",
        "\n",
        "Convert cleaned hourly observations to daily maximum temperature (Tmax).\n",
        "\n",
        "Key design principles:\n",
        "- **Market-aligned**: Tmax is computed per station-local calendar day, not UTC\n",
        "- **QC-aware**: Hours with `QC_OUT_OF_RANGE` are excluded from Tmax calculation\n",
        "- **Spike-inclusive**: Spike-flagged values ARE included (to avoid removing real heat spikes)\n",
        "- **Transparent**: Every day carries `coverage_hours` and propagated `qc_flags`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "310d8877",
        "outputId": "80bfcb75-9439-43eb-83d0-b8509766ffbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Daily Tmax schema validation passed\n",
            "\n",
            "Aggregated 1234 hourly obs -> 32 daily records\n",
            "Date range: 2023-12-31 to 2024-01-31\n",
            "\n",
            "Daily Tmax summary:\n",
            "                 date_local  tmax_c  tmax_f  coverage_hours  qc_flags\n",
            "0 2023-12-31 00:00:00-05:00     6.7    44.1               5        17\n",
            "1 2024-01-01 00:00:00-05:00     7.2    45.0              24         1\n",
            "2 2024-01-02 00:00:00-05:00     6.1    43.0              24         1\n",
            "3 2024-01-03 00:00:00-05:00     6.1    43.0              24         1\n",
            "4 2024-01-04 00:00:00-05:00     6.7    44.1              24         1\n",
            "5 2024-01-05 00:00:00-05:00     3.3    37.9              24         1\n",
            "6 2024-01-06 00:00:00-05:00     3.3    37.9              24         1\n",
            "7 2024-01-07 00:00:00-05:00     4.0    39.2              24         1\n",
            "8 2024-01-08 00:00:00-05:00     6.7    44.1              24         1\n",
            "9 2024-01-09 00:00:00-05:00    13.9    57.0              24         1\n"
          ]
        }
      ],
      "source": [
        "from tempdata.aggregate.build_daily_tmax import build_daily_tmax\n",
        "from tempdata.schemas.daily_tmax import validate_daily_tmax\n",
        "\n",
        "# Station timezone (KLGA is in Eastern time)\n",
        "STATION_TZ = \"America/New_York\"\n",
        "\n",
        "# Build daily Tmax from cleaned hourly data\n",
        "df_daily = build_daily_tmax(df_clean, station_tz=STATION_TZ)\n",
        "\n",
        "# Validate the output schema\n",
        "validate_daily_tmax(df_daily)\n",
        "print(\"Daily Tmax schema validation passed\")\n",
        "\n",
        "print(f\"\\nAggregated {len(df_clean)} hourly obs -> {len(df_daily)} daily records\")\n",
        "print(f\"Date range: {df_daily['date_local'].min().date()} to {df_daily['date_local'].max().date()}\")\n",
        "\n",
        "print(\"\\nDaily Tmax summary:\")\n",
        "print(df_daily[[\"date_local\", \"tmax_c\", \"tmax_f\", \"coverage_hours\", \"qc_flags\"]].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Coverage and Quality Analysis\n",
        "\n",
        "Check data quality metrics for the aggregated daily Tmax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coverage Statistics:\n",
            "  Min coverage: 5 hours\n",
            "  Max coverage: 24 hours\n",
            "  Mean coverage: 23.2 hours\n",
            "  Days with 24h coverage: 30\n",
            "\n",
            "QC Flag Analysis:\n",
            "  Days with QC_LOW_COVERAGE: 1\n",
            "  Days with QC_INCOMPLETE_DAY: 0\n",
            "  Days with QC_SPIKE_DETECTED: 0\n",
            "  Days with no QC issues: 1\n",
            "\n",
            "Temperature Range:\n",
            "  Min Tmax: -3.9°C (25.0°F)\n",
            "  Max Tmax: 15.0°C (59.0°F)\n",
            "  Mean Tmax: 5.1°C (41.3°F)\n"
          ]
        }
      ],
      "source": [
        "from tempdata.schemas.qc_flags import QC_LOW_COVERAGE, QC_INCOMPLETE_DAY, QC_SPIKE_DETECTED\n",
        "\n",
        "# Coverage statistics\n",
        "print(\"Coverage Statistics:\")\n",
        "print(f\"  Min coverage: {df_daily['coverage_hours'].min()} hours\")\n",
        "print(f\"  Max coverage: {df_daily['coverage_hours'].max()} hours\")\n",
        "print(f\"  Mean coverage: {df_daily['coverage_hours'].mean():.1f} hours\")\n",
        "print(f\"  Days with 24h coverage: {(df_daily['coverage_hours'] == 24).sum()}\")\n",
        "\n",
        "# QC flag breakdown\n",
        "print(\"\\nQC Flag Analysis:\")\n",
        "low_coverage_days = ((df_daily['qc_flags'] & QC_LOW_COVERAGE) != 0).sum()\n",
        "incomplete_days = ((df_daily['qc_flags'] & QC_INCOMPLETE_DAY) != 0).sum()\n",
        "spike_days = ((df_daily['qc_flags'] & QC_SPIKE_DETECTED) != 0).sum()\n",
        "\n",
        "print(f\"  Days with QC_LOW_COVERAGE: {low_coverage_days}\")\n",
        "print(f\"  Days with QC_INCOMPLETE_DAY: {incomplete_days}\")\n",
        "print(f\"  Days with QC_SPIKE_DETECTED: {spike_days}\")\n",
        "print(f\"  Days with no QC issues: {(df_daily['qc_flags'] == 0).sum()}\")\n",
        "\n",
        "# Temperature range\n",
        "print(\"\\nTemperature Range:\")\n",
        "print(f\"  Min Tmax: {df_daily['tmax_c'].min():.1f}°C ({df_daily['tmax_f'].min():.1f}°F)\")\n",
        "print(f\"  Max Tmax: {df_daily['tmax_c'].max():.1f}°C ({df_daily['tmax_f'].max():.1f}°F)\")\n",
        "print(f\"  Mean Tmax: {df_daily['tmax_c'].mean():.1f}°C ({df_daily['tmax_f'].mean():.1f}°F)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c81173fe"
      },
      "source": [
        "# Save Daily Tmax\n",
        "\n",
        "Write the daily Tmax data to parquet for downstream use (backtesting, model training, trading validation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb605429",
        "outputId": "55df9858-bba2-4163-e3c5-dc548b36e0e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[aggregate] wrote 32 rows to /content/temp-data-pipeline/data/clean/daily_tmax/KLGA.parquet\n",
            "[clean] Wrote 1234 rows to /content/temp-data-pipeline/data/clean/hourly_obs/KLGA/2024.parquet\n",
            "\n",
            "Pipeline complete!\n",
            "  Daily Tmax: /content/temp-data-pipeline/data/clean/daily_tmax/KLGA.parquet\n",
            "  Cleaned hourly: /content/temp-data-pipeline/data/clean/hourly_obs/KLGA\n"
          ]
        }
      ],
      "source": [
        "from tempdata.aggregate.build_daily_tmax import write_daily_tmax\n",
        "\n",
        "# Output paths\n",
        "DAILY_TMAX_DIR = DATA_DIR / \"clean\" / \"daily_tmax\"\n",
        "DAILY_TMAX_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "output_path = DAILY_TMAX_DIR / f\"{STATION_ID}.parquet\"\n",
        "\n",
        "# Write with schema validation\n",
        "write_daily_tmax(df_daily, output_path)\n",
        "\n",
        "# Also save cleaned hourly data for reference\n",
        "HOURLY_CLEAN_DIR = DATA_DIR / \"clean\" / \"hourly_obs\" / STATION_ID\n",
        "HOURLY_CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Determine year range for partitioning\n",
        "years = df_clean[\"ts_utc\"].dt.year.unique()\n",
        "for year in years:\n",
        "    year_df = df_clean[df_clean[\"ts_utc\"].dt.year == year]\n",
        "    year_path = HOURLY_CLEAN_DIR / f\"{year}.parquet\"\n",
        "    year_df.to_parquet(year_path, index=False)\n",
        "    print(f\"[clean] Wrote {len(year_df)} rows to {year_path}\")\n",
        "\n",
        "print(f\"\\nPipeline complete!\")\n",
        "print(f\"  Daily Tmax: {output_path}\")\n",
        "print(f\"  Cleaned hourly: {HOURLY_CLEAN_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Verify Saved Data\n",
        "\n",
        "Reload the saved parquet to confirm it was written correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reloaded 32 daily records from KLGA.parquet\n",
            "Schema validation passed\n",
            "\n",
            "Daily Tmax Data:\n",
            "               date_local station_id  tmax_c  tmax_f  coverage_hours   source  qc_flags                   updated_at_utc\n",
            "2023-12-31 00:00:00-05:00       KLGA     6.7    44.1               5 noaa_isd        17 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-01 00:00:00-05:00       KLGA     7.2    45.0              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-02 00:00:00-05:00       KLGA     6.1    43.0              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-03 00:00:00-05:00       KLGA     6.1    43.0              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-04 00:00:00-05:00       KLGA     6.7    44.1              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-05 00:00:00-05:00       KLGA     3.3    37.9              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-06 00:00:00-05:00       KLGA     3.3    37.9              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-07 00:00:00-05:00       KLGA     4.0    39.2              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-08 00:00:00-05:00       KLGA     6.7    44.1              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-09 00:00:00-05:00       KLGA    13.9    57.0              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-10 00:00:00-05:00       KLGA    13.0    55.4              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-11 00:00:00-05:00       KLGA     7.8    46.0              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-12 00:00:00-05:00       KLGA     9.4    48.9              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-13 00:00:00-05:00       KLGA    15.0    59.0              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-14 00:00:00-05:00       KLGA     6.1    43.0              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-15 00:00:00-05:00       KLGA    -1.7    28.9              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-16 00:00:00-05:00       KLGA     0.0    32.0              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-17 00:00:00-05:00       KLGA    -3.9    25.0              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-18 00:00:00-05:00       KLGA     1.1    34.0              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-19 00:00:00-05:00       KLGA     0.0    32.0              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-20 00:00:00-05:00       KLGA    -3.3    26.1              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-21 00:00:00-05:00       KLGA    -0.6    30.9              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-22 00:00:00-05:00       KLGA     2.8    37.0              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-23 00:00:00-05:00       KLGA     4.4    39.9              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-24 00:00:00-05:00       KLGA     6.1    43.0              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-25 00:00:00-05:00       KLGA    13.3    55.9              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-26 00:00:00-05:00       KLGA     6.1    43.0              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-27 00:00:00-05:00       KLGA     8.3    46.9              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-28 00:00:00-05:00       KLGA     5.6    42.1              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-29 00:00:00-05:00       KLGA     3.9    39.0              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-30 00:00:00-05:00       KLGA     3.3    37.9              24 noaa_isd         1 2026-01-19 05:45:23.888984+00:00\n",
            "2024-01-31 00:00:00-05:00       KLGA     3.9    39.0              19 noaa_isd         0 2026-01-19 05:45:23.888984+00:00\n"
          ]
        }
      ],
      "source": [
        "# Reload and verify the saved daily Tmax data\n",
        "df_verify = pd.read_parquet(output_path)\n",
        "\n",
        "# Validate schema\n",
        "validate_daily_tmax(df_verify)\n",
        "print(f\"Reloaded {len(df_verify)} daily records from {output_path.name}\")\n",
        "print(\"Schema validation passed\")\n",
        "\n",
        "# Show full dataset\n",
        "print(\"\\nDaily Tmax Data:\")\n",
        "print(df_verify.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fetch Open-Meteo Forecast\n",
        "\n",
        "Ingest daily maximum temperature (Tmax) forecasts from Open-Meteo for the same station.\n",
        "\n",
        "This creates the **feature-side** dataset: \"What did the forecast say at issue time about a target local date?\"\n",
        "\n",
        "Key concepts:\n",
        "- **Issue time**: when the forecast was fetched (UTC)\n",
        "- **Target date**: the station-local calendar date being forecasted\n",
        "- **Lead hours**: hours from issue time to target date midnight in station timezone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tempdata.fetch.openmeteo_daily_forecast'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-201447449.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtempdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenmeteo_daily_forecast\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_openmeteo_daily_tmax_forecast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtempdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschemas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdaily_tmax_forecast\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalidate_daily_tmax_forecast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Fetch forecast for the same station\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mFORECAST_DAYS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tempdata.fetch.openmeteo_daily_forecast'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from tempdata.fetch.openmeteo_daily_forecast import fetch_openmeteo_daily_tmax_forecast\n",
        "from tempdata.schemas.daily_tmax_forecast import validate_daily_tmax_forecast\n",
        "\n",
        "# Fetch forecast for the same station\n",
        "FORECAST_DAYS = 14\n",
        "\n",
        "# Output directories\n",
        "FORECAST_RAW_DIR = DATA_DIR / \"raw\" / \"forecasts\" / \"openmeteo\" / STATION_ID\n",
        "FORECAST_CLEAN_DIR = DATA_DIR / \"clean\" / \"forecasts\" / \"openmeteo\" / STATION_ID\n",
        "\n",
        "forecast_files = fetch_openmeteo_daily_tmax_forecast(\n",
        "    station_id=STATION_ID,\n",
        "    out_raw_dir=FORECAST_RAW_DIR,\n",
        "    out_parquet_dir=FORECAST_CLEAN_DIR,\n",
        "    forecast_days=FORECAST_DAYS,\n",
        "    write_raw=True,  # Save raw JSON for debugging\n",
        ")\n",
        "\n",
        "print(f\"Wrote {len(forecast_files)} files:\")\n",
        "for path in forecast_files:\n",
        "    print(f\"  - {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Verify Forecast Data\n",
        "\n",
        "Load and validate the forecast parquet, then display a summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the forecast parquet\n",
        "forecast_parquet_files = sorted(FORECAST_CLEAN_DIR.glob(\"*.parquet\"))\n",
        "if not forecast_parquet_files:\n",
        "    raise FileNotFoundError(f\"No parquet files found in {FORECAST_CLEAN_DIR}\")\n",
        "\n",
        "df_forecast = pd.read_parquet(forecast_parquet_files[-1])  # Most recent\n",
        "print(f\"Loaded {len(df_forecast)} forecast rows from {forecast_parquet_files[-1].name}\")\n",
        "\n",
        "# Validate schema\n",
        "validate_daily_tmax_forecast(df_forecast)\n",
        "print(\"Schema validation passed\")\n",
        "\n",
        "# Display summary\n",
        "print(f\"\\nForecast Summary:\")\n",
        "print(f\"  Issue time (UTC): {df_forecast['issue_time_utc'].iloc[0]}\")\n",
        "print(f\"  Target dates: {df_forecast['target_date_local'].min().date()} to {df_forecast['target_date_local'].max().date()}\")\n",
        "print(f\"  Lead hours range: {df_forecast['lead_hours'].min()} to {df_forecast['lead_hours'].max()}\")\n",
        "print(f\"  Tmax (C): {df_forecast['tmax_pred_c'].min():.1f} to {df_forecast['tmax_pred_c'].max():.1f}\")\n",
        "print(f\"  Tmax (F): {df_forecast['tmax_pred_f'].min():.1f} to {df_forecast['tmax_pred_f'].max():.1f}\")\n",
        "\n",
        "print(\"\\nForecast Data:\")\n",
        "print(df_forecast[[\"target_date_local\", \"tmax_pred_c\", \"tmax_pred_f\", \"lead_hours\"]].to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Join Forecasts to Truth\n",
        "\n",
        "Demonstrate joining forecasts to the truth dataset (`daily_tmax`) on `(station_id, target_date_local)`.\n",
        "\n",
        "This is the foundation for:\n",
        "- Model training (forecast features -> actual Tmax labels)\n",
        "- Backtesting (compare predicted vs actual)\n",
        "- Error analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare forecast for joining\n",
        "# Convert target_date_local to date for joining (strip time component)\n",
        "df_forecast_join = df_forecast.copy()\n",
        "df_forecast_join[\"target_date\"] = df_forecast_join[\"target_date_local\"].dt.date\n",
        "\n",
        "# Prepare truth data for joining\n",
        "# The date_local in daily_tmax is timezone-aware; convert to date for joining\n",
        "df_truth = df_verify.copy()\n",
        "df_truth[\"target_date\"] = df_truth[\"date_local\"].dt.date\n",
        "\n",
        "# Perform the join on (station_id, target_date)\n",
        "df_joined = df_forecast_join.merge(\n",
        "    df_truth[[\"station_id\", \"target_date\", \"tmax_c\", \"tmax_f\", \"coverage_hours\", \"qc_flags\"]],\n",
        "    on=[\"station_id\", \"target_date\"],\n",
        "    how=\"inner\",\n",
        "    suffixes=(\"_pred\", \"_actual\"),\n",
        ")\n",
        "\n",
        "if len(df_joined) > 0:\n",
        "    # Calculate forecast error\n",
        "    df_joined[\"error_c\"] = df_joined[\"tmax_pred_c\"] - df_joined[\"tmax_c\"]\n",
        "    df_joined[\"error_f\"] = df_joined[\"tmax_pred_f\"] - df_joined[\"tmax_f\"]\n",
        "    df_joined[\"abs_error_c\"] = df_joined[\"error_c\"].abs()\n",
        "\n",
        "    print(f\"Joined {len(df_joined)} forecast-truth pairs\")\n",
        "    print(f\"\\nForecast Error Summary:\")\n",
        "    print(f\"  Mean Error (C): {df_joined['error_c'].mean():.2f}\")\n",
        "    print(f\"  Mean Absolute Error (C): {df_joined['abs_error_c'].mean():.2f}\")\n",
        "    print(f\"  Max Absolute Error (C): {df_joined['abs_error_c'].max():.2f}\")\n",
        "\n",
        "    print(\"\\nJoined Data (pred vs actual):\")\n",
        "    cols = [\"target_date\", \"lead_hours\", \"tmax_pred_c\", \"tmax_c\", \"error_c\", \"coverage_hours\"]\n",
        "    print(df_joined[cols].to_string(index=False))\n",
        "else:\n",
        "    print(\"No overlapping dates between forecast and truth data.\")\n",
        "    print(\"This is expected when forecasting future dates that haven't occurred yet.\")\n",
        "    print(f\"\\nForecast dates: {df_forecast['target_date_local'].min().date()} to {df_forecast['target_date_local'].max().date()}\")\n",
        "    print(f\"Truth dates: {df_truth['target_date'].min()} to {df_truth['target_date'].max()}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
