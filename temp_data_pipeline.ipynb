{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed7f29e5"
      },
      "source": [
        "# Setup\n",
        "\n",
        "File setup for project paths and data directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6dc1d05",
        "outputId": "d0e6b31c-b307-4154-a20d-4ad8cf158f4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /content/temp-data-pipeline\n",
            "Data dir: /content/temp-data-pipeline/data\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect Colab\n",
        "IN_COLAB = \"google.colab\" in sys.modules or \"COLAB_GPU\" in os.environ\n",
        "\n",
        "PROJECT_ROOT = None\n",
        "\n",
        "if IN_COLAB:\n",
        "    import subprocess\n",
        "    colab_root = Path(\"/content/temp-data-pipeline\")\n",
        "    if not (colab_root / \"pyproject.toml\").exists():\n",
        "        # Clone repo if not present\n",
        "        subprocess.run(\n",
        "            [\"git\", \"clone\", \"https://github.com/kyler505/temp-data-pipeline.git\", str(colab_root)],\n",
        "            check=True,\n",
        "        )\n",
        "    else:\n",
        "        # Pull latest changes\n",
        "        subprocess.run([\"git\", \"pull\"], cwd=colab_root, check=True)\n",
        "    PROJECT_ROOT = colab_root\n",
        "else:\n",
        "    # Local: search upward for pyproject.toml\n",
        "    cwd = Path.cwd().resolve()\n",
        "    for parent in [cwd] + list(cwd.parents):\n",
        "        if (parent / \"pyproject.toml\").exists():\n",
        "            PROJECT_ROOT = parent\n",
        "            break\n",
        "    # Fallback to common dev location\n",
        "    if PROJECT_ROOT is None:\n",
        "        candidate = Path.home() / \"Documents\" / \"temp-data-pipeline\"\n",
        "        if (candidate / \"pyproject.toml\").exists():\n",
        "            PROJECT_ROOT = candidate\n",
        "\n",
        "if PROJECT_ROOT is None:\n",
        "    raise FileNotFoundError(\"Could not find project root. Set PROJECT_ROOT manually.\")\n",
        "\n",
        "# Add to Python path\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "src_path = PROJECT_ROOT / \"src\"\n",
        "if src_path.exists() and str(src_path) not in sys.path:\n",
        "    sys.path.insert(0, str(src_path))\n",
        "\n",
        "DATA_DIR = PROJECT_ROOT / \"data\"\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"Data dir: {DATA_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Install packages\n",
        "\n",
        "Install project dependencies in editable mode if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installed/updated tempdata in editable mode\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "import importlib\n",
        "\n",
        "# Always reinstall in editable mode to pick up any code changes\n",
        "if (PROJECT_ROOT / \"pyproject.toml\").exists():\n",
        "    subprocess.run(\n",
        "        [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-e\", str(PROJECT_ROOT)],\n",
        "        check=True,\n",
        "    )\n",
        "    # Clear cached imports so we get the latest code\n",
        "    for mod_name in list(sys.modules.keys()):\n",
        "        if mod_name.startswith(\"tempdata\"):\n",
        "            del sys.modules[mod_name]\n",
        "    print(\"Installed/updated tempdata in editable mode\")\n",
        "else:\n",
        "    raise FileNotFoundError(\n",
        "        f\"pyproject.toml not found in {PROJECT_ROOT}. \"\n",
        "        \"Update PROJECT_ROOT in the setup cell.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32c96254"
      },
      "source": [
        "# Fetch NOAA hourly data\n",
        "\n",
        "Configure a station and date range, then run the fetcher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a7c82a2",
        "outputId": "77e0a89d-0c4e-40cf-f5f7-ac51c3419431"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[noaa] 2024: rows=1236 coverage=2024-01-01 00:00:00+00:00 -> 2024-01-31 23:51:00+00:00\n",
            "Wrote 1 parquet files:\n",
            "  - /content/temp-data-pipeline/data/raw/noaa_hourly/KLGA/2024.parquet\n"
          ]
        }
      ],
      "source": [
        "from tempdata.fetch.noaa_hourly import fetch_noaa_hourly\n",
        "\n",
        "STATION_ID = \"KLGA\"\n",
        "START_DATE = \"2024-01-01\"\n",
        "END_DATE = \"2024-02-01\"  # exclusive\n",
        "\n",
        "OUTPUT_DIR = DATA_DIR / \"raw\" / \"noaa_hourly\" / STATION_ID\n",
        "CACHE_DIR = DATA_DIR / \"cache\" / \"isd_csv\" / STATION_ID\n",
        "\n",
        "written = fetch_noaa_hourly(\n",
        "    station_id=STATION_ID,\n",
        "    start_date=START_DATE,\n",
        "    end_date=END_DATE,\n",
        "    out_dir=OUTPUT_DIR,\n",
        "    cache_dir=CACHE_DIR,\n",
        ")\n",
        "\n",
        "print(f\"Wrote {len(written)} parquet files:\")\n",
        "for path in written:\n",
        "    print(f\"  - {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b91ad96"
      },
      "source": [
        "# Verify outputs\n",
        "\n",
        "Load one parquet file to confirm the fetch results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96884b12",
        "outputId": "5b9e7269-f77c-468d-b084-4ee71786ed93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1236 rows from 2024.parquet\n",
            "Schema validation passed\n",
            "                     ts_utc station_id       lat       lon  temp_c source  \\\n",
            "0 2024-01-01 00:00:00+00:00       KLGA  40.77945 -73.88027     6.1   noaa   \n",
            "1 2024-01-01 00:51:00+00:00       KLGA  40.77945 -73.88027     6.1   noaa   \n",
            "2 2024-01-01 01:51:00+00:00       KLGA  40.77945 -73.88027     6.1   noaa   \n",
            "3 2024-01-01 02:51:00+00:00       KLGA  40.77945 -73.88027     6.1   noaa   \n",
            "4 2024-01-01 03:00:00+00:00       KLGA  40.77945 -73.88027     6.1   noaa   \n",
            "\n",
            "   qc_flags  \n",
            "0         0  \n",
            "1         0  \n",
            "2         0  \n",
            "3         0  \n",
            "4         0  \n",
            "Date range: 2024-01-01 00:00:00+00:00 to 2024-01-31 23:51:00+00:00\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from tempdata.schemas import validate_hourly_obs\n",
        "\n",
        "parquet_files = sorted(OUTPUT_DIR.glob(\"*.parquet\"))\n",
        "if not parquet_files:\n",
        "    raise FileNotFoundError(f\"No parquet files found in {OUTPUT_DIR}\")\n",
        "\n",
        "df = pd.read_parquet(parquet_files[0])\n",
        "print(f\"Loaded {len(df)} rows from {parquet_files[0].name}\")\n",
        "\n",
        "# Validate schema (will raise if invalid)\n",
        "validate_hourly_obs(df, require_unique_keys=False)\n",
        "print(\"Schema validation passed\")\n",
        "\n",
        "print(df.head())\n",
        "print(f\"Date range: {df['ts_utc'].min()} to {df['ts_utc'].max()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Clean hourly data\n",
        "\n",
        "Apply the cleaning pipeline to the fetched data:\n",
        "- Validate input schema (early fail on malformed data)\n",
        "- Sort and deduplicate by (ts_utc, station_id)\n",
        "- Flag missing temperature values\n",
        "- Flag and nullify out-of-range temperatures\n",
        "- Detect hour-to-hour spikes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[clean] Cleaning summary:\n",
            "  Total rows: 1236 -> 1234 (2 duplicates removed)\n",
            "  Rows with QC flags: 31\n",
            "    QC_MISSING_VALUE: 31\n",
            "  Temp range (valid): -7.2C to 15.0C\n",
            "\n",
            "Cleaned DataFrame shape: (1234, 7)\n",
            "                     ts_utc station_id       lat       lon  temp_c source  \\\n",
            "0 2024-01-01 00:00:00+00:00       KLGA  40.77945 -73.88027     6.1   noaa   \n",
            "1 2024-01-01 00:51:00+00:00       KLGA  40.77945 -73.88027     6.1   noaa   \n",
            "2 2024-01-01 01:51:00+00:00       KLGA  40.77945 -73.88027     6.1   noaa   \n",
            "3 2024-01-01 02:51:00+00:00       KLGA  40.77945 -73.88027     6.1   noaa   \n",
            "4 2024-01-01 03:00:00+00:00       KLGA  40.77945 -73.88027     6.1   noaa   \n",
            "\n",
            "   qc_flags  \n",
            "0         0  \n",
            "1         0  \n",
            "2         0  \n",
            "3         0  \n",
            "4         0  \n"
          ]
        }
      ],
      "source": [
        "from tempdata.clean import clean_hourly_obs\n",
        "\n",
        "# Clean the fetched data\n",
        "# This applies: deduplication, missing value flags, out-of-range handling, spike detection\n",
        "df_clean = clean_hourly_obs(df)\n",
        "\n",
        "print(f\"\\nCleaned DataFrame shape: {df_clean.shape}\")\n",
        "print(df_clean.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5911ef1"
      },
      "source": [
        "# Aggregate to Daily Tmax\n",
        "\n",
        "Convert cleaned hourly observations to daily maximum temperature (Tmax).\n",
        "\n",
        "Key design principles:\n",
        "- **Market-aligned**: Tmax is computed per station-local calendar day, not UTC\n",
        "- **QC-aware**: Hours with `QC_OUT_OF_RANGE` are excluded from Tmax calculation\n",
        "- **Spike-inclusive**: Spike-flagged values ARE included (to avoid removing real heat spikes)\n",
        "- **Transparent**: Every day carries `coverage_hours` and propagated `qc_flags`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "310d8877",
        "outputId": "80bfcb75-9439-43eb-83d0-b8509766ffbf"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "[daily_tmax]Out of range: column 'coverage_hours' must be in [0, 24] (31 rows) | sample indices: [1, 2, 3, 4, 5]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4283850148.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Validate the output schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mvalidate_daily_tmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_daily\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Daily Tmax schema validation passed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/temp-data-pipeline/src/tempdata/schemas/daily_tmax.py\u001b[0m in \u001b[0;36mvalidate_daily_tmax\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;31m# Range checks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mrequire_int_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"coverage_hours\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_DATASET_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0mrequire_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tmax_c\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_DATASET_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0mrequire_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tmax_f\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m130\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m140\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_DATASET_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/temp-data-pipeline/src/tempdata/schemas/validate.py\u001b[0m in \u001b[0;36mrequire_int_range\u001b[0;34m(df, col, lo, hi, allow_null, dataset)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbad_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mfailing_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnon_null\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout_of_range\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    326\u001b[0m                 _format_error(\n\u001b[1;32m    327\u001b[0m                     \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [daily_tmax]Out of range: column 'coverage_hours' must be in [0, 24] (31 rows) | sample indices: [1, 2, 3, 4, 5]"
          ]
        }
      ],
      "source": [
        "from tempdata.aggregate.build_daily_tmax import build_daily_tmax\n",
        "from tempdata.schemas.daily_tmax import validate_daily_tmax\n",
        "\n",
        "# Station timezone (KLGA is in Eastern time)\n",
        "STATION_TZ = \"America/New_York\"\n",
        "\n",
        "# Build daily Tmax from cleaned hourly data\n",
        "df_daily = build_daily_tmax(df_clean, station_tz=STATION_TZ)\n",
        "\n",
        "# Validate the output schema\n",
        "validate_daily_tmax(df_daily)\n",
        "print(\"Daily Tmax schema validation passed\")\n",
        "\n",
        "print(f\"\\nAggregated {len(df_clean)} hourly obs -> {len(df_daily)} daily records\")\n",
        "print(f\"Date range: {df_daily['date_local'].min().date()} to {df_daily['date_local'].max().date()}\")\n",
        "\n",
        "print(\"\\nDaily Tmax summary:\")\n",
        "print(df_daily[[\"date_local\", \"tmax_c\", \"tmax_f\", \"coverage_hours\", \"qc_flags\"]].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Coverage and Quality Analysis\n",
        "\n",
        "Check data quality metrics for the aggregated daily Tmax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tempdata.schemas.qc_flags import QC_LOW_COVERAGE, QC_INCOMPLETE_DAY, QC_SPIKE_DETECTED\n",
        "\n",
        "# Coverage statistics\n",
        "print(\"Coverage Statistics:\")\n",
        "print(f\"  Min coverage: {df_daily['coverage_hours'].min()} hours\")\n",
        "print(f\"  Max coverage: {df_daily['coverage_hours'].max()} hours\")\n",
        "print(f\"  Mean coverage: {df_daily['coverage_hours'].mean():.1f} hours\")\n",
        "print(f\"  Days with 24h coverage: {(df_daily['coverage_hours'] == 24).sum()}\")\n",
        "\n",
        "# QC flag breakdown\n",
        "print(\"\\nQC Flag Analysis:\")\n",
        "low_coverage_days = ((df_daily['qc_flags'] & QC_LOW_COVERAGE) != 0).sum()\n",
        "incomplete_days = ((df_daily['qc_flags'] & QC_INCOMPLETE_DAY) != 0).sum()\n",
        "spike_days = ((df_daily['qc_flags'] & QC_SPIKE_DETECTED) != 0).sum()\n",
        "\n",
        "print(f\"  Days with QC_LOW_COVERAGE: {low_coverage_days}\")\n",
        "print(f\"  Days with QC_INCOMPLETE_DAY: {incomplete_days}\")\n",
        "print(f\"  Days with QC_SPIKE_DETECTED: {spike_days}\")\n",
        "print(f\"  Days with no QC issues: {(df_daily['qc_flags'] == 0).sum()}\")\n",
        "\n",
        "# Temperature range\n",
        "print(\"\\nTemperature Range:\")\n",
        "print(f\"  Min Tmax: {df_daily['tmax_c'].min():.1f}°C ({df_daily['tmax_f'].min():.1f}°F)\")\n",
        "print(f\"  Max Tmax: {df_daily['tmax_c'].max():.1f}°C ({df_daily['tmax_f'].max():.1f}°F)\")\n",
        "print(f\"  Mean Tmax: {df_daily['tmax_c'].mean():.1f}°C ({df_daily['tmax_f'].mean():.1f}°F)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c81173fe"
      },
      "source": [
        "# Save Daily Tmax\n",
        "\n",
        "Write the daily Tmax data to parquet for downstream use (backtesting, model training, trading validation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb605429",
        "outputId": "55df9858-bba2-4163-e3c5-dc548b36e0e1"
      },
      "outputs": [],
      "source": [
        "from tempdata.aggregate.build_daily_tmax import write_daily_tmax\n",
        "\n",
        "# Output paths\n",
        "DAILY_TMAX_DIR = DATA_DIR / \"clean\" / \"daily_tmax\"\n",
        "DAILY_TMAX_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "output_path = DAILY_TMAX_DIR / f\"{STATION_ID}.parquet\"\n",
        "\n",
        "# Write with schema validation\n",
        "write_daily_tmax(df_daily, output_path)\n",
        "\n",
        "# Also save cleaned hourly data for reference\n",
        "HOURLY_CLEAN_DIR = DATA_DIR / \"clean\" / \"hourly_obs\" / STATION_ID\n",
        "HOURLY_CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Determine year range for partitioning\n",
        "years = df_clean[\"ts_utc\"].dt.year.unique()\n",
        "for year in years:\n",
        "    year_df = df_clean[df_clean[\"ts_utc\"].dt.year == year]\n",
        "    year_path = HOURLY_CLEAN_DIR / f\"{year}.parquet\"\n",
        "    year_df.to_parquet(year_path, index=False)\n",
        "    print(f\"[clean] Wrote {len(year_df)} rows to {year_path}\")\n",
        "\n",
        "print(f\"\\nPipeline complete!\")\n",
        "print(f\"  Daily Tmax: {output_path}\")\n",
        "print(f\"  Cleaned hourly: {HOURLY_CLEAN_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Verify Saved Data\n",
        "\n",
        "Reload the saved parquet to confirm it was written correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload and verify the saved daily Tmax data\n",
        "df_verify = pd.read_parquet(output_path)\n",
        "\n",
        "# Validate schema\n",
        "validate_daily_tmax(df_verify)\n",
        "print(f\"Reloaded {len(df_verify)} daily records from {output_path.name}\")\n",
        "print(\"Schema validation passed\")\n",
        "\n",
        "# Show full dataset\n",
        "print(\"\\nDaily Tmax Data:\")\n",
        "print(df_verify.to_string(index=False))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
