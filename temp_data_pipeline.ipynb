{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed7f29e5"
      },
      "source": [
        "# Setup\n",
        "\n",
        "File setup for project paths and data directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6dc1d05",
        "outputId": "d0e6b31c-b307-4154-a20d-4ad8cf158f4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /content/temp-data-pipeline\n",
            "Data dir: /content/temp-data-pipeline/data\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect Colab\n",
        "IN_COLAB = \"google.colab\" in sys.modules or \"COLAB_GPU\" in os.environ\n",
        "\n",
        "PROJECT_ROOT = None\n",
        "\n",
        "if IN_COLAB:\n",
        "    import subprocess\n",
        "    colab_root = Path(\"/content/temp-data-pipeline\")\n",
        "    if not (colab_root / \"pyproject.toml\").exists():\n",
        "        # Clone repo if not present\n",
        "        subprocess.run(\n",
        "            [\"git\", \"clone\", \"https://github.com/kyler505/temp-data-pipeline.git\", str(colab_root)],\n",
        "            check=True,\n",
        "        )\n",
        "    else:\n",
        "        # Pull latest changes\n",
        "        subprocess.run([\"git\", \"pull\"], cwd=colab_root, check=True)\n",
        "    PROJECT_ROOT = colab_root\n",
        "else:\n",
        "    # Local: search upward for pyproject.toml\n",
        "    cwd = Path.cwd().resolve()\n",
        "    for parent in [cwd] + list(cwd.parents):\n",
        "        if (parent / \"pyproject.toml\").exists():\n",
        "            PROJECT_ROOT = parent\n",
        "            break\n",
        "    # Fallback to common dev location\n",
        "    if PROJECT_ROOT is None:\n",
        "        candidate = Path.home() / \"Documents\" / \"temp-data-pipeline\"\n",
        "        if (candidate / \"pyproject.toml\").exists():\n",
        "            PROJECT_ROOT = candidate\n",
        "\n",
        "if PROJECT_ROOT is None:\n",
        "    raise FileNotFoundError(\"Could not find project root. Set PROJECT_ROOT manually.\")\n",
        "\n",
        "# Add to Python path\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "src_path = PROJECT_ROOT / \"src\"\n",
        "if src_path.exists() and str(src_path) not in sys.path:\n",
        "    sys.path.insert(0, str(src_path))\n",
        "\n",
        "DATA_DIR = PROJECT_ROOT / \"data\"\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"Data dir: {DATA_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Install packages\n",
        "\n",
        "Install project dependencies in editable mode if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tempdata already importable\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "import importlib\n",
        "\n",
        "# Always reinstall in editable mode to pick up any code changes\n",
        "if (PROJECT_ROOT / \"pyproject.toml\").exists():\n",
        "    subprocess.run(\n",
        "        [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-e\", str(PROJECT_ROOT)],\n",
        "        check=True,\n",
        "    )\n",
        "    # Clear cached imports so we get the latest code\n",
        "    for mod_name in list(sys.modules.keys()):\n",
        "        if mod_name.startswith(\"tempdata\"):\n",
        "            del sys.modules[mod_name]\n",
        "    print(\"Installed/updated tempdata in editable mode\")\n",
        "else:\n",
        "    raise FileNotFoundError(\n",
        "        f\"pyproject.toml not found in {PROJECT_ROOT}. \"\n",
        "        \"Update PROJECT_ROOT in the setup cell.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32c96254"
      },
      "source": [
        "# Fetch NOAA hourly data\n",
        "\n",
        "Configure a station and date range, then run the fetcher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a7c82a2",
        "outputId": "77e0a89d-0c4e-40cf-f5f7-ac51c3419431"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[noaa] 2024: rows=1236 coverage=2024-01-01 00:00:00+00:00 -> 2024-01-31 23:51:00+00:00\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": []
        }
      ],
      "source": [
        "from tempdata.fetch.noaa_hourly import fetch_noaa_hourly\n",
        "\n",
        "STATION_ID = \"KLGA\"\n",
        "START_DATE = \"2024-01-01\"\n",
        "END_DATE = \"2024-02-01\"  # exclusive\n",
        "\n",
        "OUTPUT_DIR = DATA_DIR / \"raw\" / \"noaa_hourly\" / STATION_ID\n",
        "CACHE_DIR = DATA_DIR / \"cache\" / \"isd_csv\" / STATION_ID\n",
        "\n",
        "written = fetch_noaa_hourly(\n",
        "    station_id=STATION_ID,\n",
        "    start_date=START_DATE,\n",
        "    end_date=END_DATE,\n",
        "    out_dir=OUTPUT_DIR,\n",
        "    cache_dir=CACHE_DIR,\n",
        ")\n",
        "\n",
        "print(f\"Wrote {len(written)} parquet files:\")\n",
        "for path in written:\n",
        "    print(f\"  - {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b91ad96"
      },
      "source": [
        "# Verify outputs\n",
        "\n",
        "Load one parquet file to confirm the fetch results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96884b12",
        "outputId": "5b9e7269-f77c-468d-b084-4ee71786ed93"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": []
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from tempdata.schemas import validate_hourly_obs\n",
        "\n",
        "parquet_files = sorted(OUTPUT_DIR.glob(\"*.parquet\"))\n",
        "if not parquet_files:\n",
        "    raise FileNotFoundError(f\"No parquet files found in {OUTPUT_DIR}\")\n",
        "\n",
        "df = pd.read_parquet(parquet_files[0])\n",
        "print(f\"Loaded {len(df)} rows from {parquet_files[0].name}\")\n",
        "\n",
        "# Validate schema (will raise if invalid)\n",
        "validate_hourly_obs(df, require_unique_keys=False)\n",
        "print(\"Schema validation passed\")\n",
        "\n",
        "print(df.head())\n",
        "print(f\"Date range: {df['ts_utc'].min()} to {df['ts_utc'].max()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cleaning data (placeholder)\n",
        "\n",
        "Cleaning logic will be added here after data fetching is implemented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5911ef1"
      },
      "source": [
        "# Aggregate daily tmax (placeholder)\n",
        "\n",
        "Aggregation logic will be added here after cleaning is implemented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "310d8877",
        "outputId": "80bfcb75-9439-43eb-83d0-b8509766ffbf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c81173fe"
      },
      "source": [
        "# Export artifacts (placeholder)\n",
        "\n",
        "Exports (CSV/Parquet) will be listed here once the pipeline outputs are finalized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb605429",
        "outputId": "55df9858-bba2-4163-e3c5-dc548b36e0e1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
