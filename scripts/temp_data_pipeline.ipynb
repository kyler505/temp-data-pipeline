{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed7f29e5"
      },
      "source": [
        "# 1. Setup and Drive Mount\n",
        "\n",
        "Mounts Google Drive to allow for persistent data storage and sets up the workspace directory where the project repository will be located."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6dc1d05",
        "outputId": "d0e6b31c-b307-4154-a20d-4ad8cf158f4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Repository already exists, pulling latest changes...\n",
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# 1. Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Set workspace\n",
        "WORKSPACE_DIR = Path('/content/drive/MyDrive/temp-data-pipeline')\n",
        "WORKSPACE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "os.chdir(WORKSPACE_DIR)\n",
        "\n",
        "# 3. Clone repository\n",
        "# TODO: REPLACE 'YOUR_USERNAME' WITH YOUR ACTUAL GITHUB USERNAME BELOW\n",
        "if not (WORKSPACE_DIR / '.git').exists():\n",
        "    print(\"Cloning repository...\")\n",
        "    !git clone https://github.com/kyler505/temp-data-pipeline.git .\n",
        "else:\n",
        "    print(\"Repository already exists, pulling latest changes...\")\n",
        "    !git pull\n",
        "\n",
        "# Add workspace to python path\n",
        "sys.path.insert(0, str(WORKSPACE_DIR))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32c96254"
      },
      "source": [
        "# 2. Install Dependencies\n",
        "\n",
        "Installs the necessary Python libraries and the current project in editable mode so that changes to the code are immediately reflected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a7c82a2",
        "outputId": "77e0a89d-0c4e-40cf-f5f7-ac51c3419431"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing dependencies...\n",
            "Obtaining file:///content/drive/MyDrive/temp-data-pipeline\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: temp-data-pipeline\n",
            "  Building editable for temp-data-pipeline (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for temp-data-pipeline: filename=temp_data_pipeline-0.1.0-0.editable-py3-none-any.whl size=1374 sha256=ab96e7ed975a571116f2e3937f9a6dc9d36b21e16af9a33f15635b862f243765\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0h7s5qel/wheels/78/a9/9c/fcbaf7e053bef092e59418d008e31e1346c7b345f02b6ae767\n",
            "Successfully built temp-data-pipeline\n",
            "Installing collected packages: temp-data-pipeline\n",
            "  Attempting uninstall: temp-data-pipeline\n",
            "    Found existing installation: temp-data-pipeline 0.1.0\n",
            "    Uninstalling temp-data-pipeline-0.1.0:\n",
            "      Successfully uninstalled temp-data-pipeline-0.1.0\n",
            "Successfully installed temp-data-pipeline-0.1.0\n"
          ]
        }
      ],
      "source": [
        "print(\"Installing dependencies...\")\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b91ad96"
      },
      "source": [
        "# 3. Configure Data Directories\n",
        "\n",
        "Sets up the path variables for where data will be stored in Google Drive, ensuring the directories exist before running the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96884b12",
        "outputId": "5b9e7269-f77c-468d-b084-4ee71786ed93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data directory: /content/drive/MyDrive/temp-data-pipeline-data\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "DRIVE_DATA_DIR = Path('/content/drive/MyDrive/temp-data-pipeline-data')\n",
        "DRIVE_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Data directory: {DRIVE_DATA_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Diagnostic: Test imports step by step\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Ensure we can import the package\n",
        "# Sometimes Colab needs the src directory explicitly in the path\n",
        "src_path = Path(WORKSPACE_DIR) / 'src'\n",
        "if str(src_path) not in sys.path:\n",
        "    sys.path.insert(0, str(src_path))\n",
        "    print(f\"Added {src_path} to sys.path\")\n",
        "\n",
        "print(\"Testing imports...\")\n",
        "try:\n",
        "    print(\"1. Testing tempdata import...\")\n",
        "    import tempdata\n",
        "    print(f\"   ✓ tempdata imported (version: {tempdata.__version__})\")\n",
        "\n",
        "    print(\"2. Testing tempdata.fetch import...\")\n",
        "    import tempdata.fetch\n",
        "    print(\"   ✓ tempdata.fetch imported\")\n",
        "\n",
        "    print(\"3. Testing tempdata.fetch.noaa_hourly import...\")\n",
        "    from tempdata.fetch.noaa_hourly import fetch_noaa_hourly\n",
        "    print(\"   ✓ fetch_noaa_hourly imported\")\n",
        "\n",
        "    print(\"\\n✓ All imports successful!\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n✗ Import failed at step:\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5911ef1"
      },
      "source": [
        "# 4. Run Pipeline\n",
        "\n",
        "Runs the main data fetching function `fetch_noaa_hourly`. It downloads data for the specified station and date range, saving the results as Parquet files in the configured Drive directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "310d8877",
        "outputId": "80bfcb75-9439-43eb-83d0-b8509766ffbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[WARNING] Package not installed or path not setup correctly yet.\n",
            "If the pip install command above succeeded, you may need to Restart the Runtime and run this cell again.\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "import traceback\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Ensure we can import the package\n",
        "# Sometimes Colab needs the src directory explicitly in the path\n",
        "src_path = Path(WORKSPACE_DIR) / 'src'\n",
        "if str(src_path) not in sys.path:\n",
        "    sys.path.insert(0, str(src_path))\n",
        "\n",
        "try:\n",
        "    from tempdata.fetch.noaa_hourly import fetch_noaa_hourly\n",
        "\n",
        "    print(\"\\nRunning pipeline for KLGA (Jan 2024)...\")\n",
        "    written = fetch_noaa_hourly(\n",
        "        station_id='KLGA',\n",
        "        start_date='2024-01-01',\n",
        "        end_date='2024-02-01',\n",
        "        out_dir=str(DRIVE_DATA_DIR / 'raw' / 'noaa_hourly' / 'KLGA'),\n",
        "        cache_dir=str(DRIVE_DATA_DIR / 'cache' / 'isd_csv' / 'KLGA'),\n",
        "    )\n",
        "\n",
        "    print(f\"\\n✓ Pipeline completed. Wrote {len(written)} files.\")\n",
        "\n",
        "except ImportError as e:\n",
        "    print(\"\\n[ERROR] Import failed. Full traceback:\")\n",
        "    traceback.print_exc()\n",
        "    print(\"\\n[WARNING] Package not installed or path not setup correctly yet.\")\n",
        "    print(\"If the pip install command above succeeded, you may need to Restart the Runtime and run this cell again.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred: {e}\")\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c81173fe"
      },
      "source": [
        "# 5. Verify Results\n",
        "\n",
        "Scans the output directory for the generated Parquet files and loads the first one using pandas to verify that the data is readable and correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb605429",
        "outputId": "55df9858-bba2-4163-e3c5-dc548b36e0e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No parquet files found.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Verify Results\n",
        "parquet_files = sorted(\n",
        "    (DRIVE_DATA_DIR / 'raw' / 'noaa_hourly' / 'KLGA').glob('*.parquet')\n",
        ")\n",
        "\n",
        "if parquet_files:\n",
        "    df = pd.read_parquet(parquet_files[0])\n",
        "    print(f\"Loaded {len(df)} rows from {parquet_files[0].name}\")\n",
        "    print(\"\\nFirst few rows:\")\n",
        "    print(df.head())\n",
        "    print(f\"\\nDate range: {df['ts_utc'].min()} to {df['ts_utc'].max()}\")\n",
        "else:\n",
        "    print(\"No parquet files found.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
